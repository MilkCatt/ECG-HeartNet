{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchmetrics import F1Score\n",
    "from sklearn.metrics import multilabel_confusion_matrix, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Our objects\n",
    "from utils import count_parameters, find_optimal_thresholds, plot_multilabel_conf_matrices, evaluate_with_custom_thresholds\n",
    "from objects import NormalizeECG, ECGDataset\n",
    "from models import ECGTransformer, ECGEmbeddings, ECGCombined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device for torch\n",
    "device = torch.device(\"cpu\")\n",
    "# MPS for Apple Silicon GPUs\n",
    "if torch.mps.is_available():\n",
    "   print(\"MPS is available\")\n",
    "   device = torch.device(\"mps\")\n",
    "\n",
    "# CUDA for Nvidia GPUs\n",
    "if torch.cuda.is_available():\n",
    "   print(\"CUDA is available\")\n",
    "   device = torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, device, pos_weights=None, accum_steps=4, checkpoint_interval=256, lr=1e-4,\n",
    "                 resume_checkpoint=None):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.accum_steps = accum_steps\n",
    "        self.checkpoint_interval = checkpoint_interval\n",
    "        \n",
    "        # Initialize essential components\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.f1 = F1Score(task='multilabel', num_labels=self.model.num_classes, average=None)\n",
    "        self.loss = nn.BCEWithLogitsLoss(pos_weight=pos_weights.to(device) if pos_weights is not None else None)\n",
    "        self.accum_loss = 0.0\n",
    "        self.loss_history = []\n",
    "        self.acc_history = []\n",
    "        self.batch_count = 0\n",
    "        self.start_epoch = 0\n",
    "        self.start_batch = 0\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='max',              # maximize F1\n",
    "            factor=0.5,              # halve the LR\n",
    "            patience=2,              # after 2 stagnating checkpoints\n",
    "            threshold=0.001,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        if resume_checkpoint:\n",
    "            self._load_checkpoint(resume_checkpoint)\n",
    "\n",
    "    def _load_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"Load training state from checkpoint\"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)\n",
    "        \n",
    "        # Essential parameters\n",
    "        self.model.load_state_dict(checkpoint['model_state'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        \n",
    "        # Training progress\n",
    "        self.loss_history = checkpoint['loss_history']\n",
    "        self.acc_history = checkpoint['acc_history']\n",
    "        self.batch_count = checkpoint.get('batch_count', 0)\n",
    "        self.start_epoch = checkpoint['epoch']\n",
    "        self.start_batch = checkpoint.get('batch', 0) + 1\n",
    "        \n",
    "        # Configurations\n",
    "        self.checkpoint_interval = checkpoint.get('checkpoint_interval', \n",
    "                                                 self.checkpoint_interval)\n",
    "        \n",
    "        print(f\"Resuming from epoch {self.start_epoch} batch {self.start_batch}\")\n",
    "\n",
    "    def train(self, train_dataloader, test_dataloader, num_epochs, save_path=\"training_progress\"):\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(self.start_epoch, num_epochs):\n",
    "            for batch_idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "                if batch_idx < self.start_batch:\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                # Forward pass\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.loss(outputs, labels) / self.accum_steps\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Every batch\n",
    "                self.accum_loss += loss.item()\n",
    "                self.batch_count += 1\n",
    "                \n",
    "                # Every accum_steps\n",
    "                if (batch_idx + 1) % self.accum_steps == 0:\n",
    "                    self._update_parameters()\n",
    "                    \n",
    "                    # Save loss\n",
    "                    avg_loss = self.accum_loss\n",
    "                    self.loss_history.append([self.batch_count, avg_loss])\n",
    "                    self.accum_loss = 0.0\n",
    "\n",
    "                    print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx+1}/{len(train_dataloader)} | \"\n",
    "                        f\"Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "                # Every checkpoint_interval\n",
    "                if self.batch_count % self.checkpoint_interval == 0:\n",
    "                    acc = self.evaluate(test_dataloader)\n",
    "                    self.acc_history.append([self.batch_count, acc])\n",
    "\n",
    "                    # Scheduler step based on F1 macro average\n",
    "                    avg_f1 = np.mean(acc[\"f1_per_class\"])\n",
    "                    self.scheduler.step(avg_f1)\n",
    "\n",
    "                    self._save_checkpoint(save_path, epoch, batch_idx)\n",
    "                \n",
    "                # Sanity checks\n",
    "                if self.batch_count % 128 == 0:\n",
    "                    probs = torch.sigmoid(outputs)\n",
    "                    print(\"Example probs:\", probs[0].detach().cpu().numpy())\n",
    "                    print(\"Ground truth :\", labels[0].cpu().numpy())\n",
    "                \n",
    "                del inputs, labels, outputs, loss\n",
    "            self.start_batch = 0\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_samples = 0\n",
    "        num_classes = self.model.num_classes\n",
    "        mismatches_per_class = torch.zeros(num_classes, device=self.device)\n",
    "\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                \n",
    "                all_probs.append(probs)\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "                total_samples += inputs.size(0)\n",
    "\n",
    "        # Stack across batches\n",
    "        all_probs = np.vstack(all_probs)\n",
    "        all_labels = np.vstack(all_labels)\n",
    "\n",
    "        # Find optimal thresholds\n",
    "        optimal_thresholds = find_optimal_thresholds(all_labels, all_probs)\n",
    "        print(\"Optimal thresholds per class:\", optimal_thresholds)\n",
    "\n",
    "        # Apply thresholds\n",
    "        preds = (all_probs >= optimal_thresholds).astype(int)\n",
    "\n",
    "        # Metrics\n",
    "        f1_score_per_class = f1_score(all_labels, preds, average=None, zero_division=0)\n",
    "        hamming_loss_per_class = np.mean(np.not_equal(preds, all_labels), axis=0)\n",
    "        overall_hamming_loss = np.mean(np.not_equal(preds, all_labels))\n",
    "\n",
    "        precision_per_class = precision_score(all_labels, preds, average=None, zero_division=0)\n",
    "        recall_per_class = recall_score(all_labels, preds, average=None, zero_division=0)\n",
    "\n",
    "        conf_matrices = multilabel_confusion_matrix(all_labels, preds)\n",
    "        tp = conf_matrices[:, 1, 1].tolist()\n",
    "        fp = conf_matrices[:, 0, 1].tolist()\n",
    "        fn = conf_matrices[:, 1, 0].tolist()\n",
    "        tn = conf_matrices[:, 0, 0].tolist()\n",
    "\n",
    "        return {\n",
    "            \"f1_per_class\": f1_score_per_class,\n",
    "            \"overall_hamming_loss\": overall_hamming_loss,\n",
    "            \"hamming_loss_per_class\": hamming_loss_per_class,\n",
    "            \"precision\": precision_per_class,\n",
    "            \"recall\": recall_per_class,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn,\n",
    "            \"optimal_thresholds\": optimal_thresholds  # Save this if you want to reuse\n",
    "        }   \n",
    "    \n",
    "    def _update_parameters(self):\n",
    "        \"\"\"Update model parameters with gradient clipping\"\"\"\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def _save_checkpoint(self, path, epoch, batch_idx):\n",
    "        \"\"\"Save model and training state\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'batch': batch_idx,\n",
    "            'batch_count': self.batch_count,\n",
    "            'checkpoint_interval': self.checkpoint_interval,\n",
    "            'model_state': self.model.state_dict(),\n",
    "            'optimizer_state': self.optimizer.state_dict(),\n",
    "            'loss_history': self.loss_history,\n",
    "            'acc_history': self.acc_history\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, f\"{path}/checkpoint_ep{epoch}_b{batch_idx}.pt\")\n",
    "        print(f\"\\nCheckpoint saved at epoch {epoch+1} batch {batch_idx+1}\")\n",
    "\n",
    "        np.save(f\"{path}/loss_history.npy\", np.array(self.loss_history))\n",
    "        np.save(f\"{path}/acc_history.npy\", np.array(self.acc_history))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta\n",
    "diagnoses = \"data/diagnoses_balanced.csv\"\n",
    "data_path = \"data/ecg_clipped\"\n",
    "save_path = \"training_progress/clipped_balanced3\"\n",
    "checkpoint_interval = 512\n",
    "\n",
    "# Training Hyperparameters\n",
    "add_pos_weights = True\n",
    "normalize = True\n",
    "batch_size = 4\n",
    "accum_steps = 8         # Updates every accum_steps batches\n",
    "starting_lr = 5e-4      # For resuming, set lr (could be lower) at the resume cell below\n",
    "\n",
    "# Model Embeddings parameters\n",
    "d_input = 12\n",
    "d_model = 128\n",
    "\n",
    "# Model Transformer parameters\n",
    "nhead = 4\n",
    "num_encoder_layers = 2\n",
    "dim_feedforward = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_dataset = ECGDataset(path=data_path, diagnoses=diagnoses, transform=NormalizeECG() if normalize else None)\n",
    "pos_weights = ecg_dataset.get_pos_weights() if add_pos_weights else None\n",
    "num_classes = ecg_dataset.get_num_classes()\n",
    "\n",
    "train_dataset, test_dataset, val_dataset = random_split(\n",
    "                                            ecg_dataset, [len(ecg_dataset) - 1000, 500, 500])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ECGCombined(d_input=d_input, d_model=d_model, num_classes=num_classes, nhead=nhead, num_encoder_layers=num_encoder_layers, dim_feedforward=dim_feedforward).to(device)\n",
    "print(model)\n",
    "print(\"Number of parameters in embeddings\")\n",
    "count_parameters(model.embedding_model)\n",
    "print(\"Number of parameters in transformer\")\n",
    "count_parameters(model.transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training from 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, device, accum_steps=accum_steps, lr=starting_lr, pos_weights=pos_weights, checkpoint_interval=checkpoint_interval)\n",
    "trainer.train(train_dataloader, test_dataloader, num_epochs=10, save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume Training from a checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_from = f\"{save_path}/checkpoint_ep7_b4773.pt\"    # Set checkpoint here\n",
    "resume_lr = 1e-4                                        # Set a lower lr if needed\n",
    "\n",
    "trainer = Trainer(model, device, accum_steps=accum_steps, lr=starting_lr, pos_weights=pos_weights, checkpoint_interval=checkpoint_interval, resume_checkpoint=resume_from)\n",
    "trainer.train(train_dataloader, test_dataloader, num_epochs=10, save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = np.load(f'{save_path}/loss_history.npy', allow_pickle=True)  # Load loss history\n",
    "acc_history = np.load(f'{save_path}/acc_history.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss history\n",
    "x = [epoch[0] for epoch in loss_history]\n",
    "y = [epoch[1] for epoch in loss_history]\n",
    "plt.plot(x, y, label='Loss')\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1 score history\n",
    "x = [epoch[0] for epoch in acc_history]\n",
    "y = [epoch[1]['f1_per_class'] for epoch in acc_history]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Batches')\n",
    "plt.legend([f'Class {i}' for i in range(len(y[0]))])\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Hamming Loss\n",
    "x = [epoch[0] for epoch in acc_history]\n",
    "y = [epoch[1]['hamming_loss_per_class'] for epoch in acc_history]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.legend([f'Class {i}' for i in range(len(y[0]))])\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('Hamming Loss')\n",
    "#plt.yscale('log')\n",
    "plt.title('Hamming Loss per class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrices\n",
    "# Choose the latest evaluation (last checkpoint)\n",
    "last_eval = acc_history[-1][1]  # acc_history = [batch_count, eval_dict]\n",
    "\n",
    "for i in range(5):\n",
    "   # Extract TP, FP, FN, TN\n",
    "   tp = last_eval[\"tp\"][i]\n",
    "   fp = last_eval[\"fp\"][i]\n",
    "   fn = last_eval[\"fn\"][i]\n",
    "   tn = last_eval[\"tn\"][i]\n",
    "\n",
    "   # Create confusion matrix\n",
    "   cm = np.array([[tn, fp],\n",
    "                  [fn, tp]])\n",
    "\n",
    "   # Plot\n",
    "   sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "               xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
    "               yticklabels=[\"True 0\", \"True 1\"])\n",
    "   plt.title(f\"Confusion Matrix for Class {i}\")\n",
    "   plt.xlabel(\"Prediction\")\n",
    "   plt.ylabel(\"Actual\")\n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_from = f\"{save_path}/checkpoint_ep7_b5285.pt\"\n",
    "resume_lr = 1e-4\n",
    "\n",
    "model = ECGCombined(d_input=d_input, d_model=d_model, num_classes=num_classes, nhead=nhead, num_encoder_layers=num_encoder_layers, dim_feedforward=dim_feedforward).to(device)\n",
    "trainer = Trainer(model, device, accum_steps=accum_steps, lr=starting_lr, pos_weights=pos_weights, checkpoint_interval=checkpoint_interval, resume_checkpoint=resume_from)\n",
    "# Final model evaluation\n",
    "results = trainer.evaluate(test_dataloader)\n",
    "\n",
    "# Save optimal thresholds for reuse\n",
    "np.save(f\"{save_path}/thresholds.npy\", results[\"optimal_thresholds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final model checkpoint\n",
    "resume_from = f\"{save_path}/checkpoint_ep7_b4773.pt\"  # Replace with best one\n",
    "model = ECGCombined(d_input=d_input, d_model=d_model, num_classes=num_classes, nhead=nhead, num_encoder_layers=num_encoder_layers, dim_feedforward=dim_feedforward).to(device)\n",
    "trainer = Trainer(model, device, pos_weights=pos_weights, resume_checkpoint=resume_from)\n",
    "\n",
    "# Load optimal thresholds\n",
    "thresholds = np.load(f\"{save_path}/thresholds.npy\")\n",
    "\n",
    "# Evaluate with custom thresholds\n",
    "\n",
    "evaluate_with_custom_thresholds(trainer, test_dataloader, thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(test_dataloader)\n",
    "thresholds = np.load(f\"{save_path}/thresholds.npy\")\n",
    "\n",
    "# Redo predictions using new thresholds\n",
    "all_probs, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "all_probs = np.vstack(all_probs)\n",
    "all_labels = np.vstack(all_labels)\n",
    "y_pred = (all_probs >= thresholds).astype(int)\n",
    "\n",
    "# Plot confusion matrices\n",
    "plot_multilabel_conf_matrices(all_labels, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
