{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "amdgpu.ids: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    for name, module in model.named_modules():\n",
    "        params = sum(p.numel() for p in module.parameters())\n",
    "        print(f\"{name}: {params} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader\n",
    "\n",
    "To investigate: Normalization or other transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeECG:\n",
    "    def __call__(self, tensor):\n",
    "        # Z-score normalization per lead\n",
    "        means = tensor.mean(dim=1, keepdim=True)\n",
    "        stds = tensor.std(dim=1, keepdim=True)\n",
    "        return (tensor - means) / (stds + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, path=\"data/ecg\", diagnoses='data/diagnoses.csv', transform=None):\n",
    "        # Store file paths with cleaned IDs\n",
    "        self.file_info = []  # List of tuples (cleaned_id, file_path)\n",
    "        \n",
    "        # Extract numeric IDs from filenames\n",
    "        for root, _, files in os.walk(path):\n",
    "            for file in files:\n",
    "                if file.endswith('.csv'):\n",
    "                    # Extract numeric part from filename (e.g., \"abc00001.csv\" -> \"00001\")\n",
    "                    cleaned_id = re.sub(r'\\D', '', file)\n",
    "                    self.file_info.append((\n",
    "                        cleaned_id,\n",
    "                        os.path.join(root, file)\n",
    "                    ))\n",
    "\n",
    "        # Load and prepare labels\n",
    "        self.labels_df = pd.read_csv(diagnoses)\n",
    "        # Clean IDs in both DataFrame and our file list\n",
    "        self.labels_df['ID'] = self.labels_df['ID'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "        self.labels_df.set_index('ID', inplace=True)\n",
    "\n",
    "        # Filter out files without corresponding labels\n",
    "        valid_ids = set(self.labels_df.index)\n",
    "        self.file_info = [(i, p) for i, p in self.file_info if i in valid_ids]\n",
    "\n",
    "        self.transform = transform\n",
    "        self.cache = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.cache:\n",
    "            return self.cache[idx]\n",
    "\n",
    "        cleaned_id, file_path = self.file_info[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load ECG data\n",
    "            df = pd.read_csv(file_path)\n",
    "            ecg_data = df.drop(columns=['time']).values\n",
    "            tensor = torch.tensor(ecg_data, dtype=torch.float32).T  # (leads, timesteps)\n",
    "            \n",
    "            if self.transform:\n",
    "                tensor = self.transform(tensor)\n",
    "                \n",
    "            # Get corresponding label\n",
    "            label_values = self.labels_df.loc[cleaned_id].values  # Get all label columns\n",
    "            label = torch.tensor(label_values, dtype=torch.float32)  # Use float for multi-label\n",
    "\n",
    "            return tensor, label\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {str(e)}\")\n",
    "            # Return zero tensor and -1 label placeholder\n",
    "            return torch.zeros((12, 5000)), torch.tensor(-1, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 5000])\n",
      "torch.Size([94])\n"
     ]
    }
   ],
   "source": [
    "dataset = ECGDataset()\n",
    "data, label = dataset.__getitem__(0)\n",
    "print(data.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGTransformer(nn.Module):\n",
    "    def __init__(self, d_model, num_classes=63, nhead=8, num_encoder_layers=2, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "\n",
    "        # Encoder stack\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.transformer(x)\n",
    "        # encoded shape: (batch_size, seq_len, d_model)\n",
    "        # Pick out only the last in the sequence for classification\n",
    "        encoded = encoded[:, -1, :]\n",
    "        result = self.classifier(encoded)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGTransformer(\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=12, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=12, bias=True)\n",
      "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=12, out_features=63, bias=True)\n",
      ")\n",
      ": 81723 parameters\n",
      "transformer: 80904 parameters\n",
      "transformer.layers: 80904 parameters\n",
      "transformer.layers.0: 13484 parameters\n",
      "transformer.layers.0.self_attn: 624 parameters\n",
      "transformer.layers.0.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.0.linear1: 6656 parameters\n",
      "transformer.layers.0.dropout: 0 parameters\n",
      "transformer.layers.0.linear2: 6156 parameters\n",
      "transformer.layers.0.norm1: 24 parameters\n",
      "transformer.layers.0.norm2: 24 parameters\n",
      "transformer.layers.0.dropout1: 0 parameters\n",
      "transformer.layers.0.dropout2: 0 parameters\n",
      "transformer.layers.1: 13484 parameters\n",
      "transformer.layers.1.self_attn: 624 parameters\n",
      "transformer.layers.1.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.1.linear1: 6656 parameters\n",
      "transformer.layers.1.dropout: 0 parameters\n",
      "transformer.layers.1.linear2: 6156 parameters\n",
      "transformer.layers.1.norm1: 24 parameters\n",
      "transformer.layers.1.norm2: 24 parameters\n",
      "transformer.layers.1.dropout1: 0 parameters\n",
      "transformer.layers.1.dropout2: 0 parameters\n",
      "transformer.layers.2: 13484 parameters\n",
      "transformer.layers.2.self_attn: 624 parameters\n",
      "transformer.layers.2.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.2.linear1: 6656 parameters\n",
      "transformer.layers.2.dropout: 0 parameters\n",
      "transformer.layers.2.linear2: 6156 parameters\n",
      "transformer.layers.2.norm1: 24 parameters\n",
      "transformer.layers.2.norm2: 24 parameters\n",
      "transformer.layers.2.dropout1: 0 parameters\n",
      "transformer.layers.2.dropout2: 0 parameters\n",
      "transformer.layers.3: 13484 parameters\n",
      "transformer.layers.3.self_attn: 624 parameters\n",
      "transformer.layers.3.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.3.linear1: 6656 parameters\n",
      "transformer.layers.3.dropout: 0 parameters\n",
      "transformer.layers.3.linear2: 6156 parameters\n",
      "transformer.layers.3.norm1: 24 parameters\n",
      "transformer.layers.3.norm2: 24 parameters\n",
      "transformer.layers.3.dropout1: 0 parameters\n",
      "transformer.layers.3.dropout2: 0 parameters\n",
      "transformer.layers.4: 13484 parameters\n",
      "transformer.layers.4.self_attn: 624 parameters\n",
      "transformer.layers.4.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.4.linear1: 6656 parameters\n",
      "transformer.layers.4.dropout: 0 parameters\n",
      "transformer.layers.4.linear2: 6156 parameters\n",
      "transformer.layers.4.norm1: 24 parameters\n",
      "transformer.layers.4.norm2: 24 parameters\n",
      "transformer.layers.4.dropout1: 0 parameters\n",
      "transformer.layers.4.dropout2: 0 parameters\n",
      "transformer.layers.5: 13484 parameters\n",
      "transformer.layers.5.self_attn: 624 parameters\n",
      "transformer.layers.5.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.5.linear1: 6656 parameters\n",
      "transformer.layers.5.dropout: 0 parameters\n",
      "transformer.layers.5.linear2: 6156 parameters\n",
      "transformer.layers.5.norm1: 24 parameters\n",
      "transformer.layers.5.norm2: 24 parameters\n",
      "transformer.layers.5.dropout1: 0 parameters\n",
      "transformer.layers.5.dropout2: 0 parameters\n",
      "classifier: 819 parameters\n"
     ]
    }
   ],
   "source": [
    "model = ECGTransformer(d_model=12, nhead=4, num_classes=63, num_encoder_layers=6, dim_feedforward=512)\n",
    "print(model)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An embedding model \n",
    "that uses convolution\n",
    "\n",
    "Convolution turning 12 channels to 128, repeated to transfer forward 200ms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGEmbeddings(nn.Module):\n",
    "    def __init__(self, d_input, d_model):\n",
    "        super().__init__()\n",
    "        # Keep original layers\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(d_model if i>0 else d_input, d_model, 50, stride=1, padding='same')\n",
    "            for i in range(8)\n",
    "        ])\n",
    "        self.activation = nn.ReLU(inplace=False)  # Important for checkpointing\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split into gradient checkpoint blocks\n",
    "        def run_block(x, start, end):\n",
    "            for i in range(start, end+1):\n",
    "                x = self.conv_layers[i](x)\n",
    "                x = self.activation(x)\n",
    "            return x\n",
    "\n",
    "        if not x.requires_grad:\n",
    "            x = x.detach().requires_grad_(True)\n",
    "        \n",
    "        # Checkpoint groups of layers\n",
    "        x = torch.utils.checkpoint.checkpoint(run_block, x, 0, 3, use_reentrant=False)  # Layers 0-3\n",
    "        x = torch.utils.checkpoint.checkpoint(run_block, x, 4, 7, use_reentrant=False)  # Layers 4-7\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGEmbeddings(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv1d(12, 512, kernel_size=(50,), stride=(1,), padding=same)\n",
      "    (1-7): 7 x Conv1d(512, 512, kernel_size=(50,), stride=(1,), padding=same)\n",
      "  )\n",
      "  (activation): ReLU()\n",
      ")\n",
      ": 92061696 parameters\n",
      "conv_layers: 92061696 parameters\n",
      "conv_layers.0: 307712 parameters\n",
      "conv_layers.1: 13107712 parameters\n",
      "conv_layers.2: 13107712 parameters\n",
      "conv_layers.3: 13107712 parameters\n",
      "conv_layers.4: 13107712 parameters\n",
      "conv_layers.5: 13107712 parameters\n",
      "conv_layers.6: 13107712 parameters\n",
      "conv_layers.7: 13107712 parameters\n",
      "activation: 0 parameters\n"
     ]
    }
   ],
   "source": [
    "embedding_model = ECGEmbeddings(d_input = 12, d_model=512)\n",
    "print(embedding_model)\n",
    "count_parameters(embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A possible better patch-based embedding model\n",
    "Attempt to extract each heartbeat as a \"word\", then create an embedding of that beat.\n",
    "\n",
    "Creating patches like the ViT architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the raw data, split into patches\n",
    "class Splitter(nn.Module):\n",
    "    def __init__(self, d_model, patch_size=50): # Try 50-100 ballpark\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        # It's conv function but really just breaking into patches\n",
    "        self.projection = nn.Conv1d(12, d_model, patch_size, stride=patch_size) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape input to split into patches\n",
    "        x = self.projection(x)\n",
    "        return x.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in the patches and adds the positional embeddings\n",
    "class PatchPositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_patches, d_model):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.d_model = d_model\n",
    "        self.pos_embedding = self.create_position_encoding().to(device)\n",
    "\n",
    "    def create_position_encoding(self):\n",
    "        pos = np.arange(self.num_patches).reshape(-1, 1)\n",
    "        dim = np.arange(self.d_model).reshape(1, -1)\n",
    "\n",
    "        angle_rates = 1/np.power(10000, (2*(dim//2))/np.float32(self.d_model))\n",
    "        position_encoding = pos*angle_rates  # Positional information\n",
    "        # Apply sine to even indices and cosine to odd indices\n",
    "        position_encoding[:, 0::2] = np.sin(position_encoding[:, 0::2])\n",
    "        position_encoding[:, 1::2] = np.cos(position_encoding[:, 1::2])\n",
    "        return torch.tensor(position_encoding, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining together embedding with transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGCombined(nn.Module):\n",
    "    def __init__(self, d_input, d_model, num_classes=63, nhead=8, num_encoder_layers=2, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_model = ECGEmbeddings(d_input, d_model)\n",
    "        self.transformer = ECGTransformer(d_model, num_classes, nhead, num_encoder_layers, dim_feedforward)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_model(x)\n",
    "        x = x.permute(0, 2, 1)       # Reshape to (batch_size, seq_len, d_model)\n",
    "        x = self.transformer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGPatchCombined(nn.Module):\n",
    "    def __init__(self, d_input, d_model, num_classes=63, nhead=8, num_encoder_layers=2, dim_feedforward=2048, num_patches=100):\n",
    "        super().__init__()\n",
    "        self.splitter = Splitter(d_model, patch_size=50)\n",
    "        self.patch_position = PatchPositionalEncoding(num_patches=num_patches, d_model=d_model)\n",
    "        self.transformer = ECGTransformer(d_model, num_classes, nhead, num_encoder_layers, dim_feedforward)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.splitter(x)\n",
    "        x = self.patch_position(x)   # Already (batch_size, seq_len, d_model)\n",
    "        x = self.transformer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, device, accum_steps=4, checkpoint_interval=100,\n",
    "                 resume_checkpoint=None):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.accum_steps = accum_steps\n",
    "        self.checkpoint_interval = checkpoint_interval\n",
    "        \n",
    "        # Initialize essential components first\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=1e-4,\n",
    "            betas=(0.9, 0.99),    # Increased second moment decay\n",
    "            weight_decay=1e-3,    # Stronger regularization\n",
    "            amsgrad=True\n",
    "        )\n",
    "\n",
    "        # Add scheduler\n",
    "        total_batches = len(train_dataloader) // accum_steps * 1\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer,\n",
    "            T_max=total_batches//4,  # Number of batches between restarts\n",
    "            eta_min=1e-6  # Minimum learning rate\n",
    "        )\n",
    "\n",
    "        self.loss_history = []\n",
    "        self.acc_history = []\n",
    "        self.batch_count = 0\n",
    "        self.start_epoch = 0\n",
    "        self.start_batch = 0\n",
    "\n",
    "        # Override with checkpoint if provided\n",
    "        if resume_checkpoint:\n",
    "            self._load_checkpoint(resume_checkpoint)\n",
    "\n",
    "    def _load_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"Load training state from checkpoint\"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        \n",
    "        # Essential parameters\n",
    "        self.model.load_state_dict(checkpoint['model_state'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state'])\n",
    "        self.scaler.load_state_dict(checkpoint['scaler_state'])\n",
    "        \n",
    "        # Training progress\n",
    "        self.loss_history = checkpoint['loss_history']\n",
    "        self.acc_history = checkpoint['acc_history']\n",
    "        self.batch_count = checkpoint.get('batch_count', 0)\n",
    "        self.start_epoch = checkpoint['epoch']  # Resume from same epoch\n",
    "        self.start_batch = checkpoint.get('batch', 0) + 1  # Next batch\n",
    "        \n",
    "        # Configurations (optional but recommended)\n",
    "        self.checkpoint_interval = checkpoint.get('checkpoint_interval', \n",
    "                                                 self.checkpoint_interval)\n",
    "        \n",
    "        print(f\"Resuming from epoch {self.start_epoch} batch {self.start_batch}\")\n",
    "\n",
    "    def loss(self, output, target):\n",
    "        return F.binary_cross_entropy_with_logits(output, target.float())\n",
    "\n",
    "    def train(self, train_dataloader, test_dataloader, num_epochs, save_path=\"training_progress\"):\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        self.model.train()\n",
    "        \n",
    "        # Start from the last checkpoint epoch\n",
    "        for epoch in range(self.start_epoch, num_epochs):\n",
    "            # Skip batches we've already processed in this epoch\n",
    "            for batch_idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "                if batch_idx < self.start_batch:\n",
    "                    continue\n",
    "                # Forward pass\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = self.loss(outputs, labels) / self.accum_steps\n",
    "\n",
    "                # Backward pass\n",
    "                self.scaler.scale(loss).backward()\n",
    "                \n",
    "                # Gradient accumulation\n",
    "                if (batch_idx + 1) % self.accum_steps == 0:\n",
    "                    self._update_parameters()\n",
    "                \n",
    "                # Logging and checkpointing\n",
    "                current_loss = loss.item() * self.accum_steps\n",
    "                self.loss_history.append(current_loss)\n",
    "                self.batch_count += 1\n",
    "\n",
    "                # Print every batch\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx+1}/{len(train_dataloader)} | \"\n",
    "                    f\"Loss: {current_loss:.4f} | LR: {self.optimizer.param_groups[0]['lr']:.3e}\")\n",
    "\n",
    "                # Save checkpoint\n",
    "                if self.batch_count % self.checkpoint_interval == 0:\n",
    "                    acc = self.evaluate(test_dataloader)\n",
    "                    self.acc_history.append([self.batch_count, acc])\n",
    "                    self._save_checkpoint(save_path, epoch, batch_idx)\n",
    "                \n",
    "                del inputs, labels, outputs, loss\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_mismatches = 0.0\n",
    "        total_labels = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                \n",
    "                # Get binary predictions (0 or 1) using threshold\n",
    "                predicted = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "                \n",
    "                # Calculate mismatches\n",
    "                mismatches = (predicted != labels).float().sum().item()\n",
    "                total_mismatches += mismatches\n",
    "                total_labels += labels.numel()  # Total labels = batch_size * num_classes\n",
    "\n",
    "        hamming_loss = total_mismatches / total_labels\n",
    "        print(f'Hamming Loss: {hamming_loss:.4f}')\n",
    "        return hamming_loss\n",
    "\n",
    "\n",
    "    def _update_parameters(self):\n",
    "        \"\"\"Update model parameters with gradient clipping\"\"\"\n",
    "        self.scaler.unscale_(self.optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5.0)\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        self.optimizer.zero_grad()\n",
    "        self.scheduler.step()\n",
    "\n",
    "    def _save_checkpoint(self, path, epoch, batch_idx):\n",
    "        \"\"\"Save model and training state\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'batch': batch_idx,\n",
    "            'batch_count': self.batch_count,\n",
    "            'checkpoint_interval': self.checkpoint_interval,\n",
    "            'model_state': self.model.state_dict(),\n",
    "            'optimizer_state': self.optimizer.state_dict(),\n",
    "            'loss_history': self.loss_history,\n",
    "            'acc_history': self.acc_history,\n",
    "            'scaler_state': self.scaler.state_dict(),\n",
    "            'scheduler_state': self.scheduler.state_dict()\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, f\"{path}/checkpoint_ep{epoch}_b{batch_idx}.pt\")\n",
    "        print(f\"\\nCheckpoint saved at epoch {epoch+1} batch {batch_idx+1}\")\n",
    "        \n",
    "        # Save loss history separately for easy plotting\n",
    "        np.save(f\"{path}/loss_history.npy\", np.array(self.loss_history))\n",
    "        np.save(f\"{path}/acc_history.npy\", np.array(self.acc_history))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Go Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_dataset = ECGDataset(transform=None)\n",
    "train_dataset, test_dataset, val_dataset = random_split(\n",
    "                                            ecg_dataset, [len(ecg_dataset) - 1000, 500, 500], \n",
    "                                            torch.Generator().manual_seed(42))\n",
    "\n",
    "batch_size = 4\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start from 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ECGCombined(d_input=12, d_model=128, num_classes=94, nhead=4, num_encoder_layers=2, dim_feedforward=256).to(device)\n",
    "trainer = Trainer(model, device, accum_steps=4)\n",
    "trainer.train(train_dataloader, test_dataloader, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74963/102161801.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Batch 1/11038 | Loss: 0.6977 | LR: 1.000e-04\n",
      "Epoch 1/1 | Batch 2/11038 | Loss: 0.6972 | LR: 1.000e-04\n",
      "Epoch 1/1 | Batch 3/11038 | Loss: 0.6963 | LR: 1.000e-04\n",
      "Epoch 1/1 | Batch 4/11038 | Loss: 0.6841 | LR: 1.000e-04\n",
      "Epoch 1/1 | Batch 5/11038 | Loss: 0.6737 | LR: 1.000e-04\n",
      "Epoch 1/1 | Batch 6/11038 | Loss: 0.6812 | LR: 1.000e-04\n",
      "Epoch 1/1 | Batch 7/11038 | Loss: 0.6845 | LR: 1.000e-04\n",
      "Epoch 1/1 | Batch 8/11038 | Loss: 0.6810 | LR: 1.000e-04\n",
      "Epoch 1/1 | Batch 9/11038 | Loss: 0.6680 | LR: 1.000e-04\n",
      "Epoch 1/1 | Batch 10/11038 | Loss: 0.6712 | LR: 1.000e-04\n",
      "Epoch 1/1 | Batch 11/11038 | Loss: 0.6689 | LR: 1.000e-04\n",
      "Epoch 1/1 | Batch 12/11038 | Loss: 0.6642 | LR: 1.000e-04\n",
      "Epoch 1/1 | Batch 13/11038 | Loss: 0.6495 | LR: 1.000e-04\n",
      "Epoch 1/1 | Batch 14/11038 | Loss: 0.6470 | LR: 1.000e-04\n",
      "Epoch 1/1 | Batch 15/11038 | Loss: 0.6509 | LR: 1.000e-04\n",
      "Epoch 1/1 | Batch 16/11038 | Loss: 0.6471 | LR: 9.999e-05\n",
      "Epoch 1/1 | Batch 17/11038 | Loss: 0.6505 | LR: 9.999e-05\n",
      "Epoch 1/1 | Batch 18/11038 | Loss: 0.6319 | LR: 9.999e-05\n",
      "Epoch 1/1 | Batch 19/11038 | Loss: 0.6357 | LR: 9.999e-05\n",
      "Epoch 1/1 | Batch 20/11038 | Loss: 0.6292 | LR: 9.999e-05\n",
      "Epoch 1/1 | Batch 21/11038 | Loss: 0.6231 | LR: 9.999e-05\n",
      "Epoch 1/1 | Batch 22/11038 | Loss: 0.6251 | LR: 9.999e-05\n",
      "Epoch 1/1 | Batch 23/11038 | Loss: 0.6204 | LR: 9.999e-05\n",
      "Epoch 1/1 | Batch 24/11038 | Loss: 0.6181 | LR: 9.998e-05\n",
      "Epoch 1/1 | Batch 25/11038 | Loss: 0.6132 | LR: 9.998e-05\n",
      "Epoch 1/1 | Batch 26/11038 | Loss: 0.6124 | LR: 9.998e-05\n",
      "Epoch 1/1 | Batch 27/11038 | Loss: 0.6196 | LR: 9.998e-05\n",
      "Epoch 1/1 | Batch 28/11038 | Loss: 0.6064 | LR: 9.997e-05\n",
      "Epoch 1/1 | Batch 29/11038 | Loss: 0.5851 | LR: 9.997e-05\n",
      "Epoch 1/1 | Batch 30/11038 | Loss: 0.5987 | LR: 9.997e-05\n",
      "Epoch 1/1 | Batch 31/11038 | Loss: 0.5952 | LR: 9.997e-05\n",
      "Epoch 1/1 | Batch 32/11038 | Loss: 0.5981 | LR: 9.997e-05\n",
      "Epoch 1/1 | Batch 33/11038 | Loss: 0.5862 | LR: 9.997e-05\n",
      "Epoch 1/1 | Batch 34/11038 | Loss: 0.5821 | LR: 9.997e-05\n",
      "Epoch 1/1 | Batch 35/11038 | Loss: 0.5854 | LR: 9.997e-05\n",
      "Epoch 1/1 | Batch 36/11038 | Loss: 0.5874 | LR: 9.996e-05\n",
      "Epoch 1/1 | Batch 37/11038 | Loss: 0.5736 | LR: 9.996e-05\n",
      "Epoch 1/1 | Batch 38/11038 | Loss: 0.5563 | LR: 9.996e-05\n",
      "Epoch 1/1 | Batch 39/11038 | Loss: 0.5692 | LR: 9.996e-05\n",
      "Epoch 1/1 | Batch 40/11038 | Loss: 0.5611 | LR: 9.995e-05\n",
      "Epoch 1/1 | Batch 41/11038 | Loss: 0.5495 | LR: 9.995e-05\n",
      "Epoch 1/1 | Batch 42/11038 | Loss: 0.5614 | LR: 9.995e-05\n",
      "Epoch 1/1 | Batch 43/11038 | Loss: 0.5624 | LR: 9.995e-05\n",
      "Epoch 1/1 | Batch 44/11038 | Loss: 0.5553 | LR: 9.994e-05\n",
      "Epoch 1/1 | Batch 45/11038 | Loss: 0.5473 | LR: 9.994e-05\n",
      "Epoch 1/1 | Batch 46/11038 | Loss: 0.5497 | LR: 9.994e-05\n",
      "Epoch 1/1 | Batch 47/11038 | Loss: 0.5446 | LR: 9.994e-05\n",
      "Epoch 1/1 | Batch 48/11038 | Loss: 0.5441 | LR: 9.993e-05\n",
      "Epoch 1/1 | Batch 49/11038 | Loss: 0.5428 | LR: 9.993e-05\n",
      "Epoch 1/1 | Batch 50/11038 | Loss: 0.5407 | LR: 9.993e-05\n",
      "Epoch 1/1 | Batch 51/11038 | Loss: 0.5329 | LR: 9.993e-05\n",
      "Epoch 1/1 | Batch 52/11038 | Loss: 0.5358 | LR: 9.991e-05\n",
      "Epoch 1/1 | Batch 53/11038 | Loss: 0.5178 | LR: 9.991e-05\n",
      "Epoch 1/1 | Batch 54/11038 | Loss: 0.5159 | LR: 9.991e-05\n",
      "Epoch 1/1 | Batch 55/11038 | Loss: 0.5208 | LR: 9.991e-05\n",
      "Epoch 1/1 | Batch 56/11038 | Loss: 0.5274 | LR: 9.990e-05\n",
      "Epoch 1/1 | Batch 57/11038 | Loss: 0.5048 | LR: 9.990e-05\n",
      "Epoch 1/1 | Batch 58/11038 | Loss: 0.5236 | LR: 9.990e-05\n",
      "Epoch 1/1 | Batch 59/11038 | Loss: 0.5270 | LR: 9.990e-05\n",
      "Epoch 1/1 | Batch 60/11038 | Loss: 0.5015 | LR: 9.988e-05\n",
      "Epoch 1/1 | Batch 61/11038 | Loss: 0.5025 | LR: 9.988e-05\n",
      "Epoch 1/1 | Batch 62/11038 | Loss: 0.5168 | LR: 9.988e-05\n",
      "Epoch 1/1 | Batch 63/11038 | Loss: 0.5005 | LR: 9.988e-05\n",
      "Epoch 1/1 | Batch 64/11038 | Loss: 0.4983 | LR: 9.987e-05\n",
      "Epoch 1/1 | Batch 65/11038 | Loss: 0.4955 | LR: 9.987e-05\n",
      "Epoch 1/1 | Batch 66/11038 | Loss: 0.5045 | LR: 9.987e-05\n",
      "Epoch 1/1 | Batch 67/11038 | Loss: 0.4938 | LR: 9.987e-05\n",
      "Epoch 1/1 | Batch 68/11038 | Loss: 0.5071 | LR: 9.985e-05\n",
      "Epoch 1/1 | Batch 69/11038 | Loss: 0.4822 | LR: 9.985e-05\n",
      "Epoch 1/1 | Batch 70/11038 | Loss: 0.4896 | LR: 9.985e-05\n",
      "Epoch 1/1 | Batch 71/11038 | Loss: 0.4831 | LR: 9.985e-05\n",
      "Epoch 1/1 | Batch 72/11038 | Loss: 0.4926 | LR: 9.983e-05\n",
      "Epoch 1/1 | Batch 73/11038 | Loss: 0.4811 | LR: 9.983e-05\n",
      "Epoch 1/1 | Batch 74/11038 | Loss: 0.4740 | LR: 9.983e-05\n",
      "Epoch 1/1 | Batch 75/11038 | Loss: 0.4717 | LR: 9.983e-05\n",
      "Epoch 1/1 | Batch 76/11038 | Loss: 0.4937 | LR: 9.981e-05\n",
      "Epoch 1/1 | Batch 77/11038 | Loss: 0.4709 | LR: 9.981e-05\n",
      "Epoch 1/1 | Batch 78/11038 | Loss: 0.4907 | LR: 9.981e-05\n",
      "Epoch 1/1 | Batch 79/11038 | Loss: 0.4724 | LR: 9.981e-05\n",
      "Epoch 1/1 | Batch 80/11038 | Loss: 0.4802 | LR: 9.979e-05\n",
      "Epoch 1/1 | Batch 81/11038 | Loss: 0.4695 | LR: 9.979e-05\n",
      "Epoch 1/1 | Batch 82/11038 | Loss: 0.4723 | LR: 9.979e-05\n",
      "Epoch 1/1 | Batch 83/11038 | Loss: 0.4891 | LR: 9.979e-05\n",
      "Epoch 1/1 | Batch 84/11038 | Loss: 0.4745 | LR: 9.977e-05\n",
      "Epoch 1/1 | Batch 85/11038 | Loss: 0.4570 | LR: 9.977e-05\n",
      "Epoch 1/1 | Batch 86/11038 | Loss: 0.4660 | LR: 9.977e-05\n",
      "Epoch 1/1 | Batch 87/11038 | Loss: 0.4529 | LR: 9.977e-05\n",
      "Epoch 1/1 | Batch 88/11038 | Loss: 0.4711 | LR: 9.975e-05\n",
      "Epoch 1/1 | Batch 89/11038 | Loss: 0.4583 | LR: 9.975e-05\n",
      "Epoch 1/1 | Batch 90/11038 | Loss: 0.4473 | LR: 9.975e-05\n",
      "Epoch 1/1 | Batch 91/11038 | Loss: 0.4502 | LR: 9.975e-05\n",
      "Epoch 1/1 | Batch 92/11038 | Loss: 0.4608 | LR: 9.973e-05\n",
      "Epoch 1/1 | Batch 93/11038 | Loss: 0.4567 | LR: 9.973e-05\n",
      "Epoch 1/1 | Batch 94/11038 | Loss: 0.4456 | LR: 9.973e-05\n",
      "Epoch 1/1 | Batch 95/11038 | Loss: 0.4461 | LR: 9.973e-05\n",
      "Epoch 1/1 | Batch 96/11038 | Loss: 0.4429 | LR: 9.970e-05\n",
      "Epoch 1/1 | Batch 97/11038 | Loss: 0.4541 | LR: 9.970e-05\n",
      "Epoch 1/1 | Batch 98/11038 | Loss: 0.4438 | LR: 9.970e-05\n",
      "Epoch 1/1 | Batch 99/11038 | Loss: 0.4397 | LR: 9.970e-05\n",
      "Epoch 1/1 | Batch 100/11038 | Loss: 0.4408 | LR: 9.968e-05\n",
      "Hamming Loss: 0.0977\n",
      "\n",
      "Checkpoint saved at epoch 1 batch 100\n",
      "Epoch 1/1 | Batch 101/11038 | Loss: 0.4348 | LR: 9.968e-05\n",
      "Epoch 1/1 | Batch 102/11038 | Loss: 0.4298 | LR: 9.968e-05\n",
      "Epoch 1/1 | Batch 103/11038 | Loss: 0.4322 | LR: 9.968e-05\n",
      "Epoch 1/1 | Batch 104/11038 | Loss: 0.4266 | LR: 9.965e-05\n",
      "Epoch 1/1 | Batch 105/11038 | Loss: 0.4182 | LR: 9.965e-05\n",
      "Epoch 1/1 | Batch 106/11038 | Loss: 0.4208 | LR: 9.965e-05\n",
      "Epoch 1/1 | Batch 107/11038 | Loss: 0.4173 | LR: 9.965e-05\n",
      "Epoch 1/1 | Batch 108/11038 | Loss: 0.4235 | LR: 9.963e-05\n",
      "Epoch 1/1 | Batch 109/11038 | Loss: 0.4199 | LR: 9.963e-05\n",
      "Epoch 1/1 | Batch 110/11038 | Loss: 0.4244 | LR: 9.963e-05\n",
      "Epoch 1/1 | Batch 111/11038 | Loss: 0.4134 | LR: 9.963e-05\n",
      "Epoch 1/1 | Batch 112/11038 | Loss: 0.4201 | LR: 9.960e-05\n",
      "Epoch 1/1 | Batch 113/11038 | Loss: 0.4228 | LR: 9.960e-05\n",
      "Epoch 1/1 | Batch 114/11038 | Loss: 0.4168 | LR: 9.960e-05\n",
      "Epoch 1/1 | Batch 115/11038 | Loss: 0.4097 | LR: 9.960e-05\n",
      "Epoch 1/1 | Batch 116/11038 | Loss: 0.4103 | LR: 9.957e-05\n",
      "Epoch 1/1 | Batch 117/11038 | Loss: 0.4140 | LR: 9.957e-05\n",
      "Epoch 1/1 | Batch 118/11038 | Loss: 0.4068 | LR: 9.957e-05\n",
      "Epoch 1/1 | Batch 119/11038 | Loss: 0.4284 | LR: 9.957e-05\n",
      "Epoch 1/1 | Batch 120/11038 | Loss: 0.4019 | LR: 9.954e-05\n",
      "Epoch 1/1 | Batch 121/11038 | Loss: 0.4053 | LR: 9.954e-05\n",
      "Epoch 1/1 | Batch 122/11038 | Loss: 0.4021 | LR: 9.954e-05\n",
      "Epoch 1/1 | Batch 123/11038 | Loss: 0.3957 | LR: 9.954e-05\n",
      "Epoch 1/1 | Batch 124/11038 | Loss: 0.4215 | LR: 9.951e-05\n",
      "Epoch 1/1 | Batch 125/11038 | Loss: 0.4082 | LR: 9.951e-05\n",
      "Epoch 1/1 | Batch 126/11038 | Loss: 0.4030 | LR: 9.951e-05\n",
      "Epoch 1/1 | Batch 127/11038 | Loss: 0.3973 | LR: 9.951e-05\n",
      "Epoch 1/1 | Batch 128/11038 | Loss: 0.3933 | LR: 9.947e-05\n",
      "Epoch 1/1 | Batch 129/11038 | Loss: 0.4016 | LR: 9.947e-05\n",
      "Epoch 1/1 | Batch 130/11038 | Loss: 0.3952 | LR: 9.947e-05\n",
      "Epoch 1/1 | Batch 131/11038 | Loss: 0.3951 | LR: 9.947e-05\n",
      "Epoch 1/1 | Batch 132/11038 | Loss: 0.3936 | LR: 9.944e-05\n",
      "Epoch 1/1 | Batch 133/11038 | Loss: 0.3871 | LR: 9.944e-05\n",
      "Epoch 1/1 | Batch 134/11038 | Loss: 0.3981 | LR: 9.944e-05\n",
      "Epoch 1/1 | Batch 135/11038 | Loss: 0.3890 | LR: 9.944e-05\n",
      "Epoch 1/1 | Batch 136/11038 | Loss: 0.3941 | LR: 9.941e-05\n",
      "Epoch 1/1 | Batch 137/11038 | Loss: 0.3911 | LR: 9.941e-05\n",
      "Epoch 1/1 | Batch 138/11038 | Loss: 0.3790 | LR: 9.941e-05\n",
      "Epoch 1/1 | Batch 139/11038 | Loss: 0.3910 | LR: 9.941e-05\n",
      "Epoch 1/1 | Batch 140/11038 | Loss: 0.3856 | LR: 9.937e-05\n",
      "Epoch 1/1 | Batch 141/11038 | Loss: 0.3926 | LR: 9.937e-05\n",
      "Epoch 1/1 | Batch 142/11038 | Loss: 0.3867 | LR: 9.937e-05\n",
      "Epoch 1/1 | Batch 143/11038 | Loss: 0.3831 | LR: 9.937e-05\n",
      "Epoch 1/1 | Batch 144/11038 | Loss: 0.3920 | LR: 9.933e-05\n",
      "Epoch 1/1 | Batch 145/11038 | Loss: 0.3865 | LR: 9.933e-05\n",
      "Epoch 1/1 | Batch 146/11038 | Loss: 0.3754 | LR: 9.933e-05\n",
      "Epoch 1/1 | Batch 147/11038 | Loss: 0.3778 | LR: 9.933e-05\n",
      "Epoch 1/1 | Batch 148/11038 | Loss: 0.3783 | LR: 9.930e-05\n",
      "Epoch 1/1 | Batch 149/11038 | Loss: 0.3823 | LR: 9.930e-05\n",
      "Epoch 1/1 | Batch 150/11038 | Loss: 0.3643 | LR: 9.930e-05\n",
      "Epoch 1/1 | Batch 151/11038 | Loss: 0.3824 | LR: 9.930e-05\n",
      "Epoch 1/1 | Batch 152/11038 | Loss: 0.3731 | LR: 9.926e-05\n",
      "Epoch 1/1 | Batch 153/11038 | Loss: 0.3674 | LR: 9.926e-05\n",
      "Epoch 1/1 | Batch 154/11038 | Loss: 0.3712 | LR: 9.926e-05\n",
      "Epoch 1/1 | Batch 155/11038 | Loss: 0.3712 | LR: 9.926e-05\n",
      "Epoch 1/1 | Batch 156/11038 | Loss: 0.3722 | LR: 9.922e-05\n",
      "Epoch 1/1 | Batch 157/11038 | Loss: 0.3846 | LR: 9.922e-05\n",
      "Epoch 1/1 | Batch 158/11038 | Loss: 0.3632 | LR: 9.922e-05\n",
      "Epoch 1/1 | Batch 159/11038 | Loss: 0.3631 | LR: 9.922e-05\n",
      "Epoch 1/1 | Batch 160/11038 | Loss: 0.3610 | LR: 9.918e-05\n",
      "Epoch 1/1 | Batch 161/11038 | Loss: 0.3550 | LR: 9.918e-05\n",
      "Epoch 1/1 | Batch 162/11038 | Loss: 0.3617 | LR: 9.918e-05\n",
      "Epoch 1/1 | Batch 163/11038 | Loss: 0.3566 | LR: 9.918e-05\n",
      "Epoch 1/1 | Batch 164/11038 | Loss: 0.3589 | LR: 9.914e-05\n",
      "Epoch 1/1 | Batch 165/11038 | Loss: 0.3661 | LR: 9.914e-05\n",
      "Epoch 1/1 | Batch 166/11038 | Loss: 0.3558 | LR: 9.914e-05\n",
      "Epoch 1/1 | Batch 167/11038 | Loss: 0.3543 | LR: 9.914e-05\n",
      "Epoch 1/1 | Batch 168/11038 | Loss: 0.3532 | LR: 9.910e-05\n",
      "Epoch 1/1 | Batch 169/11038 | Loss: 0.3552 | LR: 9.910e-05\n",
      "Epoch 1/1 | Batch 170/11038 | Loss: 0.3596 | LR: 9.910e-05\n",
      "Epoch 1/1 | Batch 171/11038 | Loss: 0.3664 | LR: 9.910e-05\n",
      "Epoch 1/1 | Batch 172/11038 | Loss: 0.3697 | LR: 9.905e-05\n",
      "Epoch 1/1 | Batch 173/11038 | Loss: 0.3536 | LR: 9.905e-05\n",
      "Epoch 1/1 | Batch 174/11038 | Loss: 0.3489 | LR: 9.905e-05\n",
      "Epoch 1/1 | Batch 175/11038 | Loss: 0.3501 | LR: 9.905e-05\n",
      "Epoch 1/1 | Batch 176/11038 | Loss: nan | LR: 9.901e-05\n",
      "Epoch 1/1 | Batch 177/11038 | Loss: 0.3549 | LR: 9.901e-05\n",
      "Epoch 1/1 | Batch 178/11038 | Loss: 0.3507 | LR: 9.901e-05\n",
      "Epoch 1/1 | Batch 179/11038 | Loss: 0.3501 | LR: 9.901e-05\n",
      "Epoch 1/1 | Batch 180/11038 | Loss: 0.3560 | LR: 9.896e-05\n",
      "Epoch 1/1 | Batch 181/11038 | Loss: 0.3480 | LR: 9.896e-05\n",
      "Epoch 1/1 | Batch 182/11038 | Loss: 0.3503 | LR: 9.896e-05\n",
      "Epoch 1/1 | Batch 183/11038 | Loss: 0.3497 | LR: 9.896e-05\n",
      "Epoch 1/1 | Batch 184/11038 | Loss: 0.3643 | LR: 9.892e-05\n",
      "Epoch 1/1 | Batch 185/11038 | Loss: 0.3444 | LR: 9.892e-05\n",
      "Epoch 1/1 | Batch 186/11038 | Loss: 0.3477 | LR: 9.892e-05\n",
      "Epoch 1/1 | Batch 187/11038 | Loss: 0.3512 | LR: 9.892e-05\n",
      "Epoch 1/1 | Batch 188/11038 | Loss: 0.3390 | LR: 9.887e-05\n",
      "Epoch 1/1 | Batch 189/11038 | Loss: 0.3345 | LR: 9.887e-05\n",
      "Epoch 1/1 | Batch 190/11038 | Loss: 0.3319 | LR: 9.887e-05\n",
      "Epoch 1/1 | Batch 191/11038 | Loss: 0.3365 | LR: 9.887e-05\n",
      "Epoch 1/1 | Batch 192/11038 | Loss: 0.3410 | LR: 9.882e-05\n",
      "Epoch 1/1 | Batch 193/11038 | Loss: 0.3381 | LR: 9.882e-05\n",
      "Epoch 1/1 | Batch 194/11038 | Loss: 0.3317 | LR: 9.882e-05\n",
      "Epoch 1/1 | Batch 195/11038 | Loss: 0.3317 | LR: 9.882e-05\n",
      "Epoch 1/1 | Batch 196/11038 | Loss: 0.3412 | LR: 9.877e-05\n",
      "Epoch 1/1 | Batch 197/11038 | Loss: 0.3439 | LR: 9.877e-05\n",
      "Epoch 1/1 | Batch 198/11038 | Loss: 0.3466 | LR: 9.877e-05\n",
      "Epoch 1/1 | Batch 199/11038 | Loss: 0.3453 | LR: 9.877e-05\n",
      "Epoch 1/1 | Batch 200/11038 | Loss: 0.3319 | LR: 9.872e-05\n",
      "Hamming Loss: 0.0307\n",
      "\n",
      "Checkpoint saved at epoch 1 batch 200\n",
      "Epoch 1/1 | Batch 201/11038 | Loss: 0.3329 | LR: 9.872e-05\n",
      "Epoch 1/1 | Batch 202/11038 | Loss: 0.3232 | LR: 9.872e-05\n",
      "Epoch 1/1 | Batch 203/11038 | Loss: 0.3246 | LR: 9.872e-05\n",
      "Epoch 1/1 | Batch 204/11038 | Loss: 0.3404 | LR: 9.867e-05\n",
      "Epoch 1/1 | Batch 205/11038 | Loss: 0.3503 | LR: 9.867e-05\n",
      "Epoch 1/1 | Batch 206/11038 | Loss: 0.3311 | LR: 9.867e-05\n",
      "Epoch 1/1 | Batch 207/11038 | Loss: 0.3360 | LR: 9.867e-05\n",
      "Epoch 1/1 | Batch 208/11038 | Loss: 0.3357 | LR: 9.862e-05\n",
      "Epoch 1/1 | Batch 209/11038 | Loss: 0.3373 | LR: 9.862e-05\n",
      "Epoch 1/1 | Batch 210/11038 | Loss: 0.3213 | LR: 9.862e-05\n",
      "Epoch 1/1 | Batch 211/11038 | Loss: 0.3286 | LR: 9.862e-05\n",
      "Epoch 1/1 | Batch 212/11038 | Loss: 0.3223 | LR: 9.856e-05\n",
      "Epoch 1/1 | Batch 213/11038 | Loss: 0.3297 | LR: 9.856e-05\n",
      "Epoch 1/1 | Batch 214/11038 | Loss: 0.3403 | LR: 9.856e-05\n",
      "Epoch 1/1 | Batch 215/11038 | Loss: nan | LR: 9.856e-05\n",
      "Epoch 1/1 | Batch 216/11038 | Loss: 0.3304 | LR: 9.851e-05\n",
      "Epoch 1/1 | Batch 217/11038 | Loss: 0.3344 | LR: 9.851e-05\n",
      "Epoch 1/1 | Batch 218/11038 | Loss: 0.3384 | LR: 9.851e-05\n",
      "Epoch 1/1 | Batch 219/11038 | Loss: 0.3253 | LR: 9.851e-05\n",
      "Epoch 1/1 | Batch 220/11038 | Loss: 0.3238 | LR: 9.845e-05\n",
      "Epoch 1/1 | Batch 221/11038 | Loss: 0.3184 | LR: 9.845e-05\n",
      "Epoch 1/1 | Batch 222/11038 | Loss: 0.3253 | LR: 9.845e-05\n",
      "Epoch 1/1 | Batch 223/11038 | Loss: 0.3162 | LR: 9.845e-05\n",
      "Epoch 1/1 | Batch 224/11038 | Loss: 0.3219 | LR: 9.840e-05\n",
      "Epoch 1/1 | Batch 225/11038 | Loss: 0.3221 | LR: 9.840e-05\n",
      "Epoch 1/1 | Batch 226/11038 | Loss: 0.3445 | LR: 9.840e-05\n",
      "Epoch 1/1 | Batch 227/11038 | Loss: 0.3357 | LR: 9.840e-05\n",
      "Epoch 1/1 | Batch 228/11038 | Loss: 0.3308 | LR: 9.834e-05\n",
      "Epoch 1/1 | Batch 229/11038 | Loss: 0.3176 | LR: 9.834e-05\n",
      "Epoch 1/1 | Batch 230/11038 | Loss: 0.3174 | LR: 9.834e-05\n",
      "Epoch 1/1 | Batch 231/11038 | Loss: 0.3212 | LR: 9.834e-05\n",
      "Epoch 1/1 | Batch 232/11038 | Loss: 0.3121 | LR: 9.828e-05\n",
      "Epoch 1/1 | Batch 233/11038 | Loss: 0.3332 | LR: 9.828e-05\n",
      "Epoch 1/1 | Batch 234/11038 | Loss: 0.3214 | LR: 9.828e-05\n",
      "Epoch 1/1 | Batch 235/11038 | Loss: 0.3232 | LR: 9.828e-05\n",
      "Epoch 1/1 | Batch 236/11038 | Loss: 0.3175 | LR: 9.822e-05\n",
      "Epoch 1/1 | Batch 237/11038 | Loss: 0.3164 | LR: 9.822e-05\n",
      "Epoch 1/1 | Batch 238/11038 | Loss: 0.3188 | LR: 9.822e-05\n",
      "Epoch 1/1 | Batch 239/11038 | Loss: 0.3053 | LR: 9.822e-05\n",
      "Epoch 1/1 | Batch 240/11038 | Loss: 0.3143 | LR: 9.816e-05\n",
      "Epoch 1/1 | Batch 241/11038 | Loss: 0.3153 | LR: 9.816e-05\n",
      "Epoch 1/1 | Batch 242/11038 | Loss: 0.3300 | LR: 9.816e-05\n",
      "Epoch 1/1 | Batch 243/11038 | Loss: 0.3134 | LR: 9.816e-05\n",
      "Epoch 1/1 | Batch 244/11038 | Loss: 0.3387 | LR: 9.810e-05\n",
      "Epoch 1/1 | Batch 245/11038 | Loss: 0.3339 | LR: 9.810e-05\n",
      "Epoch 1/1 | Batch 246/11038 | Loss: 0.3056 | LR: 9.810e-05\n",
      "Epoch 1/1 | Batch 247/11038 | Loss: 0.3031 | LR: 9.810e-05\n",
      "Epoch 1/1 | Batch 248/11038 | Loss: 0.3137 | LR: 9.804e-05\n",
      "Epoch 1/1 | Batch 249/11038 | Loss: 0.2999 | LR: 9.804e-05\n",
      "Epoch 1/1 | Batch 250/11038 | Loss: 0.3033 | LR: 9.804e-05\n",
      "Epoch 1/1 | Batch 251/11038 | Loss: 0.3024 | LR: 9.804e-05\n",
      "Epoch 1/1 | Batch 252/11038 | Loss: 0.3185 | LR: 9.797e-05\n",
      "Epoch 1/1 | Batch 253/11038 | Loss: 0.2984 | LR: 9.797e-05\n",
      "Epoch 1/1 | Batch 254/11038 | Loss: 0.3156 | LR: 9.797e-05\n",
      "Epoch 1/1 | Batch 255/11038 | Loss: 0.3135 | LR: 9.797e-05\n",
      "Epoch 1/1 | Batch 256/11038 | Loss: 0.3003 | LR: 9.791e-05\n",
      "Epoch 1/1 | Batch 257/11038 | Loss: 0.2930 | LR: 9.791e-05\n",
      "Epoch 1/1 | Batch 258/11038 | Loss: 0.3133 | LR: 9.791e-05\n",
      "Epoch 1/1 | Batch 259/11038 | Loss: 0.2955 | LR: 9.791e-05\n",
      "Epoch 1/1 | Batch 260/11038 | Loss: 0.3011 | LR: 9.784e-05\n",
      "Epoch 1/1 | Batch 261/11038 | Loss: 0.3297 | LR: 9.784e-05\n",
      "Epoch 1/1 | Batch 262/11038 | Loss: 0.2935 | LR: 9.784e-05\n",
      "Epoch 1/1 | Batch 263/11038 | Loss: 0.2939 | LR: 9.784e-05\n",
      "Epoch 1/1 | Batch 264/11038 | Loss: 0.2994 | LR: 9.778e-05\n",
      "Epoch 1/1 | Batch 265/11038 | Loss: 0.2945 | LR: 9.778e-05\n",
      "Epoch 1/1 | Batch 266/11038 | Loss: 0.3044 | LR: 9.778e-05\n",
      "Epoch 1/1 | Batch 267/11038 | Loss: 0.2994 | LR: 9.778e-05\n",
      "Epoch 1/1 | Batch 268/11038 | Loss: 0.2913 | LR: 9.771e-05\n",
      "Epoch 1/1 | Batch 269/11038 | Loss: 0.3108 | LR: 9.771e-05\n",
      "Epoch 1/1 | Batch 270/11038 | Loss: 0.3024 | LR: 9.771e-05\n",
      "Epoch 1/1 | Batch 271/11038 | Loss: 0.2859 | LR: 9.771e-05\n",
      "Epoch 1/1 | Batch 272/11038 | Loss: 0.2894 | LR: 9.764e-05\n",
      "Epoch 1/1 | Batch 273/11038 | Loss: 0.2967 | LR: 9.764e-05\n",
      "Epoch 1/1 | Batch 274/11038 | Loss: 0.3081 | LR: 9.764e-05\n",
      "Epoch 1/1 | Batch 275/11038 | Loss: 0.3067 | LR: 9.764e-05\n",
      "Epoch 1/1 | Batch 276/11038 | Loss: 0.2897 | LR: 9.757e-05\n",
      "Epoch 1/1 | Batch 277/11038 | Loss: 0.3094 | LR: 9.757e-05\n",
      "Epoch 1/1 | Batch 278/11038 | Loss: 0.3002 | LR: 9.757e-05\n",
      "Epoch 1/1 | Batch 279/11038 | Loss: 0.2947 | LR: 9.757e-05\n",
      "Epoch 1/1 | Batch 280/11038 | Loss: 0.2935 | LR: 9.750e-05\n",
      "Epoch 1/1 | Batch 281/11038 | Loss: 0.2860 | LR: 9.750e-05\n",
      "Epoch 1/1 | Batch 282/11038 | Loss: 0.3082 | LR: 9.750e-05\n",
      "Epoch 1/1 | Batch 283/11038 | Loss: 0.2861 | LR: 9.750e-05\n",
      "Epoch 1/1 | Batch 284/11038 | Loss: 0.3066 | LR: 9.743e-05\n",
      "Epoch 1/1 | Batch 285/11038 | Loss: 0.2817 | LR: 9.743e-05\n",
      "Epoch 1/1 | Batch 286/11038 | Loss: 0.2973 | LR: 9.743e-05\n",
      "Epoch 1/1 | Batch 287/11038 | Loss: 0.2864 | LR: 9.743e-05\n",
      "Epoch 1/1 | Batch 288/11038 | Loss: 0.2855 | LR: 9.736e-05\n",
      "Epoch 1/1 | Batch 289/11038 | Loss: 0.2756 | LR: 9.736e-05\n",
      "Epoch 1/1 | Batch 290/11038 | Loss: 0.2940 | LR: 9.736e-05\n",
      "Epoch 1/1 | Batch 291/11038 | Loss: 0.2890 | LR: 9.736e-05\n",
      "Epoch 1/1 | Batch 292/11038 | Loss: 0.2808 | LR: 9.728e-05\n",
      "Epoch 1/1 | Batch 293/11038 | Loss: 0.2840 | LR: 9.728e-05\n",
      "Epoch 1/1 | Batch 294/11038 | Loss: 0.3070 | LR: 9.728e-05\n",
      "Epoch 1/1 | Batch 295/11038 | Loss: 0.2783 | LR: 9.728e-05\n",
      "Epoch 1/1 | Batch 296/11038 | Loss: 0.2822 | LR: 9.721e-05\n",
      "Epoch 1/1 | Batch 297/11038 | Loss: 0.2879 | LR: 9.721e-05\n",
      "Epoch 1/1 | Batch 298/11038 | Loss: 0.2777 | LR: 9.721e-05\n",
      "Epoch 1/1 | Batch 299/11038 | Loss: 0.2770 | LR: 9.721e-05\n",
      "Epoch 1/1 | Batch 300/11038 | Loss: 0.2811 | LR: 9.713e-05\n",
      "Hamming Loss: 0.0296\n",
      "\n",
      "Checkpoint saved at epoch 1 batch 300\n",
      "Epoch 1/1 | Batch 301/11038 | Loss: 0.2849 | LR: 9.713e-05\n",
      "Epoch 1/1 | Batch 302/11038 | Loss: 0.2772 | LR: 9.713e-05\n",
      "Epoch 1/1 | Batch 303/11038 | Loss: 0.2803 | LR: 9.713e-05\n",
      "Epoch 1/1 | Batch 304/11038 | Loss: 0.2675 | LR: 9.706e-05\n",
      "Epoch 1/1 | Batch 305/11038 | Loss: 0.2784 | LR: 9.706e-05\n",
      "Epoch 1/1 | Batch 306/11038 | Loss: 0.2995 | LR: 9.706e-05\n",
      "Epoch 1/1 | Batch 307/11038 | Loss: 0.2965 | LR: 9.706e-05\n",
      "Epoch 1/1 | Batch 308/11038 | Loss: 0.2782 | LR: 9.698e-05\n",
      "Epoch 1/1 | Batch 309/11038 | Loss: 0.2939 | LR: 9.698e-05\n",
      "Epoch 1/1 | Batch 310/11038 | Loss: 0.2680 | LR: 9.698e-05\n",
      "Epoch 1/1 | Batch 311/11038 | Loss: nan | LR: 9.698e-05\n",
      "Epoch 1/1 | Batch 312/11038 | Loss: 0.2820 | LR: 9.690e-05\n",
      "Epoch 1/1 | Batch 313/11038 | Loss: 0.2749 | LR: 9.690e-05\n",
      "Epoch 1/1 | Batch 314/11038 | Loss: 0.2797 | LR: 9.690e-05\n",
      "Epoch 1/1 | Batch 315/11038 | Loss: 0.2757 | LR: 9.690e-05\n",
      "Epoch 1/1 | Batch 316/11038 | Loss: 0.2754 | LR: 9.682e-05\n",
      "Epoch 1/1 | Batch 317/11038 | Loss: 0.2727 | LR: 9.682e-05\n",
      "Epoch 1/1 | Batch 318/11038 | Loss: 0.2865 | LR: 9.682e-05\n",
      "Epoch 1/1 | Batch 319/11038 | Loss: 0.2723 | LR: 9.682e-05\n",
      "Epoch 1/1 | Batch 320/11038 | Loss: 0.2842 | LR: 9.674e-05\n",
      "Epoch 1/1 | Batch 321/11038 | Loss: 0.2684 | LR: 9.674e-05\n",
      "Epoch 1/1 | Batch 322/11038 | Loss: 0.2692 | LR: 9.674e-05\n",
      "Epoch 1/1 | Batch 323/11038 | Loss: 0.2688 | LR: 9.674e-05\n",
      "Epoch 1/1 | Batch 324/11038 | Loss: 0.2720 | LR: 9.666e-05\n",
      "Epoch 1/1 | Batch 325/11038 | Loss: 0.2790 | LR: 9.666e-05\n",
      "Epoch 1/1 | Batch 326/11038 | Loss: 0.2806 | LR: 9.666e-05\n",
      "Epoch 1/1 | Batch 327/11038 | Loss: 0.2730 | LR: 9.666e-05\n",
      "Epoch 1/1 | Batch 328/11038 | Loss: 0.2870 | LR: 9.658e-05\n",
      "Epoch 1/1 | Batch 329/11038 | Loss: 0.2581 | LR: 9.658e-05\n",
      "Epoch 1/1 | Batch 330/11038 | Loss: 0.2638 | LR: 9.658e-05\n",
      "Epoch 1/1 | Batch 331/11038 | Loss: 0.2719 | LR: 9.658e-05\n",
      "Epoch 1/1 | Batch 332/11038 | Loss: 0.2679 | LR: 9.650e-05\n",
      "Epoch 1/1 | Batch 333/11038 | Loss: 0.2689 | LR: 9.650e-05\n",
      "Epoch 1/1 | Batch 334/11038 | Loss: 0.2674 | LR: 9.650e-05\n",
      "Epoch 1/1 | Batch 335/11038 | Loss: 0.2826 | LR: 9.650e-05\n",
      "Epoch 1/1 | Batch 336/11038 | Loss: 0.2612 | LR: 9.641e-05\n",
      "Epoch 1/1 | Batch 337/11038 | Loss: 0.2592 | LR: 9.641e-05\n",
      "Epoch 1/1 | Batch 338/11038 | Loss: 0.2677 | LR: 9.641e-05\n",
      "Epoch 1/1 | Batch 339/11038 | Loss: 0.2614 | LR: 9.641e-05\n",
      "Epoch 1/1 | Batch 340/11038 | Loss: 0.2764 | LR: 9.633e-05\n",
      "Epoch 1/1 | Batch 341/11038 | Loss: 0.2556 | LR: 9.633e-05\n",
      "Epoch 1/1 | Batch 342/11038 | Loss: 0.2625 | LR: 9.633e-05\n",
      "Epoch 1/1 | Batch 343/11038 | Loss: 0.2627 | LR: 9.633e-05\n",
      "Epoch 1/1 | Batch 344/11038 | Loss: 0.2669 | LR: 9.624e-05\n",
      "Epoch 1/1 | Batch 345/11038 | Loss: 0.2667 | LR: 9.624e-05\n",
      "Epoch 1/1 | Batch 346/11038 | Loss: 0.2657 | LR: 9.624e-05\n",
      "Epoch 1/1 | Batch 347/11038 | Loss: 0.2680 | LR: 9.624e-05\n",
      "Epoch 1/1 | Batch 348/11038 | Loss: 0.2694 | LR: 9.616e-05\n",
      "Epoch 1/1 | Batch 349/11038 | Loss: 0.2572 | LR: 9.616e-05\n",
      "Epoch 1/1 | Batch 350/11038 | Loss: 0.2565 | LR: 9.616e-05\n",
      "Epoch 1/1 | Batch 351/11038 | Loss: 0.2553 | LR: 9.616e-05\n",
      "Epoch 1/1 | Batch 352/11038 | Loss: 0.2678 | LR: 9.607e-05\n",
      "Epoch 1/1 | Batch 353/11038 | Loss: 0.2499 | LR: 9.607e-05\n",
      "Epoch 1/1 | Batch 354/11038 | Loss: 0.2544 | LR: 9.607e-05\n",
      "Epoch 1/1 | Batch 355/11038 | Loss: 0.2679 | LR: 9.607e-05\n",
      "Epoch 1/1 | Batch 356/11038 | Loss: 0.2677 | LR: 9.598e-05\n",
      "Epoch 1/1 | Batch 357/11038 | Loss: 0.2549 | LR: 9.598e-05\n",
      "Epoch 1/1 | Batch 358/11038 | Loss: 0.2595 | LR: 9.598e-05\n",
      "Epoch 1/1 | Batch 359/11038 | Loss: 0.2459 | LR: 9.598e-05\n",
      "Epoch 1/1 | Batch 360/11038 | Loss: 0.2729 | LR: 9.589e-05\n",
      "Epoch 1/1 | Batch 361/11038 | Loss: 0.2438 | LR: 9.589e-05\n",
      "Epoch 1/1 | Batch 362/11038 | Loss: 0.2532 | LR: 9.589e-05\n",
      "Epoch 1/1 | Batch 363/11038 | Loss: 0.2631 | LR: 9.589e-05\n",
      "Epoch 1/1 | Batch 364/11038 | Loss: 0.2694 | LR: 9.580e-05\n",
      "Epoch 1/1 | Batch 365/11038 | Loss: 0.2494 | LR: 9.580e-05\n",
      "Epoch 1/1 | Batch 366/11038 | Loss: 0.2515 | LR: 9.580e-05\n",
      "Epoch 1/1 | Batch 367/11038 | Loss: 0.2370 | LR: 9.580e-05\n",
      "Epoch 1/1 | Batch 368/11038 | Loss: 0.2546 | LR: 9.571e-05\n",
      "Epoch 1/1 | Batch 369/11038 | Loss: 0.2757 | LR: 9.571e-05\n",
      "Epoch 1/1 | Batch 370/11038 | Loss: 0.2658 | LR: 9.571e-05\n",
      "Epoch 1/1 | Batch 371/11038 | Loss: 0.2572 | LR: 9.571e-05\n",
      "Epoch 1/1 | Batch 372/11038 | Loss: 0.2503 | LR: 9.562e-05\n",
      "Epoch 1/1 | Batch 373/11038 | Loss: 0.2378 | LR: 9.562e-05\n",
      "Epoch 1/1 | Batch 374/11038 | Loss: 0.2535 | LR: 9.562e-05\n",
      "Epoch 1/1 | Batch 375/11038 | Loss: 0.2551 | LR: 9.562e-05\n",
      "Epoch 1/1 | Batch 376/11038 | Loss: 0.2465 | LR: 9.552e-05\n",
      "Epoch 1/1 | Batch 377/11038 | Loss: 0.2471 | LR: 9.552e-05\n",
      "Epoch 1/1 | Batch 378/11038 | Loss: 0.2595 | LR: 9.552e-05\n",
      "Epoch 1/1 | Batch 379/11038 | Loss: 0.2713 | LR: 9.552e-05\n",
      "Epoch 1/1 | Batch 380/11038 | Loss: 0.2393 | LR: 9.543e-05\n",
      "Epoch 1/1 | Batch 381/11038 | Loss: 0.2687 | LR: 9.543e-05\n",
      "Epoch 1/1 | Batch 382/11038 | Loss: 0.2502 | LR: 9.543e-05\n",
      "Epoch 1/1 | Batch 383/11038 | Loss: 0.2481 | LR: 9.543e-05\n",
      "Epoch 1/1 | Batch 384/11038 | Loss: 0.2461 | LR: 9.533e-05\n",
      "Epoch 1/1 | Batch 385/11038 | Loss: 0.2577 | LR: 9.533e-05\n",
      "Epoch 1/1 | Batch 386/11038 | Loss: 0.2374 | LR: 9.533e-05\n",
      "Epoch 1/1 | Batch 387/11038 | Loss: 0.2412 | LR: 9.533e-05\n",
      "Epoch 1/1 | Batch 388/11038 | Loss: 0.2441 | LR: 9.524e-05\n",
      "Epoch 1/1 | Batch 389/11038 | Loss: 0.2437 | LR: 9.524e-05\n",
      "Epoch 1/1 | Batch 390/11038 | Loss: 0.2420 | LR: 9.524e-05\n",
      "Epoch 1/1 | Batch 391/11038 | Loss: 0.2401 | LR: 9.524e-05\n",
      "Epoch 1/1 | Batch 392/11038 | Loss: 0.2560 | LR: 9.514e-05\n",
      "Epoch 1/1 | Batch 393/11038 | Loss: 0.2420 | LR: 9.514e-05\n",
      "Epoch 1/1 | Batch 394/11038 | Loss: 0.2356 | LR: 9.514e-05\n",
      "Epoch 1/1 | Batch 395/11038 | Loss: 0.2679 | LR: 9.514e-05\n",
      "Epoch 1/1 | Batch 396/11038 | Loss: 0.2448 | LR: 9.504e-05\n",
      "Epoch 1/1 | Batch 397/11038 | Loss: 0.2400 | LR: 9.504e-05\n",
      "Epoch 1/1 | Batch 398/11038 | Loss: 0.2323 | LR: 9.504e-05\n",
      "Epoch 1/1 | Batch 399/11038 | Loss: 0.2508 | LR: 9.504e-05\n",
      "Epoch 1/1 | Batch 400/11038 | Loss: 0.2637 | LR: 9.494e-05\n",
      "Hamming Loss: 0.0213\n",
      "\n",
      "Checkpoint saved at epoch 1 batch 400\n",
      "Epoch 1/1 | Batch 401/11038 | Loss: 0.2507 | LR: 9.494e-05\n",
      "Epoch 1/1 | Batch 402/11038 | Loss: 0.2338 | LR: 9.494e-05\n",
      "Epoch 1/1 | Batch 403/11038 | Loss: 0.2340 | LR: 9.494e-05\n",
      "Epoch 1/1 | Batch 404/11038 | Loss: 0.2334 | LR: 9.484e-05\n",
      "Epoch 1/1 | Batch 405/11038 | Loss: 0.2474 | LR: 9.484e-05\n",
      "Epoch 1/1 | Batch 406/11038 | Loss: 0.2267 | LR: 9.484e-05\n",
      "Epoch 1/1 | Batch 407/11038 | Loss: 0.2361 | LR: 9.484e-05\n",
      "Epoch 1/1 | Batch 408/11038 | Loss: 0.2446 | LR: 9.474e-05\n",
      "Epoch 1/1 | Batch 409/11038 | Loss: 0.2327 | LR: 9.474e-05\n",
      "Epoch 1/1 | Batch 410/11038 | Loss: 0.2550 | LR: 9.474e-05\n",
      "Epoch 1/1 | Batch 411/11038 | Loss: 0.2269 | LR: 9.474e-05\n",
      "Epoch 1/1 | Batch 412/11038 | Loss: 0.2575 | LR: 9.464e-05\n",
      "Epoch 1/1 | Batch 413/11038 | Loss: 0.2424 | LR: 9.464e-05\n",
      "Epoch 1/1 | Batch 414/11038 | Loss: 0.2356 | LR: 9.464e-05\n",
      "Epoch 1/1 | Batch 415/11038 | Loss: 0.2546 | LR: 9.464e-05\n",
      "Epoch 1/1 | Batch 416/11038 | Loss: 0.2278 | LR: 9.454e-05\n",
      "Epoch 1/1 | Batch 417/11038 | Loss: 0.2476 | LR: 9.454e-05\n",
      "Epoch 1/1 | Batch 418/11038 | Loss: 0.2234 | LR: 9.454e-05\n",
      "Epoch 1/1 | Batch 419/11038 | Loss: 0.2268 | LR: 9.454e-05\n",
      "Epoch 1/1 | Batch 420/11038 | Loss: 0.2438 | LR: 9.443e-05\n",
      "Epoch 1/1 | Batch 421/11038 | Loss: 0.2413 | LR: 9.443e-05\n",
      "Epoch 1/1 | Batch 422/11038 | Loss: 0.2385 | LR: 9.443e-05\n",
      "Epoch 1/1 | Batch 423/11038 | Loss: 0.2421 | LR: 9.443e-05\n",
      "Epoch 1/1 | Batch 424/11038 | Loss: 0.2412 | LR: 9.433e-05\n",
      "Epoch 1/1 | Batch 425/11038 | Loss: 0.2300 | LR: 9.433e-05\n",
      "Epoch 1/1 | Batch 426/11038 | Loss: 0.2202 | LR: 9.433e-05\n",
      "Epoch 1/1 | Batch 427/11038 | Loss: 0.2407 | LR: 9.433e-05\n",
      "Epoch 1/1 | Batch 428/11038 | Loss: 0.2449 | LR: 9.422e-05\n",
      "Epoch 1/1 | Batch 429/11038 | Loss: 0.2432 | LR: 9.422e-05\n",
      "Epoch 1/1 | Batch 430/11038 | Loss: 0.2407 | LR: 9.422e-05\n",
      "Epoch 1/1 | Batch 431/11038 | Loss: 0.2292 | LR: 9.422e-05\n",
      "Epoch 1/1 | Batch 432/11038 | Loss: 0.2533 | LR: 9.412e-05\n",
      "Epoch 1/1 | Batch 433/11038 | Loss: 0.2295 | LR: 9.412e-05\n",
      "Epoch 1/1 | Batch 434/11038 | Loss: 0.2338 | LR: 9.412e-05\n",
      "Epoch 1/1 | Batch 435/11038 | Loss: 0.2334 | LR: 9.412e-05\n",
      "Epoch 1/1 | Batch 436/11038 | Loss: 0.2372 | LR: 9.401e-05\n",
      "Epoch 1/1 | Batch 437/11038 | Loss: 0.2381 | LR: 9.401e-05\n",
      "Epoch 1/1 | Batch 438/11038 | Loss: 0.2562 | LR: 9.401e-05\n",
      "Epoch 1/1 | Batch 439/11038 | Loss: 0.2259 | LR: 9.401e-05\n",
      "Epoch 1/1 | Batch 440/11038 | Loss: 0.2095 | LR: 9.390e-05\n",
      "Epoch 1/1 | Batch 441/11038 | Loss: 0.2145 | LR: 9.390e-05\n",
      "Epoch 1/1 | Batch 442/11038 | Loss: 0.2206 | LR: 9.390e-05\n",
      "Epoch 1/1 | Batch 443/11038 | Loss: 0.2440 | LR: 9.390e-05\n",
      "Epoch 1/1 | Batch 444/11038 | Loss: 0.2199 | LR: 9.379e-05\n",
      "Epoch 1/1 | Batch 445/11038 | Loss: 0.2447 | LR: 9.379e-05\n",
      "Epoch 1/1 | Batch 446/11038 | Loss: 0.2209 | LR: 9.379e-05\n",
      "Epoch 1/1 | Batch 447/11038 | Loss: 0.2151 | LR: 9.379e-05\n",
      "Epoch 1/1 | Batch 448/11038 | Loss: 0.2165 | LR: 9.368e-05\n",
      "Epoch 1/1 | Batch 449/11038 | Loss: 0.2169 | LR: 9.368e-05\n",
      "Epoch 1/1 | Batch 450/11038 | Loss: 0.2241 | LR: 9.368e-05\n",
      "Epoch 1/1 | Batch 451/11038 | Loss: 0.2180 | LR: 9.368e-05\n",
      "Epoch 1/1 | Batch 452/11038 | Loss: 0.2108 | LR: 9.357e-05\n",
      "Epoch 1/1 | Batch 453/11038 | Loss: 0.2326 | LR: 9.357e-05\n",
      "Epoch 1/1 | Batch 454/11038 | Loss: 0.2409 | LR: 9.357e-05\n",
      "Epoch 1/1 | Batch 455/11038 | Loss: 0.2175 | LR: 9.357e-05\n",
      "Epoch 1/1 | Batch 456/11038 | Loss: 0.2208 | LR: 9.346e-05\n",
      "Epoch 1/1 | Batch 457/11038 | Loss: 0.2344 | LR: 9.346e-05\n",
      "Epoch 1/1 | Batch 458/11038 | Loss: 0.2176 | LR: 9.346e-05\n",
      "Epoch 1/1 | Batch 459/11038 | Loss: 0.2286 | LR: 9.346e-05\n",
      "Epoch 1/1 | Batch 460/11038 | Loss: 0.2261 | LR: 9.335e-05\n",
      "Epoch 1/1 | Batch 461/11038 | Loss: 0.2164 | LR: 9.335e-05\n",
      "Epoch 1/1 | Batch 462/11038 | Loss: 0.2441 | LR: 9.335e-05\n",
      "Epoch 1/1 | Batch 463/11038 | Loss: 0.2354 | LR: 9.335e-05\n",
      "Epoch 1/1 | Batch 464/11038 | Loss: 0.2177 | LR: 9.324e-05\n",
      "Epoch 1/1 | Batch 465/11038 | Loss: 0.2305 | LR: 9.324e-05\n",
      "Epoch 1/1 | Batch 466/11038 | Loss: 0.2166 | LR: 9.324e-05\n",
      "Epoch 1/1 | Batch 467/11038 | Loss: 0.2427 | LR: 9.324e-05\n",
      "Epoch 1/1 | Batch 468/11038 | Loss: 0.2246 | LR: 9.312e-05\n",
      "Epoch 1/1 | Batch 469/11038 | Loss: 0.2217 | LR: 9.312e-05\n",
      "Epoch 1/1 | Batch 470/11038 | Loss: 0.2158 | LR: 9.312e-05\n",
      "Epoch 1/1 | Batch 471/11038 | Loss: 0.2101 | LR: 9.312e-05\n",
      "Epoch 1/1 | Batch 472/11038 | Loss: 0.2171 | LR: 9.301e-05\n",
      "Epoch 1/1 | Batch 473/11038 | Loss: 0.2166 | LR: 9.301e-05\n",
      "Epoch 1/1 | Batch 474/11038 | Loss: 0.2194 | LR: 9.301e-05\n",
      "Epoch 1/1 | Batch 475/11038 | Loss: 0.2221 | LR: 9.301e-05\n",
      "Epoch 1/1 | Batch 476/11038 | Loss: 0.2115 | LR: 9.289e-05\n",
      "Epoch 1/1 | Batch 477/11038 | Loss: 0.2111 | LR: 9.289e-05\n",
      "Epoch 1/1 | Batch 478/11038 | Loss: 0.2046 | LR: 9.289e-05\n",
      "Epoch 1/1 | Batch 479/11038 | Loss: 0.2054 | LR: 9.289e-05\n",
      "Epoch 1/1 | Batch 480/11038 | Loss: 0.2199 | LR: 9.277e-05\n",
      "Epoch 1/1 | Batch 481/11038 | Loss: 0.2188 | LR: 9.277e-05\n",
      "Epoch 1/1 | Batch 482/11038 | Loss: 0.2156 | LR: 9.277e-05\n",
      "Epoch 1/1 | Batch 483/11038 | Loss: 0.2147 | LR: 9.277e-05\n",
      "Epoch 1/1 | Batch 484/11038 | Loss: 0.2063 | LR: 9.266e-05\n",
      "Epoch 1/1 | Batch 485/11038 | Loss: 0.2199 | LR: 9.266e-05\n",
      "Epoch 1/1 | Batch 486/11038 | Loss: 0.2217 | LR: 9.266e-05\n",
      "Epoch 1/1 | Batch 487/11038 | Loss: 0.2135 | LR: 9.266e-05\n",
      "Epoch 1/1 | Batch 488/11038 | Loss: 0.2105 | LR: 9.254e-05\n",
      "Epoch 1/1 | Batch 489/11038 | Loss: 0.2164 | LR: 9.254e-05\n",
      "Epoch 1/1 | Batch 490/11038 | Loss: 0.2113 | LR: 9.254e-05\n",
      "Epoch 1/1 | Batch 491/11038 | Loss: 0.1975 | LR: 9.254e-05\n",
      "Epoch 1/1 | Batch 492/11038 | Loss: 0.2125 | LR: 9.242e-05\n",
      "Epoch 1/1 | Batch 493/11038 | Loss: 0.2263 | LR: 9.242e-05\n",
      "Epoch 1/1 | Batch 494/11038 | Loss: nan | LR: 9.242e-05\n",
      "Epoch 1/1 | Batch 495/11038 | Loss: 0.2186 | LR: 9.242e-05\n",
      "Epoch 1/1 | Batch 496/11038 | Loss: 0.1909 | LR: 9.230e-05\n",
      "Epoch 1/1 | Batch 497/11038 | Loss: 0.2173 | LR: 9.230e-05\n",
      "Epoch 1/1 | Batch 498/11038 | Loss: 0.2308 | LR: 9.230e-05\n",
      "Epoch 1/1 | Batch 499/11038 | Loss: 0.2032 | LR: 9.230e-05\n",
      "Epoch 1/1 | Batch 500/11038 | Loss: 0.2108 | LR: 9.218e-05\n",
      "Hamming Loss: 0.0199\n",
      "\n",
      "Checkpoint saved at epoch 1 batch 500\n",
      "Epoch 1/1 | Batch 501/11038 | Loss: 0.2125 | LR: 9.218e-05\n",
      "Epoch 1/1 | Batch 502/11038 | Loss: 0.2031 | LR: 9.218e-05\n",
      "Epoch 1/1 | Batch 503/11038 | Loss: 0.2102 | LR: 9.218e-05\n",
      "Epoch 1/1 | Batch 504/11038 | Loss: 0.2144 | LR: 9.205e-05\n",
      "Epoch 1/1 | Batch 505/11038 | Loss: 0.2016 | LR: 9.205e-05\n",
      "Epoch 1/1 | Batch 506/11038 | Loss: 0.2022 | LR: 9.205e-05\n",
      "Epoch 1/1 | Batch 507/11038 | Loss: 0.2235 | LR: 9.205e-05\n",
      "Epoch 1/1 | Batch 508/11038 | Loss: 0.2081 | LR: 9.193e-05\n",
      "Epoch 1/1 | Batch 509/11038 | Loss: 0.2081 | LR: 9.193e-05\n",
      "Epoch 1/1 | Batch 510/11038 | Loss: 0.1951 | LR: 9.193e-05\n",
      "Epoch 1/1 | Batch 511/11038 | Loss: 0.1917 | LR: 9.193e-05\n",
      "Epoch 1/1 | Batch 512/11038 | Loss: 0.2039 | LR: 9.181e-05\n",
      "Epoch 1/1 | Batch 513/11038 | Loss: 0.1984 | LR: 9.181e-05\n",
      "Epoch 1/1 | Batch 514/11038 | Loss: 0.2122 | LR: 9.181e-05\n",
      "Epoch 1/1 | Batch 515/11038 | Loss: 0.2204 | LR: 9.181e-05\n",
      "Epoch 1/1 | Batch 516/11038 | Loss: 0.2083 | LR: 9.168e-05\n",
      "Epoch 1/1 | Batch 517/11038 | Loss: 0.2203 | LR: 9.168e-05\n",
      "Epoch 1/1 | Batch 518/11038 | Loss: 0.2184 | LR: 9.168e-05\n",
      "Epoch 1/1 | Batch 519/11038 | Loss: 0.1920 | LR: 9.168e-05\n",
      "Epoch 1/1 | Batch 520/11038 | Loss: 0.2077 | LR: 9.156e-05\n",
      "Epoch 1/1 | Batch 521/11038 | Loss: 0.1999 | LR: 9.156e-05\n",
      "Epoch 1/1 | Batch 522/11038 | Loss: 0.1955 | LR: 9.156e-05\n",
      "Epoch 1/1 | Batch 523/11038 | Loss: 0.2303 | LR: 9.156e-05\n",
      "Epoch 1/1 | Batch 524/11038 | Loss: 0.2068 | LR: 9.143e-05\n",
      "Epoch 1/1 | Batch 525/11038 | Loss: 0.1895 | LR: 9.143e-05\n",
      "Epoch 1/1 | Batch 526/11038 | Loss: 0.1930 | LR: 9.143e-05\n",
      "Epoch 1/1 | Batch 527/11038 | Loss: 0.2229 | LR: 9.143e-05\n",
      "Epoch 1/1 | Batch 528/11038 | Loss: 0.2031 | LR: 9.130e-05\n",
      "Epoch 1/1 | Batch 529/11038 | Loss: 0.2113 | LR: 9.130e-05\n",
      "Epoch 1/1 | Batch 530/11038 | Loss: 0.2174 | LR: 9.130e-05\n",
      "Epoch 1/1 | Batch 531/11038 | Loss: 0.1878 | LR: 9.130e-05\n",
      "Epoch 1/1 | Batch 532/11038 | Loss: 0.1886 | LR: 9.117e-05\n",
      "Epoch 1/1 | Batch 533/11038 | Loss: 0.1982 | LR: 9.117e-05\n",
      "Epoch 1/1 | Batch 534/11038 | Loss: 0.2272 | LR: 9.117e-05\n",
      "Epoch 1/1 | Batch 535/11038 | Loss: 0.2003 | LR: 9.117e-05\n",
      "Epoch 1/1 | Batch 536/11038 | Loss: 0.1993 | LR: 9.104e-05\n",
      "Epoch 1/1 | Batch 537/11038 | Loss: 0.2238 | LR: 9.104e-05\n",
      "Epoch 1/1 | Batch 538/11038 | Loss: 0.1883 | LR: 9.104e-05\n",
      "Epoch 1/1 | Batch 539/11038 | Loss: 0.1966 | LR: 9.104e-05\n",
      "Epoch 1/1 | Batch 540/11038 | Loss: 0.1964 | LR: 9.091e-05\n",
      "Epoch 1/1 | Batch 541/11038 | Loss: 0.2267 | LR: 9.091e-05\n",
      "Epoch 1/1 | Batch 542/11038 | Loss: 0.1924 | LR: 9.091e-05\n",
      "Epoch 1/1 | Batch 543/11038 | Loss: 0.2396 | LR: 9.091e-05\n",
      "Epoch 1/1 | Batch 544/11038 | Loss: 0.1915 | LR: 9.078e-05\n",
      "Epoch 1/1 | Batch 545/11038 | Loss: 0.2313 | LR: 9.078e-05\n",
      "Epoch 1/1 | Batch 546/11038 | Loss: 0.1949 | LR: 9.078e-05\n",
      "Epoch 1/1 | Batch 547/11038 | Loss: 0.2271 | LR: 9.078e-05\n",
      "Epoch 1/1 | Batch 548/11038 | Loss: 0.1961 | LR: 9.065e-05\n",
      "Epoch 1/1 | Batch 549/11038 | Loss: 0.2063 | LR: 9.065e-05\n",
      "Epoch 1/1 | Batch 550/11038 | Loss: 0.1846 | LR: 9.065e-05\n",
      "Epoch 1/1 | Batch 551/11038 | Loss: 0.1914 | LR: 9.065e-05\n",
      "Epoch 1/1 | Batch 552/11038 | Loss: 0.2170 | LR: 9.052e-05\n",
      "Epoch 1/1 | Batch 553/11038 | Loss: 0.1944 | LR: 9.052e-05\n",
      "Epoch 1/1 | Batch 554/11038 | Loss: 0.1851 | LR: 9.052e-05\n",
      "Epoch 1/1 | Batch 555/11038 | Loss: 0.1946 | LR: 9.052e-05\n",
      "Epoch 1/1 | Batch 556/11038 | Loss: 0.2136 | LR: 9.039e-05\n",
      "Epoch 1/1 | Batch 557/11038 | Loss: 0.1831 | LR: 9.039e-05\n",
      "Epoch 1/1 | Batch 558/11038 | Loss: 0.1909 | LR: 9.039e-05\n",
      "Epoch 1/1 | Batch 559/11038 | Loss: 0.1942 | LR: 9.039e-05\n",
      "Epoch 1/1 | Batch 560/11038 | Loss: 0.1830 | LR: 9.025e-05\n",
      "Epoch 1/1 | Batch 561/11038 | Loss: 0.1845 | LR: 9.025e-05\n",
      "Epoch 1/1 | Batch 562/11038 | Loss: 0.1869 | LR: 9.025e-05\n",
      "Epoch 1/1 | Batch 563/11038 | Loss: 0.1783 | LR: 9.025e-05\n",
      "Epoch 1/1 | Batch 564/11038 | Loss: 0.2298 | LR: 9.012e-05\n",
      "Epoch 1/1 | Batch 565/11038 | Loss: 0.2197 | LR: 9.012e-05\n",
      "Epoch 1/1 | Batch 566/11038 | Loss: 0.1900 | LR: 9.012e-05\n",
      "Epoch 1/1 | Batch 567/11038 | Loss: 0.1969 | LR: 9.012e-05\n",
      "Epoch 1/1 | Batch 568/11038 | Loss: 0.2048 | LR: 8.998e-05\n",
      "Epoch 1/1 | Batch 569/11038 | Loss: 0.1821 | LR: 8.998e-05\n",
      "Epoch 1/1 | Batch 570/11038 | Loss: 0.1829 | LR: 8.998e-05\n",
      "Epoch 1/1 | Batch 571/11038 | Loss: 0.1775 | LR: 8.998e-05\n",
      "Epoch 1/1 | Batch 572/11038 | Loss: 0.1805 | LR: 8.985e-05\n",
      "Epoch 1/1 | Batch 573/11038 | Loss: 0.2101 | LR: 8.985e-05\n",
      "Epoch 1/1 | Batch 574/11038 | Loss: 0.1817 | LR: 8.985e-05\n",
      "Epoch 1/1 | Batch 575/11038 | Loss: 0.1797 | LR: 8.985e-05\n",
      "Epoch 1/1 | Batch 576/11038 | Loss: 0.1977 | LR: 8.971e-05\n",
      "Epoch 1/1 | Batch 577/11038 | Loss: 0.1768 | LR: 8.971e-05\n",
      "Epoch 1/1 | Batch 578/11038 | Loss: 0.1815 | LR: 8.971e-05\n",
      "Epoch 1/1 | Batch 579/11038 | Loss: 0.2107 | LR: 8.971e-05\n",
      "Epoch 1/1 | Batch 580/11038 | Loss: 0.1815 | LR: 8.957e-05\n",
      "Epoch 1/1 | Batch 581/11038 | Loss: 0.1960 | LR: 8.957e-05\n",
      "Epoch 1/1 | Batch 582/11038 | Loss: 0.1957 | LR: 8.957e-05\n",
      "Epoch 1/1 | Batch 583/11038 | Loss: 0.2118 | LR: 8.957e-05\n",
      "Epoch 1/1 | Batch 584/11038 | Loss: 0.1873 | LR: 8.943e-05\n",
      "Epoch 1/1 | Batch 585/11038 | Loss: 0.1753 | LR: 8.943e-05\n",
      "Epoch 1/1 | Batch 586/11038 | Loss: 0.2206 | LR: 8.943e-05\n",
      "Epoch 1/1 | Batch 587/11038 | Loss: 0.1843 | LR: 8.943e-05\n",
      "Epoch 1/1 | Batch 588/11038 | Loss: 0.1883 | LR: 8.929e-05\n",
      "Epoch 1/1 | Batch 589/11038 | Loss: 0.1907 | LR: 8.929e-05\n",
      "Epoch 1/1 | Batch 590/11038 | Loss: 0.2008 | LR: 8.929e-05\n",
      "Epoch 1/1 | Batch 591/11038 | Loss: 0.1715 | LR: 8.929e-05\n",
      "Epoch 1/1 | Batch 592/11038 | Loss: 0.1882 | LR: 8.915e-05\n",
      "Epoch 1/1 | Batch 593/11038 | Loss: 0.1846 | LR: 8.915e-05\n",
      "Epoch 1/1 | Batch 594/11038 | Loss: 0.1869 | LR: 8.915e-05\n",
      "Epoch 1/1 | Batch 595/11038 | Loss: 0.1801 | LR: 8.915e-05\n",
      "Epoch 1/1 | Batch 596/11038 | Loss: 0.1815 | LR: 8.901e-05\n",
      "Epoch 1/1 | Batch 597/11038 | Loss: 0.1936 | LR: 8.901e-05\n",
      "Epoch 1/1 | Batch 598/11038 | Loss: 0.2048 | LR: 8.901e-05\n",
      "Epoch 1/1 | Batch 599/11038 | Loss: 0.1835 | LR: 8.901e-05\n",
      "Epoch 1/1 | Batch 600/11038 | Loss: 0.1930 | LR: 8.887e-05\n",
      "Hamming Loss: 0.0199\n",
      "\n",
      "Checkpoint saved at epoch 1 batch 600\n",
      "Epoch 1/1 | Batch 601/11038 | Loss: 0.1837 | LR: 8.887e-05\n",
      "Epoch 1/1 | Batch 602/11038 | Loss: 0.1916 | LR: 8.887e-05\n",
      "Epoch 1/1 | Batch 603/11038 | Loss: 0.2010 | LR: 8.887e-05\n",
      "Epoch 1/1 | Batch 604/11038 | Loss: 0.1935 | LR: 8.872e-05\n",
      "Epoch 1/1 | Batch 605/11038 | Loss: 0.1950 | LR: 8.872e-05\n",
      "Epoch 1/1 | Batch 606/11038 | Loss: 0.1701 | LR: 8.872e-05\n",
      "Epoch 1/1 | Batch 607/11038 | Loss: 0.1826 | LR: 8.872e-05\n",
      "Epoch 1/1 | Batch 608/11038 | Loss: 0.2093 | LR: 8.858e-05\n",
      "Epoch 1/1 | Batch 609/11038 | Loss: 0.1662 | LR: 8.858e-05\n",
      "Epoch 1/1 | Batch 610/11038 | Loss: 0.2265 | LR: 8.858e-05\n",
      "Epoch 1/1 | Batch 611/11038 | Loss: 0.1741 | LR: 8.858e-05\n",
      "Epoch 1/1 | Batch 612/11038 | Loss: 0.1991 | LR: 8.844e-05\n",
      "Epoch 1/1 | Batch 613/11038 | Loss: 0.2159 | LR: 8.844e-05\n",
      "Epoch 1/1 | Batch 614/11038 | Loss: 0.1957 | LR: 8.844e-05\n",
      "Epoch 1/1 | Batch 615/11038 | Loss: 0.1792 | LR: 8.844e-05\n",
      "Epoch 1/1 | Batch 616/11038 | Loss: 0.2055 | LR: 8.829e-05\n",
      "Epoch 1/1 | Batch 617/11038 | Loss: 0.1807 | LR: 8.829e-05\n",
      "Epoch 1/1 | Batch 618/11038 | Loss: 0.1703 | LR: 8.829e-05\n",
      "Epoch 1/1 | Batch 619/11038 | Loss: 0.1904 | LR: 8.829e-05\n",
      "Epoch 1/1 | Batch 620/11038 | Loss: 0.1607 | LR: 8.814e-05\n",
      "Epoch 1/1 | Batch 621/11038 | Loss: 0.1809 | LR: 8.814e-05\n",
      "Epoch 1/1 | Batch 622/11038 | Loss: 0.1818 | LR: 8.814e-05\n",
      "Epoch 1/1 | Batch 623/11038 | Loss: 0.1720 | LR: 8.814e-05\n",
      "Epoch 1/1 | Batch 624/11038 | Loss: 0.1727 | LR: 8.800e-05\n",
      "Epoch 1/1 | Batch 625/11038 | Loss: 0.1800 | LR: 8.800e-05\n",
      "Epoch 1/1 | Batch 626/11038 | Loss: 0.1873 | LR: 8.800e-05\n",
      "Epoch 1/1 | Batch 627/11038 | Loss: 0.1840 | LR: 8.800e-05\n",
      "Epoch 1/1 | Batch 628/11038 | Loss: 0.2051 | LR: 8.785e-05\n",
      "Epoch 1/1 | Batch 629/11038 | Loss: 0.1759 | LR: 8.785e-05\n",
      "Epoch 1/1 | Batch 630/11038 | Loss: 0.1758 | LR: 8.785e-05\n",
      "Epoch 1/1 | Batch 631/11038 | Loss: 0.1918 | LR: 8.785e-05\n",
      "Epoch 1/1 | Batch 632/11038 | Loss: 0.1792 | LR: 8.770e-05\n",
      "Epoch 1/1 | Batch 633/11038 | Loss: 0.1664 | LR: 8.770e-05\n",
      "Epoch 1/1 | Batch 634/11038 | Loss: 0.1858 | LR: 8.770e-05\n",
      "Epoch 1/1 | Batch 635/11038 | Loss: 0.1706 | LR: 8.770e-05\n",
      "Epoch 1/1 | Batch 636/11038 | Loss: 0.1623 | LR: 8.755e-05\n",
      "Epoch 1/1 | Batch 637/11038 | Loss: 0.1615 | LR: 8.755e-05\n",
      "Epoch 1/1 | Batch 638/11038 | Loss: 0.1846 | LR: 8.755e-05\n",
      "Epoch 1/1 | Batch 639/11038 | Loss: 0.1650 | LR: 8.755e-05\n",
      "Epoch 1/1 | Batch 640/11038 | Loss: 0.1691 | LR: 8.740e-05\n",
      "Epoch 1/1 | Batch 641/11038 | Loss: 0.1688 | LR: 8.740e-05\n",
      "Epoch 1/1 | Batch 642/11038 | Loss: 0.1762 | LR: 8.740e-05\n",
      "Epoch 1/1 | Batch 643/11038 | Loss: 0.1728 | LR: 8.740e-05\n",
      "Epoch 1/1 | Batch 644/11038 | Loss: 0.2057 | LR: 8.725e-05\n",
      "Epoch 1/1 | Batch 645/11038 | Loss: 0.1677 | LR: 8.725e-05\n",
      "Epoch 1/1 | Batch 646/11038 | Loss: 0.1891 | LR: 8.725e-05\n",
      "Epoch 1/1 | Batch 647/11038 | Loss: 0.1812 | LR: 8.725e-05\n",
      "Epoch 1/1 | Batch 648/11038 | Loss: 0.1815 | LR: 8.710e-05\n",
      "Epoch 1/1 | Batch 649/11038 | Loss: 0.1646 | LR: 8.710e-05\n",
      "Epoch 1/1 | Batch 650/11038 | Loss: 0.1796 | LR: 8.710e-05\n",
      "Epoch 1/1 | Batch 651/11038 | Loss: 0.1728 | LR: 8.710e-05\n",
      "Epoch 1/1 | Batch 652/11038 | Loss: 0.1933 | LR: 8.695e-05\n",
      "Epoch 1/1 | Batch 653/11038 | Loss: 0.1617 | LR: 8.695e-05\n",
      "Epoch 1/1 | Batch 654/11038 | Loss: 0.1754 | LR: 8.695e-05\n",
      "Epoch 1/1 | Batch 655/11038 | Loss: 0.1806 | LR: 8.695e-05\n",
      "Epoch 1/1 | Batch 656/11038 | Loss: 0.1630 | LR: 8.679e-05\n",
      "Epoch 1/1 | Batch 657/11038 | Loss: 0.1751 | LR: 8.679e-05\n",
      "Epoch 1/1 | Batch 658/11038 | Loss: 0.1635 | LR: 8.679e-05\n",
      "Epoch 1/1 | Batch 659/11038 | Loss: 0.1645 | LR: 8.679e-05\n",
      "Epoch 1/1 | Batch 660/11038 | Loss: 0.1697 | LR: 8.664e-05\n",
      "Epoch 1/1 | Batch 661/11038 | Loss: 0.1542 | LR: 8.664e-05\n",
      "Epoch 1/1 | Batch 662/11038 | Loss: 0.1871 | LR: 8.664e-05\n",
      "Epoch 1/1 | Batch 663/11038 | Loss: 0.1601 | LR: 8.664e-05\n",
      "Epoch 1/1 | Batch 664/11038 | Loss: 0.1676 | LR: 8.648e-05\n",
      "Epoch 1/1 | Batch 665/11038 | Loss: 0.1684 | LR: 8.648e-05\n",
      "Epoch 1/1 | Batch 666/11038 | Loss: 0.1605 | LR: 8.648e-05\n",
      "Epoch 1/1 | Batch 667/11038 | Loss: 0.1802 | LR: 8.648e-05\n",
      "Epoch 1/1 | Batch 668/11038 | Loss: 0.1646 | LR: 8.633e-05\n",
      "Epoch 1/1 | Batch 669/11038 | Loss: 0.1542 | LR: 8.633e-05\n",
      "Epoch 1/1 | Batch 670/11038 | Loss: 0.1633 | LR: 8.633e-05\n",
      "Epoch 1/1 | Batch 671/11038 | Loss: 0.1808 | LR: 8.633e-05\n",
      "Epoch 1/1 | Batch 672/11038 | Loss: 0.1918 | LR: 8.617e-05\n",
      "Epoch 1/1 | Batch 673/11038 | Loss: 0.1680 | LR: 8.617e-05\n",
      "Epoch 1/1 | Batch 674/11038 | Loss: 0.1641 | LR: 8.617e-05\n",
      "Epoch 1/1 | Batch 675/11038 | Loss: 0.1606 | LR: 8.617e-05\n",
      "Epoch 1/1 | Batch 676/11038 | Loss: 0.1534 | LR: 8.602e-05\n",
      "Epoch 1/1 | Batch 677/11038 | Loss: 0.1636 | LR: 8.602e-05\n",
      "Epoch 1/1 | Batch 678/11038 | Loss: 0.1986 | LR: 8.602e-05\n",
      "Epoch 1/1 | Batch 679/11038 | Loss: 0.1756 | LR: 8.602e-05\n",
      "Epoch 1/1 | Batch 680/11038 | Loss: 0.1588 | LR: 8.586e-05\n",
      "Epoch 1/1 | Batch 681/11038 | Loss: 0.1641 | LR: 8.586e-05\n",
      "Epoch 1/1 | Batch 682/11038 | Loss: 0.1914 | LR: 8.586e-05\n",
      "Epoch 1/1 | Batch 683/11038 | Loss: 0.1814 | LR: 8.586e-05\n",
      "Epoch 1/1 | Batch 684/11038 | Loss: 0.1539 | LR: 8.570e-05\n",
      "Epoch 1/1 | Batch 685/11038 | Loss: 0.1797 | LR: 8.570e-05\n",
      "Epoch 1/1 | Batch 686/11038 | Loss: 0.1727 | LR: 8.570e-05\n",
      "Epoch 1/1 | Batch 687/11038 | Loss: 0.1557 | LR: 8.570e-05\n",
      "Epoch 1/1 | Batch 688/11038 | Loss: 0.1538 | LR: 8.554e-05\n",
      "Epoch 1/1 | Batch 689/11038 | Loss: 0.1585 | LR: 8.554e-05\n",
      "Epoch 1/1 | Batch 690/11038 | Loss: 0.1580 | LR: 8.554e-05\n",
      "Epoch 1/1 | Batch 691/11038 | Loss: 0.1708 | LR: 8.554e-05\n",
      "Epoch 1/1 | Batch 692/11038 | Loss: 0.1532 | LR: 8.538e-05\n",
      "Epoch 1/1 | Batch 693/11038 | Loss: 0.1706 | LR: 8.538e-05\n",
      "Epoch 1/1 | Batch 694/11038 | Loss: 0.1742 | LR: 8.538e-05\n",
      "Epoch 1/1 | Batch 695/11038 | Loss: 0.1533 | LR: 8.538e-05\n",
      "Epoch 1/1 | Batch 696/11038 | Loss: 0.1802 | LR: 8.522e-05\n",
      "Epoch 1/1 | Batch 697/11038 | Loss: 0.1679 | LR: 8.522e-05\n",
      "Epoch 1/1 | Batch 698/11038 | Loss: 0.1760 | LR: 8.522e-05\n",
      "Epoch 1/1 | Batch 699/11038 | Loss: 0.1526 | LR: 8.522e-05\n",
      "Epoch 1/1 | Batch 700/11038 | Loss: 0.1661 | LR: 8.506e-05\n",
      "Hamming Loss: 0.0199\n",
      "\n",
      "Checkpoint saved at epoch 1 batch 700\n",
      "Epoch 1/1 | Batch 701/11038 | Loss: 0.1653 | LR: 8.506e-05\n",
      "Epoch 1/1 | Batch 702/11038 | Loss: 0.1586 | LR: 8.506e-05\n",
      "Epoch 1/1 | Batch 703/11038 | Loss: 0.1659 | LR: 8.506e-05\n",
      "Epoch 1/1 | Batch 704/11038 | Loss: 0.1519 | LR: 8.490e-05\n",
      "Epoch 1/1 | Batch 705/11038 | Loss: 0.1545 | LR: 8.490e-05\n",
      "Epoch 1/1 | Batch 706/11038 | Loss: 0.1516 | LR: 8.490e-05\n",
      "Epoch 1/1 | Batch 707/11038 | Loss: 0.1493 | LR: 8.490e-05\n",
      "Epoch 1/1 | Batch 708/11038 | Loss: 0.1740 | LR: 8.474e-05\n",
      "Epoch 1/1 | Batch 709/11038 | Loss: 0.1561 | LR: 8.474e-05\n",
      "Epoch 1/1 | Batch 710/11038 | Loss: 0.1567 | LR: 8.474e-05\n",
      "Epoch 1/1 | Batch 711/11038 | Loss: 0.1610 | LR: 8.474e-05\n",
      "Epoch 1/1 | Batch 712/11038 | Loss: 0.1869 | LR: 8.457e-05\n",
      "Epoch 1/1 | Batch 713/11038 | Loss: 0.1718 | LR: 8.457e-05\n",
      "Epoch 1/1 | Batch 714/11038 | Loss: 0.1569 | LR: 8.457e-05\n",
      "Epoch 1/1 | Batch 715/11038 | Loss: 0.1748 | LR: 8.457e-05\n",
      "Epoch 1/1 | Batch 716/11038 | Loss: 0.1712 | LR: 8.441e-05\n",
      "Epoch 1/1 | Batch 717/11038 | Loss: 0.1629 | LR: 8.441e-05\n",
      "Epoch 1/1 | Batch 718/11038 | Loss: 0.1468 | LR: 8.441e-05\n",
      "Epoch 1/1 | Batch 719/11038 | Loss: 0.1702 | LR: 8.441e-05\n",
      "Epoch 1/1 | Batch 720/11038 | Loss: 0.1627 | LR: 8.424e-05\n",
      "Epoch 1/1 | Batch 721/11038 | Loss: 0.1522 | LR: 8.424e-05\n",
      "Epoch 1/1 | Batch 722/11038 | Loss: 0.1832 | LR: 8.424e-05\n",
      "Epoch 1/1 | Batch 723/11038 | Loss: 0.1593 | LR: 8.424e-05\n",
      "Epoch 1/1 | Batch 724/11038 | Loss: 0.1523 | LR: 8.408e-05\n",
      "Epoch 1/1 | Batch 725/11038 | Loss: 0.1466 | LR: 8.408e-05\n",
      "Epoch 1/1 | Batch 726/11038 | Loss: 0.1823 | LR: 8.408e-05\n",
      "Epoch 1/1 | Batch 727/11038 | Loss: 0.1497 | LR: 8.408e-05\n",
      "Epoch 1/1 | Batch 728/11038 | Loss: 0.1537 | LR: 8.391e-05\n",
      "Epoch 1/1 | Batch 729/11038 | Loss: 0.1384 | LR: 8.391e-05\n",
      "Epoch 1/1 | Batch 730/11038 | Loss: 0.1864 | LR: 8.391e-05\n",
      "Epoch 1/1 | Batch 731/11038 | Loss: 0.1801 | LR: 8.391e-05\n",
      "Epoch 1/1 | Batch 732/11038 | Loss: 0.1573 | LR: 8.374e-05\n",
      "Epoch 1/1 | Batch 733/11038 | Loss: 0.1648 | LR: 8.374e-05\n",
      "Epoch 1/1 | Batch 734/11038 | Loss: 0.1449 | LR: 8.374e-05\n",
      "Epoch 1/1 | Batch 735/11038 | Loss: 0.1551 | LR: 8.374e-05\n",
      "Epoch 1/1 | Batch 736/11038 | Loss: 0.1548 | LR: 8.358e-05\n",
      "Epoch 1/1 | Batch 737/11038 | Loss: 0.1638 | LR: 8.358e-05\n",
      "Epoch 1/1 | Batch 738/11038 | Loss: 0.1670 | LR: 8.358e-05\n",
      "Epoch 1/1 | Batch 739/11038 | Loss: 0.1462 | LR: 8.358e-05\n",
      "Epoch 1/1 | Batch 740/11038 | Loss: 0.1542 | LR: 8.341e-05\n",
      "Epoch 1/1 | Batch 741/11038 | Loss: 0.1648 | LR: 8.341e-05\n",
      "Epoch 1/1 | Batch 742/11038 | Loss: 0.1544 | LR: 8.341e-05\n",
      "Epoch 1/1 | Batch 743/11038 | Loss: 0.1485 | LR: 8.341e-05\n",
      "Epoch 1/1 | Batch 744/11038 | Loss: 0.1713 | LR: 8.324e-05\n",
      "Epoch 1/1 | Batch 745/11038 | Loss: 0.1966 | LR: 8.324e-05\n",
      "Epoch 1/1 | Batch 746/11038 | Loss: 0.1485 | LR: 8.324e-05\n",
      "Epoch 1/1 | Batch 747/11038 | Loss: 0.1662 | LR: 8.324e-05\n",
      "Epoch 1/1 | Batch 748/11038 | Loss: 0.1549 | LR: 8.307e-05\n",
      "Epoch 1/1 | Batch 749/11038 | Loss: 0.1443 | LR: 8.307e-05\n",
      "Epoch 1/1 | Batch 750/11038 | Loss: 0.1394 | LR: 8.307e-05\n",
      "Epoch 1/1 | Batch 751/11038 | Loss: 0.1617 | LR: 8.307e-05\n",
      "Epoch 1/1 | Batch 752/11038 | Loss: 0.1627 | LR: 8.290e-05\n",
      "Epoch 1/1 | Batch 753/11038 | Loss: 0.1591 | LR: 8.290e-05\n",
      "Epoch 1/1 | Batch 754/11038 | Loss: 0.1573 | LR: 8.290e-05\n",
      "Epoch 1/1 | Batch 755/11038 | Loss: 0.1518 | LR: 8.290e-05\n",
      "Epoch 1/1 | Batch 756/11038 | Loss: 0.1627 | LR: 8.273e-05\n",
      "Epoch 1/1 | Batch 757/11038 | Loss: 0.1422 | LR: 8.273e-05\n",
      "Epoch 1/1 | Batch 758/11038 | Loss: 0.1580 | LR: 8.273e-05\n",
      "Epoch 1/1 | Batch 759/11038 | Loss: 0.1671 | LR: 8.273e-05\n",
      "Epoch 1/1 | Batch 760/11038 | Loss: 0.1431 | LR: 8.256e-05\n",
      "Epoch 1/1 | Batch 761/11038 | Loss: 0.1825 | LR: 8.256e-05\n",
      "Epoch 1/1 | Batch 762/11038 | Loss: 0.1427 | LR: 8.256e-05\n",
      "Epoch 1/1 | Batch 763/11038 | Loss: 0.1723 | LR: 8.256e-05\n",
      "Epoch 1/1 | Batch 764/11038 | Loss: 0.1697 | LR: 8.239e-05\n",
      "Epoch 1/1 | Batch 765/11038 | Loss: 0.1640 | LR: 8.239e-05\n",
      "Epoch 1/1 | Batch 766/11038 | Loss: 0.1366 | LR: 8.239e-05\n",
      "Epoch 1/1 | Batch 767/11038 | Loss: 0.1663 | LR: 8.239e-05\n",
      "Epoch 1/1 | Batch 768/11038 | Loss: 0.1498 | LR: 8.221e-05\n",
      "Epoch 1/1 | Batch 769/11038 | Loss: 0.1409 | LR: 8.221e-05\n",
      "Epoch 1/1 | Batch 770/11038 | Loss: 0.1707 | LR: 8.221e-05\n",
      "Epoch 1/1 | Batch 771/11038 | Loss: 0.1592 | LR: 8.221e-05\n",
      "Epoch 1/1 | Batch 772/11038 | Loss: 0.1834 | LR: 8.204e-05\n",
      "Epoch 1/1 | Batch 773/11038 | Loss: 0.1453 | LR: 8.204e-05\n",
      "Epoch 1/1 | Batch 774/11038 | Loss: 0.1649 | LR: 8.204e-05\n",
      "Epoch 1/1 | Batch 775/11038 | Loss: 0.1629 | LR: 8.204e-05\n",
      "Epoch 1/1 | Batch 776/11038 | Loss: 0.1498 | LR: 8.186e-05\n",
      "Epoch 1/1 | Batch 777/11038 | Loss: 0.1464 | LR: 8.186e-05\n",
      "Epoch 1/1 | Batch 778/11038 | Loss: 0.1452 | LR: 8.186e-05\n",
      "Epoch 1/1 | Batch 779/11038 | Loss: 0.1552 | LR: 8.186e-05\n",
      "Epoch 1/1 | Batch 780/11038 | Loss: 0.1433 | LR: 8.169e-05\n",
      "Epoch 1/1 | Batch 781/11038 | Loss: 0.1938 | LR: 8.169e-05\n",
      "Epoch 1/1 | Batch 782/11038 | Loss: 0.1656 | LR: 8.169e-05\n",
      "Epoch 1/1 | Batch 783/11038 | Loss: 0.1570 | LR: 8.169e-05\n",
      "Epoch 1/1 | Batch 784/11038 | Loss: 0.1491 | LR: 8.151e-05\n",
      "Epoch 1/1 | Batch 785/11038 | Loss: 0.1420 | LR: 8.151e-05\n",
      "Epoch 1/1 | Batch 786/11038 | Loss: 0.1414 | LR: 8.151e-05\n",
      "Epoch 1/1 | Batch 787/11038 | Loss: 0.1513 | LR: 8.151e-05\n",
      "Epoch 1/1 | Batch 788/11038 | Loss: 0.1348 | LR: 8.134e-05\n",
      "Epoch 1/1 | Batch 789/11038 | Loss: 0.1365 | LR: 8.134e-05\n",
      "Epoch 1/1 | Batch 790/11038 | Loss: 0.1574 | LR: 8.134e-05\n",
      "Epoch 1/1 | Batch 791/11038 | Loss: 0.1673 | LR: 8.134e-05\n",
      "Epoch 1/1 | Batch 792/11038 | Loss: 0.1596 | LR: 8.116e-05\n",
      "Epoch 1/1 | Batch 793/11038 | Loss: 0.1484 | LR: 8.116e-05\n",
      "Epoch 1/1 | Batch 794/11038 | Loss: 0.1646 | LR: 8.116e-05\n",
      "Epoch 1/1 | Batch 795/11038 | Loss: 0.1707 | LR: 8.116e-05\n",
      "Epoch 1/1 | Batch 796/11038 | Loss: 0.1533 | LR: 8.098e-05\n",
      "Epoch 1/1 | Batch 797/11038 | Loss: 0.1719 | LR: 8.098e-05\n",
      "Epoch 1/1 | Batch 798/11038 | Loss: 0.1673 | LR: 8.098e-05\n",
      "Epoch 1/1 | Batch 799/11038 | Loss: 0.1402 | LR: 8.098e-05\n",
      "Epoch 1/1 | Batch 800/11038 | Loss: 0.1375 | LR: 8.081e-05\n",
      "Hamming Loss: 0.0199\n",
      "\n",
      "Checkpoint saved at epoch 1 batch 800\n",
      "Epoch 1/1 | Batch 801/11038 | Loss: 0.1379 | LR: 8.081e-05\n",
      "Epoch 1/1 | Batch 802/11038 | Loss: 0.1504 | LR: 8.081e-05\n",
      "Epoch 1/1 | Batch 803/11038 | Loss: 0.1462 | LR: 8.081e-05\n",
      "Epoch 1/1 | Batch 804/11038 | Loss: 0.1347 | LR: 8.063e-05\n",
      "Epoch 1/1 | Batch 805/11038 | Loss: 0.1385 | LR: 8.063e-05\n",
      "Epoch 1/1 | Batch 806/11038 | Loss: 0.1504 | LR: 8.063e-05\n",
      "Epoch 1/1 | Batch 807/11038 | Loss: 0.1294 | LR: 8.063e-05\n",
      "Epoch 1/1 | Batch 808/11038 | Loss: 0.1674 | LR: 8.045e-05\n",
      "Epoch 1/1 | Batch 809/11038 | Loss: 0.1408 | LR: 8.045e-05\n",
      "Epoch 1/1 | Batch 810/11038 | Loss: 0.1315 | LR: 8.045e-05\n",
      "Epoch 1/1 | Batch 811/11038 | Loss: 0.1423 | LR: 8.045e-05\n",
      "Epoch 1/1 | Batch 812/11038 | Loss: 0.2083 | LR: 8.027e-05\n",
      "Epoch 1/1 | Batch 813/11038 | Loss: 0.1531 | LR: 8.027e-05\n",
      "Epoch 1/1 | Batch 814/11038 | Loss: 0.1420 | LR: 8.027e-05\n",
      "Epoch 1/1 | Batch 815/11038 | Loss: 0.1372 | LR: 8.027e-05\n",
      "Epoch 1/1 | Batch 816/11038 | Loss: 0.1726 | LR: 8.009e-05\n",
      "Epoch 1/1 | Batch 817/11038 | Loss: 0.1491 | LR: 8.009e-05\n",
      "Epoch 1/1 | Batch 818/11038 | Loss: 0.1447 | LR: 8.009e-05\n",
      "Epoch 1/1 | Batch 819/11038 | Loss: 0.1540 | LR: 8.009e-05\n",
      "Epoch 1/1 | Batch 820/11038 | Loss: 0.1282 | LR: 7.990e-05\n",
      "Epoch 1/1 | Batch 821/11038 | Loss: 0.1337 | LR: 7.990e-05\n",
      "Epoch 1/1 | Batch 822/11038 | Loss: 0.1622 | LR: 7.990e-05\n",
      "Epoch 1/1 | Batch 823/11038 | Loss: 0.1591 | LR: 7.990e-05\n",
      "Epoch 1/1 | Batch 824/11038 | Loss: 0.1741 | LR: 7.972e-05\n",
      "Epoch 1/1 | Batch 825/11038 | Loss: 0.1506 | LR: 7.972e-05\n",
      "Epoch 1/1 | Batch 826/11038 | Loss: 0.1427 | LR: 7.972e-05\n",
      "Epoch 1/1 | Batch 827/11038 | Loss: 0.1310 | LR: 7.972e-05\n",
      "Epoch 1/1 | Batch 828/11038 | Loss: 0.1217 | LR: 7.954e-05\n",
      "Epoch 1/1 | Batch 829/11038 | Loss: 0.1470 | LR: 7.954e-05\n",
      "Epoch 1/1 | Batch 830/11038 | Loss: 0.1381 | LR: 7.954e-05\n",
      "Epoch 1/1 | Batch 831/11038 | Loss: 0.1640 | LR: 7.954e-05\n",
      "Epoch 1/1 | Batch 832/11038 | Loss: 0.1672 | LR: 7.936e-05\n",
      "Epoch 1/1 | Batch 833/11038 | Loss: 0.1615 | LR: 7.936e-05\n",
      "Epoch 1/1 | Batch 834/11038 | Loss: 0.1568 | LR: 7.936e-05\n",
      "Epoch 1/1 | Batch 835/11038 | Loss: nan | LR: 7.936e-05\n",
      "Epoch 1/1 | Batch 836/11038 | Loss: 0.1483 | LR: 7.917e-05\n",
      "Epoch 1/1 | Batch 837/11038 | Loss: 0.1405 | LR: 7.917e-05\n",
      "Epoch 1/1 | Batch 838/11038 | Loss: 0.1568 | LR: 7.917e-05\n",
      "Epoch 1/1 | Batch 839/11038 | Loss: 0.1563 | LR: 7.917e-05\n",
      "Epoch 1/1 | Batch 840/11038 | Loss: 0.1340 | LR: 7.899e-05\n",
      "Epoch 1/1 | Batch 841/11038 | Loss: 0.1382 | LR: 7.899e-05\n",
      "Epoch 1/1 | Batch 842/11038 | Loss: 0.1532 | LR: 7.899e-05\n",
      "Epoch 1/1 | Batch 843/11038 | Loss: 0.1414 | LR: 7.899e-05\n",
      "Epoch 1/1 | Batch 844/11038 | Loss: 0.1573 | LR: 7.880e-05\n",
      "Epoch 1/1 | Batch 845/11038 | Loss: 0.1347 | LR: 7.880e-05\n",
      "Epoch 1/1 | Batch 846/11038 | Loss: 0.1428 | LR: 7.880e-05\n",
      "Epoch 1/1 | Batch 847/11038 | Loss: 0.1395 | LR: 7.880e-05\n",
      "Epoch 1/1 | Batch 848/11038 | Loss: 0.1502 | LR: 7.862e-05\n",
      "Epoch 1/1 | Batch 849/11038 | Loss: 0.1442 | LR: 7.862e-05\n",
      "Epoch 1/1 | Batch 850/11038 | Loss: 0.1760 | LR: 7.862e-05\n",
      "Epoch 1/1 | Batch 851/11038 | Loss: 0.1800 | LR: 7.862e-05\n",
      "Epoch 1/1 | Batch 852/11038 | Loss: 0.1461 | LR: 7.843e-05\n",
      "Epoch 1/1 | Batch 853/11038 | Loss: 0.1488 | LR: 7.843e-05\n",
      "Epoch 1/1 | Batch 854/11038 | Loss: 0.1666 | LR: 7.843e-05\n",
      "Epoch 1/1 | Batch 855/11038 | Loss: 0.1277 | LR: 7.843e-05\n",
      "Epoch 1/1 | Batch 856/11038 | Loss: 0.1808 | LR: 7.825e-05\n",
      "Epoch 1/1 | Batch 857/11038 | Loss: 0.1641 | LR: 7.825e-05\n",
      "Epoch 1/1 | Batch 858/11038 | Loss: 0.1249 | LR: 7.825e-05\n",
      "Epoch 1/1 | Batch 859/11038 | Loss: 0.1615 | LR: 7.825e-05\n",
      "Epoch 1/1 | Batch 860/11038 | Loss: 0.1532 | LR: 7.806e-05\n",
      "Epoch 1/1 | Batch 861/11038 | Loss: 0.1449 | LR: 7.806e-05\n",
      "Epoch 1/1 | Batch 862/11038 | Loss: 0.1705 | LR: 7.806e-05\n",
      "Epoch 1/1 | Batch 863/11038 | Loss: 0.1604 | LR: 7.806e-05\n",
      "Epoch 1/1 | Batch 864/11038 | Loss: 0.1825 | LR: 7.787e-05\n",
      "Epoch 1/1 | Batch 865/11038 | Loss: 0.1366 | LR: 7.787e-05\n",
      "Epoch 1/1 | Batch 866/11038 | Loss: 0.1486 | LR: 7.787e-05\n",
      "Epoch 1/1 | Batch 867/11038 | Loss: 0.1568 | LR: 7.787e-05\n",
      "Epoch 1/1 | Batch 868/11038 | Loss: 0.1345 | LR: 7.768e-05\n",
      "Epoch 1/1 | Batch 869/11038 | Loss: 0.1472 | LR: 7.768e-05\n",
      "Epoch 1/1 | Batch 870/11038 | Loss: 0.1560 | LR: 7.768e-05\n",
      "Epoch 1/1 | Batch 871/11038 | Loss: 0.1255 | LR: 7.768e-05\n",
      "Epoch 1/1 | Batch 872/11038 | Loss: 0.1578 | LR: 7.749e-05\n",
      "Epoch 1/1 | Batch 873/11038 | Loss: 0.1484 | LR: 7.749e-05\n",
      "Epoch 1/1 | Batch 874/11038 | Loss: 0.1679 | LR: 7.749e-05\n",
      "Epoch 1/1 | Batch 875/11038 | Loss: 0.1425 | LR: 7.749e-05\n",
      "Epoch 1/1 | Batch 876/11038 | Loss: 0.1635 | LR: 7.730e-05\n",
      "Epoch 1/1 | Batch 877/11038 | Loss: 0.1268 | LR: 7.730e-05\n",
      "Epoch 1/1 | Batch 878/11038 | Loss: 0.1372 | LR: 7.730e-05\n",
      "Epoch 1/1 | Batch 879/11038 | Loss: 0.1625 | LR: 7.730e-05\n",
      "Epoch 1/1 | Batch 880/11038 | Loss: 0.1259 | LR: 7.711e-05\n",
      "Epoch 1/1 | Batch 881/11038 | Loss: 0.1339 | LR: 7.711e-05\n",
      "Epoch 1/1 | Batch 882/11038 | Loss: 0.1361 | LR: 7.711e-05\n",
      "Epoch 1/1 | Batch 883/11038 | Loss: 0.1862 | LR: 7.711e-05\n",
      "Epoch 1/1 | Batch 884/11038 | Loss: 0.1451 | LR: 7.692e-05\n",
      "Epoch 1/1 | Batch 885/11038 | Loss: 0.1390 | LR: 7.692e-05\n",
      "Epoch 1/1 | Batch 886/11038 | Loss: 0.1261 | LR: 7.692e-05\n",
      "Epoch 1/1 | Batch 887/11038 | Loss: 0.1354 | LR: 7.692e-05\n",
      "Epoch 1/1 | Batch 888/11038 | Loss: 0.1298 | LR: 7.673e-05\n",
      "Epoch 1/1 | Batch 889/11038 | Loss: 0.1430 | LR: 7.673e-05\n",
      "Epoch 1/1 | Batch 890/11038 | Loss: 0.1475 | LR: 7.673e-05\n",
      "Epoch 1/1 | Batch 891/11038 | Loss: 0.1257 | LR: 7.673e-05\n",
      "Epoch 1/1 | Batch 892/11038 | Loss: 0.1326 | LR: 7.654e-05\n",
      "Epoch 1/1 | Batch 893/11038 | Loss: 0.1400 | LR: 7.654e-05\n",
      "Epoch 1/1 | Batch 894/11038 | Loss: 0.1532 | LR: 7.654e-05\n",
      "Epoch 1/1 | Batch 895/11038 | Loss: 0.1367 | LR: 7.654e-05\n",
      "Epoch 1/1 | Batch 896/11038 | Loss: 0.1434 | LR: 7.635e-05\n",
      "Epoch 1/1 | Batch 897/11038 | Loss: 0.1621 | LR: 7.635e-05\n",
      "Epoch 1/1 | Batch 898/11038 | Loss: 0.1575 | LR: 7.635e-05\n",
      "Epoch 1/1 | Batch 899/11038 | Loss: 0.1209 | LR: 7.635e-05\n",
      "Epoch 1/1 | Batch 900/11038 | Loss: 0.1401 | LR: 7.616e-05\n",
      "Hamming Loss: 0.0199\n",
      "\n",
      "Checkpoint saved at epoch 1 batch 900\n",
      "Epoch 1/1 | Batch 901/11038 | Loss: 0.1234 | LR: 7.616e-05\n",
      "Epoch 1/1 | Batch 902/11038 | Loss: 0.1428 | LR: 7.616e-05\n",
      "Epoch 1/1 | Batch 903/11038 | Loss: 0.1587 | LR: 7.616e-05\n",
      "Epoch 1/1 | Batch 904/11038 | Loss: 0.1328 | LR: 7.596e-05\n",
      "Epoch 1/1 | Batch 905/11038 | Loss: nan | LR: 7.596e-05\n",
      "Epoch 1/1 | Batch 906/11038 | Loss: 0.1340 | LR: 7.596e-05\n",
      "Epoch 1/1 | Batch 907/11038 | Loss: 0.1363 | LR: 7.596e-05\n",
      "Epoch 1/1 | Batch 908/11038 | Loss: 0.1390 | LR: 7.577e-05\n",
      "Epoch 1/1 | Batch 909/11038 | Loss: 0.1501 | LR: 7.577e-05\n",
      "Epoch 1/1 | Batch 910/11038 | Loss: 0.1414 | LR: 7.577e-05\n",
      "Epoch 1/1 | Batch 911/11038 | Loss: 0.1462 | LR: 7.577e-05\n",
      "Epoch 1/1 | Batch 912/11038 | Loss: 0.1439 | LR: 7.558e-05\n",
      "Epoch 1/1 | Batch 913/11038 | Loss: 0.1537 | LR: 7.558e-05\n",
      "Epoch 1/1 | Batch 914/11038 | Loss: 0.1436 | LR: 7.558e-05\n",
      "Epoch 1/1 | Batch 915/11038 | Loss: 0.1370 | LR: 7.558e-05\n",
      "Epoch 1/1 | Batch 916/11038 | Loss: 0.1433 | LR: 7.538e-05\n",
      "Epoch 1/1 | Batch 917/11038 | Loss: 0.1286 | LR: 7.538e-05\n",
      "Epoch 1/1 | Batch 918/11038 | Loss: 0.1353 | LR: 7.538e-05\n",
      "Epoch 1/1 | Batch 919/11038 | Loss: 0.1444 | LR: 7.538e-05\n",
      "Epoch 1/1 | Batch 920/11038 | Loss: 0.1359 | LR: 7.518e-05\n",
      "Epoch 1/1 | Batch 921/11038 | Loss: 0.1503 | LR: 7.518e-05\n",
      "Epoch 1/1 | Batch 922/11038 | Loss: 0.1445 | LR: 7.518e-05\n",
      "Epoch 1/1 | Batch 923/11038 | Loss: 0.1390 | LR: 7.518e-05\n",
      "Epoch 1/1 | Batch 924/11038 | Loss: 0.1540 | LR: 7.499e-05\n",
      "Epoch 1/1 | Batch 925/11038 | Loss: 0.1494 | LR: 7.499e-05\n",
      "Epoch 1/1 | Batch 926/11038 | Loss: 0.1252 | LR: 7.499e-05\n",
      "Epoch 1/1 | Batch 927/11038 | Loss: 0.1189 | LR: 7.499e-05\n",
      "Epoch 1/1 | Batch 928/11038 | Loss: 0.1335 | LR: 7.479e-05\n",
      "Epoch 1/1 | Batch 929/11038 | Loss: 0.1219 | LR: 7.479e-05\n",
      "Epoch 1/1 | Batch 930/11038 | Loss: 0.1694 | LR: 7.479e-05\n",
      "Epoch 1/1 | Batch 931/11038 | Loss: 0.1362 | LR: 7.479e-05\n",
      "Epoch 1/1 | Batch 932/11038 | Loss: 0.1308 | LR: 7.460e-05\n",
      "Epoch 1/1 | Batch 933/11038 | Loss: 0.1347 | LR: 7.460e-05\n",
      "Epoch 1/1 | Batch 934/11038 | Loss: 0.1336 | LR: 7.460e-05\n",
      "Epoch 1/1 | Batch 935/11038 | Loss: 0.1493 | LR: 7.460e-05\n",
      "Epoch 1/1 | Batch 936/11038 | Loss: 0.1336 | LR: 7.440e-05\n",
      "Epoch 1/1 | Batch 937/11038 | Loss: 0.1366 | LR: 7.440e-05\n",
      "Epoch 1/1 | Batch 938/11038 | Loss: 0.1225 | LR: 7.440e-05\n",
      "Epoch 1/1 | Batch 939/11038 | Loss: 0.1357 | LR: 7.440e-05\n",
      "Epoch 1/1 | Batch 940/11038 | Loss: 0.1283 | LR: 7.420e-05\n",
      "Epoch 1/1 | Batch 941/11038 | Loss: 0.1370 | LR: 7.420e-05\n",
      "Epoch 1/1 | Batch 942/11038 | Loss: 0.1312 | LR: 7.420e-05\n",
      "Epoch 1/1 | Batch 943/11038 | Loss: 0.1515 | LR: 7.420e-05\n",
      "Epoch 1/1 | Batch 944/11038 | Loss: 0.1415 | LR: 7.400e-05\n",
      "Epoch 1/1 | Batch 945/11038 | Loss: 0.1460 | LR: 7.400e-05\n",
      "Epoch 1/1 | Batch 946/11038 | Loss: 0.1245 | LR: 7.400e-05\n",
      "Epoch 1/1 | Batch 947/11038 | Loss: 0.1467 | LR: 7.400e-05\n",
      "Epoch 1/1 | Batch 948/11038 | Loss: 0.1331 | LR: 7.380e-05\n",
      "Epoch 1/1 | Batch 949/11038 | Loss: 0.1500 | LR: 7.380e-05\n",
      "Epoch 1/1 | Batch 950/11038 | Loss: 0.1335 | LR: 7.380e-05\n",
      "Epoch 1/1 | Batch 951/11038 | Loss: 0.1270 | LR: 7.380e-05\n",
      "Epoch 1/1 | Batch 952/11038 | Loss: 0.1411 | LR: 7.360e-05\n",
      "Epoch 1/1 | Batch 953/11038 | Loss: 0.1638 | LR: 7.360e-05\n",
      "Epoch 1/1 | Batch 954/11038 | Loss: 0.1523 | LR: 7.360e-05\n",
      "Epoch 1/1 | Batch 955/11038 | Loss: 0.1282 | LR: 7.360e-05\n",
      "Epoch 1/1 | Batch 956/11038 | Loss: 0.1340 | LR: 7.340e-05\n",
      "Epoch 1/1 | Batch 957/11038 | Loss: 0.1323 | LR: 7.340e-05\n",
      "Epoch 1/1 | Batch 958/11038 | Loss: 0.1968 | LR: 7.340e-05\n",
      "Epoch 1/1 | Batch 959/11038 | Loss: 0.1201 | LR: 7.340e-05\n",
      "Epoch 1/1 | Batch 960/11038 | Loss: 0.1276 | LR: 7.320e-05\n",
      "Epoch 1/1 | Batch 961/11038 | Loss: 0.1247 | LR: 7.320e-05\n",
      "Epoch 1/1 | Batch 962/11038 | Loss: 0.1194 | LR: 7.320e-05\n",
      "Epoch 1/1 | Batch 963/11038 | Loss: 0.1206 | LR: 7.320e-05\n",
      "Epoch 1/1 | Batch 964/11038 | Loss: 0.1184 | LR: 7.300e-05\n",
      "Epoch 1/1 | Batch 965/11038 | Loss: 0.1409 | LR: 7.300e-05\n",
      "Epoch 1/1 | Batch 966/11038 | Loss: 0.1176 | LR: 7.300e-05\n",
      "Epoch 1/1 | Batch 967/11038 | Loss: 0.1230 | LR: 7.300e-05\n",
      "Epoch 1/1 | Batch 968/11038 | Loss: 0.1597 | LR: 7.280e-05\n",
      "Epoch 1/1 | Batch 969/11038 | Loss: 0.1261 | LR: 7.280e-05\n",
      "Epoch 1/1 | Batch 970/11038 | Loss: 0.1346 | LR: 7.280e-05\n",
      "Epoch 1/1 | Batch 971/11038 | Loss: 0.1464 | LR: 7.280e-05\n",
      "Epoch 1/1 | Batch 972/11038 | Loss: 0.1346 | LR: 7.260e-05\n",
      "Epoch 1/1 | Batch 973/11038 | Loss: 0.1343 | LR: 7.260e-05\n",
      "Epoch 1/1 | Batch 974/11038 | Loss: 0.1526 | LR: 7.260e-05\n",
      "Epoch 1/1 | Batch 975/11038 | Loss: 0.1522 | LR: 7.260e-05\n",
      "Epoch 1/1 | Batch 976/11038 | Loss: 0.1465 | LR: 7.240e-05\n",
      "Epoch 1/1 | Batch 977/11038 | Loss: 0.1287 | LR: 7.240e-05\n",
      "Epoch 1/1 | Batch 978/11038 | Loss: 0.1657 | LR: 7.240e-05\n",
      "Epoch 1/1 | Batch 979/11038 | Loss: 0.1375 | LR: 7.240e-05\n",
      "Epoch 1/1 | Batch 980/11038 | Loss: 0.1706 | LR: 7.219e-05\n",
      "Epoch 1/1 | Batch 981/11038 | Loss: 0.1430 | LR: 7.219e-05\n",
      "Epoch 1/1 | Batch 982/11038 | Loss: 0.1622 | LR: 7.219e-05\n",
      "Epoch 1/1 | Batch 983/11038 | Loss: 0.1294 | LR: 7.219e-05\n",
      "Epoch 1/1 | Batch 984/11038 | Loss: 0.1245 | LR: 7.199e-05\n",
      "Epoch 1/1 | Batch 985/11038 | Loss: 0.1254 | LR: 7.199e-05\n",
      "Epoch 1/1 | Batch 986/11038 | Loss: 0.1504 | LR: 7.199e-05\n",
      "Epoch 1/1 | Batch 987/11038 | Loss: 0.1601 | LR: 7.199e-05\n",
      "Epoch 1/1 | Batch 988/11038 | Loss: 0.1308 | LR: 7.179e-05\n",
      "Epoch 1/1 | Batch 989/11038 | Loss: 0.1573 | LR: 7.179e-05\n",
      "Epoch 1/1 | Batch 990/11038 | Loss: 0.1182 | LR: 7.179e-05\n",
      "Epoch 1/1 | Batch 991/11038 | Loss: 0.1282 | LR: 7.179e-05\n",
      "Epoch 1/1 | Batch 992/11038 | Loss: 0.1274 | LR: 7.158e-05\n",
      "Epoch 1/1 | Batch 993/11038 | Loss: 0.1190 | LR: 7.158e-05\n",
      "Epoch 1/1 | Batch 994/11038 | Loss: 0.1268 | LR: 7.158e-05\n",
      "Epoch 1/1 | Batch 995/11038 | Loss: 0.1195 | LR: 7.158e-05\n",
      "Epoch 1/1 | Batch 996/11038 | Loss: 0.1366 | LR: 7.138e-05\n",
      "Epoch 1/1 | Batch 997/11038 | Loss: 0.1430 | LR: 7.138e-05\n",
      "Epoch 1/1 | Batch 998/11038 | Loss: 0.1151 | LR: 7.138e-05\n",
      "Epoch 1/1 | Batch 999/11038 | Loss: 0.1357 | LR: 7.138e-05\n",
      "Epoch 1/1 | Batch 1000/11038 | Loss: 0.1236 | LR: 7.117e-05\n",
      "Hamming Loss: 0.0199\n",
      "\n",
      "Checkpoint saved at epoch 1 batch 1000\n",
      "Epoch 1/1 | Batch 1001/11038 | Loss: 0.1281 | LR: 7.117e-05\n",
      "Epoch 1/1 | Batch 1002/11038 | Loss: 0.1165 | LR: 7.117e-05\n",
      "Epoch 1/1 | Batch 1003/11038 | Loss: 0.1606 | LR: 7.117e-05\n",
      "Epoch 1/1 | Batch 1004/11038 | Loss: 0.1342 | LR: 7.097e-05\n",
      "Epoch 1/1 | Batch 1005/11038 | Loss: 0.1341 | LR: 7.097e-05\n",
      "Epoch 1/1 | Batch 1006/11038 | Loss: 0.1554 | LR: 7.097e-05\n",
      "Epoch 1/1 | Batch 1007/11038 | Loss: 0.1237 | LR: 7.097e-05\n",
      "Epoch 1/1 | Batch 1008/11038 | Loss: 0.1648 | LR: 7.076e-05\n",
      "Epoch 1/1 | Batch 1009/11038 | Loss: 0.1299 | LR: 7.076e-05\n",
      "Epoch 1/1 | Batch 1010/11038 | Loss: 0.1236 | LR: 7.076e-05\n",
      "Epoch 1/1 | Batch 1011/11038 | Loss: 0.1584 | LR: 7.076e-05\n",
      "Epoch 1/1 | Batch 1012/11038 | Loss: 0.1265 | LR: 7.056e-05\n",
      "Epoch 1/1 | Batch 1013/11038 | Loss: 0.1203 | LR: 7.056e-05\n",
      "Epoch 1/1 | Batch 1014/11038 | Loss: 0.1140 | LR: 7.056e-05\n",
      "Epoch 1/1 | Batch 1015/11038 | Loss: 0.1254 | LR: 7.056e-05\n",
      "Epoch 1/1 | Batch 1016/11038 | Loss: 0.1204 | LR: 7.035e-05\n",
      "Epoch 1/1 | Batch 1017/11038 | Loss: 0.1424 | LR: 7.035e-05\n",
      "Epoch 1/1 | Batch 1018/11038 | Loss: 0.1170 | LR: 7.035e-05\n",
      "Epoch 1/1 | Batch 1019/11038 | Loss: nan | LR: 7.035e-05\n",
      "Epoch 1/1 | Batch 1020/11038 | Loss: 0.1676 | LR: 7.014e-05\n",
      "Epoch 1/1 | Batch 1021/11038 | Loss: 0.1578 | LR: 7.014e-05\n",
      "Epoch 1/1 | Batch 1022/11038 | Loss: 0.1537 | LR: 7.014e-05\n",
      "Epoch 1/1 | Batch 1023/11038 | Loss: 0.1190 | LR: 7.014e-05\n",
      "Epoch 1/1 | Batch 1024/11038 | Loss: 0.1361 | LR: 6.994e-05\n",
      "Epoch 1/1 | Batch 1025/11038 | Loss: 0.1263 | LR: 6.994e-05\n",
      "Epoch 1/1 | Batch 1026/11038 | Loss: 0.1187 | LR: 6.994e-05\n",
      "Epoch 1/1 | Batch 1027/11038 | Loss: 0.1278 | LR: 6.994e-05\n",
      "Epoch 1/1 | Batch 1028/11038 | Loss: 0.1422 | LR: 6.973e-05\n",
      "Epoch 1/1 | Batch 1029/11038 | Loss: 0.1227 | LR: 6.973e-05\n",
      "Epoch 1/1 | Batch 1030/11038 | Loss: 0.1362 | LR: 6.973e-05\n",
      "Epoch 1/1 | Batch 1031/11038 | Loss: 0.1250 | LR: 6.973e-05\n",
      "Epoch 1/1 | Batch 1032/11038 | Loss: 0.1452 | LR: 6.952e-05\n",
      "Epoch 1/1 | Batch 1033/11038 | Loss: 0.1260 | LR: 6.952e-05\n",
      "Epoch 1/1 | Batch 1034/11038 | Loss: 0.1115 | LR: 6.952e-05\n",
      "Epoch 1/1 | Batch 1035/11038 | Loss: 0.1348 | LR: 6.952e-05\n",
      "Epoch 1/1 | Batch 1036/11038 | Loss: 0.1262 | LR: 6.931e-05\n",
      "Epoch 1/1 | Batch 1037/11038 | Loss: 0.1541 | LR: 6.931e-05\n",
      "Epoch 1/1 | Batch 1038/11038 | Loss: 0.1361 | LR: 6.931e-05\n",
      "Epoch 1/1 | Batch 1039/11038 | Loss: 0.1364 | LR: 6.931e-05\n",
      "Epoch 1/1 | Batch 1040/11038 | Loss: 0.1318 | LR: 6.910e-05\n",
      "Epoch 1/1 | Batch 1041/11038 | Loss: 0.1174 | LR: 6.910e-05\n",
      "Epoch 1/1 | Batch 1042/11038 | Loss: 0.1435 | LR: 6.910e-05\n",
      "Epoch 1/1 | Batch 1043/11038 | Loss: 0.1397 | LR: 6.910e-05\n",
      "Epoch 1/1 | Batch 1044/11038 | Loss: 0.1386 | LR: 6.889e-05\n",
      "Epoch 1/1 | Batch 1045/11038 | Loss: 0.1272 | LR: 6.889e-05\n",
      "Epoch 1/1 | Batch 1046/11038 | Loss: 0.1337 | LR: 6.889e-05\n",
      "Epoch 1/1 | Batch 1047/11038 | Loss: 0.1187 | LR: 6.889e-05\n",
      "Epoch 1/1 | Batch 1048/11038 | Loss: 0.1154 | LR: 6.868e-05\n",
      "Epoch 1/1 | Batch 1049/11038 | Loss: 0.1222 | LR: 6.868e-05\n",
      "Epoch 1/1 | Batch 1050/11038 | Loss: 0.1373 | LR: 6.868e-05\n",
      "Epoch 1/1 | Batch 1051/11038 | Loss: 0.1183 | LR: 6.868e-05\n",
      "Epoch 1/1 | Batch 1052/11038 | Loss: 0.1030 | LR: 6.847e-05\n",
      "Epoch 1/1 | Batch 1053/11038 | Loss: 0.1445 | LR: 6.847e-05\n",
      "Epoch 1/1 | Batch 1054/11038 | Loss: 0.1256 | LR: 6.847e-05\n",
      "Epoch 1/1 | Batch 1055/11038 | Loss: 0.1251 | LR: 6.847e-05\n",
      "Epoch 1/1 | Batch 1056/11038 | Loss: 0.1360 | LR: 6.826e-05\n",
      "Epoch 1/1 | Batch 1057/11038 | Loss: 0.1160 | LR: 6.826e-05\n",
      "Epoch 1/1 | Batch 1058/11038 | Loss: 0.1272 | LR: 6.826e-05\n",
      "Epoch 1/1 | Batch 1059/11038 | Loss: 0.1272 | LR: 6.826e-05\n",
      "Epoch 1/1 | Batch 1060/11038 | Loss: 0.1215 | LR: 6.805e-05\n",
      "Epoch 1/1 | Batch 1061/11038 | Loss: 0.1413 | LR: 6.805e-05\n",
      "Epoch 1/1 | Batch 1062/11038 | Loss: 0.1266 | LR: 6.805e-05\n",
      "Epoch 1/1 | Batch 1063/11038 | Loss: 0.1393 | LR: 6.805e-05\n",
      "Epoch 1/1 | Batch 1064/11038 | Loss: 0.1148 | LR: 6.784e-05\n",
      "Epoch 1/1 | Batch 1065/11038 | Loss: 0.1439 | LR: 6.784e-05\n",
      "Epoch 1/1 | Batch 1066/11038 | Loss: 0.1218 | LR: 6.784e-05\n",
      "Epoch 1/1 | Batch 1067/11038 | Loss: 0.1151 | LR: 6.784e-05\n",
      "Epoch 1/1 | Batch 1068/11038 | Loss: 0.1211 | LR: 6.763e-05\n",
      "Epoch 1/1 | Batch 1069/11038 | Loss: 0.1658 | LR: 6.763e-05\n",
      "Epoch 1/1 | Batch 1070/11038 | Loss: 0.1550 | LR: 6.763e-05\n",
      "Epoch 1/1 | Batch 1071/11038 | Loss: 0.1152 | LR: 6.763e-05\n",
      "Epoch 1/1 | Batch 1072/11038 | Loss: 0.1398 | LR: 6.742e-05\n",
      "Epoch 1/1 | Batch 1073/11038 | Loss: 0.1330 | LR: 6.742e-05\n",
      "Epoch 1/1 | Batch 1074/11038 | Loss: 0.1119 | LR: 6.742e-05\n",
      "Epoch 1/1 | Batch 1075/11038 | Loss: 0.1185 | LR: 6.742e-05\n",
      "Epoch 1/1 | Batch 1076/11038 | Loss: 0.1878 | LR: 6.721e-05\n",
      "Epoch 1/1 | Batch 1077/11038 | Loss: 0.1551 | LR: 6.721e-05\n",
      "Epoch 1/1 | Batch 1078/11038 | Loss: 0.1384 | LR: 6.721e-05\n",
      "Epoch 1/1 | Batch 1079/11038 | Loss: 0.1085 | LR: 6.721e-05\n",
      "Epoch 1/1 | Batch 1080/11038 | Loss: 0.1110 | LR: 6.699e-05\n",
      "Epoch 1/1 | Batch 1081/11038 | Loss: 0.1118 | LR: 6.699e-05\n",
      "Epoch 1/1 | Batch 1082/11038 | Loss: 0.1326 | LR: 6.699e-05\n",
      "Epoch 1/1 | Batch 1083/11038 | Loss: 0.1652 | LR: 6.699e-05\n",
      "Epoch 1/1 | Batch 1084/11038 | Loss: 0.1338 | LR: 6.678e-05\n",
      "Epoch 1/1 | Batch 1085/11038 | Loss: 0.1405 | LR: 6.678e-05\n",
      "Epoch 1/1 | Batch 1086/11038 | Loss: 0.1235 | LR: 6.678e-05\n",
      "Epoch 1/1 | Batch 1087/11038 | Loss: 0.1339 | LR: 6.678e-05\n",
      "Epoch 1/1 | Batch 1088/11038 | Loss: 0.1163 | LR: 6.657e-05\n",
      "Epoch 1/1 | Batch 1089/11038 | Loss: 0.1111 | LR: 6.657e-05\n",
      "Epoch 1/1 | Batch 1090/11038 | Loss: 0.1259 | LR: 6.657e-05\n",
      "Epoch 1/1 | Batch 1091/11038 | Loss: 0.1222 | LR: 6.657e-05\n",
      "Epoch 1/1 | Batch 1092/11038 | Loss: 0.1178 | LR: 6.635e-05\n",
      "Epoch 1/1 | Batch 1093/11038 | Loss: 0.1051 | LR: 6.635e-05\n",
      "Epoch 1/1 | Batch 1094/11038 | Loss: 0.1139 | LR: 6.635e-05\n",
      "Epoch 1/1 | Batch 1095/11038 | Loss: 0.1108 | LR: 6.635e-05\n",
      "Epoch 1/1 | Batch 1096/11038 | Loss: 0.1199 | LR: 6.614e-05\n",
      "Epoch 1/1 | Batch 1097/11038 | Loss: 0.1212 | LR: 6.614e-05\n",
      "Epoch 1/1 | Batch 1098/11038 | Loss: 0.1393 | LR: 6.614e-05\n",
      "Epoch 1/1 | Batch 1099/11038 | Loss: 0.1260 | LR: 6.614e-05\n",
      "Epoch 1/1 | Batch 1100/11038 | Loss: 0.1492 | LR: 6.593e-05\n",
      "Hamming Loss: 0.0199\n",
      "\n",
      "Checkpoint saved at epoch 1 batch 1100\n",
      "Epoch 1/1 | Batch 1101/11038 | Loss: 0.1319 | LR: 6.593e-05\n",
      "Epoch 1/1 | Batch 1102/11038 | Loss: 0.1284 | LR: 6.593e-05\n",
      "Epoch 1/1 | Batch 1103/11038 | Loss: 0.1750 | LR: 6.593e-05\n",
      "Epoch 1/1 | Batch 1104/11038 | Loss: 0.1342 | LR: 6.571e-05\n",
      "Epoch 1/1 | Batch 1105/11038 | Loss: 0.1210 | LR: 6.571e-05\n",
      "Epoch 1/1 | Batch 1106/11038 | Loss: 0.1340 | LR: 6.571e-05\n",
      "Epoch 1/1 | Batch 1107/11038 | Loss: 0.1273 | LR: 6.571e-05\n",
      "Epoch 1/1 | Batch 1108/11038 | Loss: 0.1505 | LR: 6.550e-05\n",
      "Epoch 1/1 | Batch 1109/11038 | Loss: 0.1100 | LR: 6.550e-05\n",
      "Epoch 1/1 | Batch 1110/11038 | Loss: 0.1319 | LR: 6.550e-05\n",
      "Epoch 1/1 | Batch 1111/11038 | Loss: 0.1019 | LR: 6.550e-05\n",
      "Epoch 1/1 | Batch 1112/11038 | Loss: 0.1161 | LR: 6.528e-05\n",
      "Epoch 1/1 | Batch 1113/11038 | Loss: 0.1271 | LR: 6.528e-05\n",
      "Epoch 1/1 | Batch 1114/11038 | Loss: 0.1386 | LR: 6.528e-05\n",
      "Epoch 1/1 | Batch 1115/11038 | Loss: 0.1254 | LR: 6.528e-05\n",
      "Epoch 1/1 | Batch 1116/11038 | Loss: 0.1216 | LR: 6.506e-05\n",
      "Epoch 1/1 | Batch 1117/11038 | Loss: 0.1299 | LR: 6.506e-05\n",
      "Epoch 1/1 | Batch 1118/11038 | Loss: 0.1350 | LR: 6.506e-05\n",
      "Epoch 1/1 | Batch 1119/11038 | Loss: 0.1295 | LR: 6.506e-05\n",
      "Epoch 1/1 | Batch 1120/11038 | Loss: 0.1299 | LR: 6.485e-05\n",
      "Epoch 1/1 | Batch 1121/11038 | Loss: 0.1214 | LR: 6.485e-05\n",
      "Epoch 1/1 | Batch 1122/11038 | Loss: 0.1468 | LR: 6.485e-05\n",
      "Epoch 1/1 | Batch 1123/11038 | Loss: 0.1270 | LR: 6.485e-05\n",
      "Epoch 1/1 | Batch 1124/11038 | Loss: 0.1124 | LR: 6.463e-05\n",
      "Epoch 1/1 | Batch 1125/11038 | Loss: 0.1663 | LR: 6.463e-05\n",
      "Epoch 1/1 | Batch 1126/11038 | Loss: 0.1227 | LR: 6.463e-05\n",
      "Epoch 1/1 | Batch 1127/11038 | Loss: 0.1042 | LR: 6.463e-05\n",
      "Epoch 1/1 | Batch 1128/11038 | Loss: 0.1620 | LR: 6.442e-05\n",
      "Epoch 1/1 | Batch 1129/11038 | Loss: 0.1435 | LR: 6.442e-05\n",
      "Epoch 1/1 | Batch 1130/11038 | Loss: 0.1259 | LR: 6.442e-05\n",
      "Epoch 1/1 | Batch 1131/11038 | Loss: 0.1078 | LR: 6.442e-05\n",
      "Epoch 1/1 | Batch 1132/11038 | Loss: 0.1134 | LR: 6.420e-05\n",
      "Epoch 1/1 | Batch 1133/11038 | Loss: 0.1142 | LR: 6.420e-05\n",
      "Epoch 1/1 | Batch 1134/11038 | Loss: 0.1217 | LR: 6.420e-05\n",
      "Epoch 1/1 | Batch 1135/11038 | Loss: 0.1034 | LR: 6.420e-05\n",
      "Epoch 1/1 | Batch 1136/11038 | Loss: 0.1158 | LR: 6.398e-05\n",
      "Epoch 1/1 | Batch 1137/11038 | Loss: 0.1377 | LR: 6.398e-05\n",
      "Epoch 1/1 | Batch 1138/11038 | Loss: 0.0978 | LR: 6.398e-05\n",
      "Epoch 1/1 | Batch 1139/11038 | Loss: 0.1441 | LR: 6.398e-05\n",
      "Epoch 1/1 | Batch 1140/11038 | Loss: 0.1304 | LR: 6.377e-05\n",
      "Epoch 1/1 | Batch 1141/11038 | Loss: 0.1121 | LR: 6.377e-05\n",
      "Epoch 1/1 | Batch 1142/11038 | Loss: 0.1070 | LR: 6.377e-05\n",
      "Epoch 1/1 | Batch 1143/11038 | Loss: 0.1266 | LR: 6.377e-05\n",
      "Epoch 1/1 | Batch 1144/11038 | Loss: 0.1190 | LR: 6.355e-05\n",
      "Epoch 1/1 | Batch 1145/11038 | Loss: 0.1068 | LR: 6.355e-05\n",
      "Epoch 1/1 | Batch 1146/11038 | Loss: 0.1129 | LR: 6.355e-05\n",
      "Epoch 1/1 | Batch 1147/11038 | Loss: 0.1260 | LR: 6.355e-05\n",
      "Epoch 1/1 | Batch 1148/11038 | Loss: 0.1429 | LR: 6.333e-05\n",
      "Epoch 1/1 | Batch 1149/11038 | Loss: 0.1448 | LR: 6.333e-05\n",
      "Epoch 1/1 | Batch 1150/11038 | Loss: 0.1246 | LR: 6.333e-05\n",
      "Epoch 1/1 | Batch 1151/11038 | Loss: 0.1159 | LR: 6.333e-05\n",
      "Epoch 1/1 | Batch 1152/11038 | Loss: 0.1406 | LR: 6.311e-05\n",
      "Epoch 1/1 | Batch 1153/11038 | Loss: 0.1023 | LR: 6.311e-05\n",
      "Epoch 1/1 | Batch 1154/11038 | Loss: 0.1065 | LR: 6.311e-05\n",
      "Epoch 1/1 | Batch 1155/11038 | Loss: 0.1336 | LR: 6.311e-05\n",
      "Epoch 1/1 | Batch 1156/11038 | Loss: 0.1325 | LR: 6.289e-05\n",
      "Epoch 1/1 | Batch 1157/11038 | Loss: 0.1074 | LR: 6.289e-05\n",
      "Epoch 1/1 | Batch 1158/11038 | Loss: 0.1572 | LR: 6.289e-05\n",
      "Epoch 1/1 | Batch 1159/11038 | Loss: 0.1373 | LR: 6.289e-05\n",
      "Epoch 1/1 | Batch 1160/11038 | Loss: 0.1385 | LR: 6.267e-05\n",
      "Epoch 1/1 | Batch 1161/11038 | Loss: 0.1400 | LR: 6.267e-05\n",
      "Epoch 1/1 | Batch 1162/11038 | Loss: 0.1188 | LR: 6.267e-05\n",
      "Epoch 1/1 | Batch 1163/11038 | Loss: 0.1295 | LR: 6.267e-05\n",
      "Epoch 1/1 | Batch 1164/11038 | Loss: 0.1462 | LR: 6.246e-05\n",
      "Epoch 1/1 | Batch 1165/11038 | Loss: 0.1247 | LR: 6.246e-05\n",
      "Epoch 1/1 | Batch 1166/11038 | Loss: 0.1155 | LR: 6.246e-05\n",
      "Epoch 1/1 | Batch 1167/11038 | Loss: 0.1583 | LR: 6.246e-05\n",
      "Epoch 1/1 | Batch 1168/11038 | Loss: 0.1297 | LR: 6.224e-05\n",
      "Epoch 1/1 | Batch 1169/11038 | Loss: 0.1276 | LR: 6.224e-05\n",
      "Epoch 1/1 | Batch 1170/11038 | Loss: 0.1160 | LR: 6.224e-05\n",
      "Epoch 1/1 | Batch 1171/11038 | Loss: 0.1612 | LR: 6.224e-05\n",
      "Epoch 1/1 | Batch 1172/11038 | Loss: 0.1296 | LR: 6.202e-05\n",
      "Epoch 1/1 | Batch 1173/11038 | Loss: 0.1685 | LR: 6.202e-05\n",
      "Epoch 1/1 | Batch 1174/11038 | Loss: 0.1337 | LR: 6.202e-05\n",
      "Epoch 1/1 | Batch 1175/11038 | Loss: 0.1625 | LR: 6.202e-05\n",
      "Epoch 1/1 | Batch 1176/11038 | Loss: 0.1019 | LR: 6.180e-05\n",
      "Epoch 1/1 | Batch 1177/11038 | Loss: 0.1337 | LR: 6.180e-05\n",
      "Epoch 1/1 | Batch 1178/11038 | Loss: 0.1231 | LR: 6.180e-05\n",
      "Epoch 1/1 | Batch 1179/11038 | Loss: 0.1100 | LR: 6.180e-05\n",
      "Epoch 1/1 | Batch 1180/11038 | Loss: 0.1171 | LR: 6.158e-05\n",
      "Epoch 1/1 | Batch 1181/11038 | Loss: 0.1306 | LR: 6.158e-05\n",
      "Epoch 1/1 | Batch 1182/11038 | Loss: 0.1127 | LR: 6.158e-05\n",
      "Epoch 1/1 | Batch 1183/11038 | Loss: 0.1193 | LR: 6.158e-05\n",
      "Epoch 1/1 | Batch 1184/11038 | Loss: 0.1114 | LR: 6.136e-05\n",
      "Epoch 1/1 | Batch 1185/11038 | Loss: 0.1550 | LR: 6.136e-05\n",
      "Epoch 1/1 | Batch 1186/11038 | Loss: 0.0998 | LR: 6.136e-05\n",
      "Epoch 1/1 | Batch 1187/11038 | Loss: nan | LR: 6.136e-05\n",
      "Epoch 1/1 | Batch 1188/11038 | Loss: 0.1421 | LR: 6.114e-05\n",
      "Epoch 1/1 | Batch 1189/11038 | Loss: 0.1384 | LR: 6.114e-05\n",
      "Epoch 1/1 | Batch 1190/11038 | Loss: 0.1167 | LR: 6.114e-05\n",
      "Epoch 1/1 | Batch 1191/11038 | Loss: 0.1444 | LR: 6.114e-05\n",
      "Epoch 1/1 | Batch 1192/11038 | Loss: 0.1075 | LR: 6.092e-05\n",
      "Epoch 1/1 | Batch 1193/11038 | Loss: 0.1118 | LR: 6.092e-05\n",
      "Epoch 1/1 | Batch 1194/11038 | Loss: 0.1277 | LR: 6.092e-05\n",
      "Epoch 1/1 | Batch 1195/11038 | Loss: 0.1443 | LR: 6.092e-05\n",
      "Epoch 1/1 | Batch 1196/11038 | Loss: 0.1024 | LR: 6.070e-05\n",
      "Epoch 1/1 | Batch 1197/11038 | Loss: 0.1150 | LR: 6.070e-05\n",
      "Epoch 1/1 | Batch 1198/11038 | Loss: 0.1180 | LR: 6.070e-05\n",
      "Epoch 1/1 | Batch 1199/11038 | Loss: 0.1184 | LR: 6.070e-05\n",
      "Epoch 1/1 | Batch 1200/11038 | Loss: 0.1233 | LR: 6.047e-05\n",
      "Hamming Loss: 0.0199\n",
      "\n",
      "Checkpoint saved at epoch 1 batch 1200\n",
      "Epoch 1/1 | Batch 1201/11038 | Loss: 0.1263 | LR: 6.047e-05\n",
      "Epoch 1/1 | Batch 1202/11038 | Loss: 0.1107 | LR: 6.047e-05\n",
      "Epoch 1/1 | Batch 1203/11038 | Loss: 0.1026 | LR: 6.047e-05\n",
      "Epoch 1/1 | Batch 1204/11038 | Loss: 0.1359 | LR: 6.025e-05\n",
      "Epoch 1/1 | Batch 1205/11038 | Loss: 0.1347 | LR: 6.025e-05\n",
      "Epoch 1/1 | Batch 1206/11038 | Loss: 0.1332 | LR: 6.025e-05\n",
      "Epoch 1/1 | Batch 1207/11038 | Loss: 0.1113 | LR: 6.025e-05\n",
      "Epoch 1/1 | Batch 1208/11038 | Loss: 0.1323 | LR: 6.003e-05\n",
      "Epoch 1/1 | Batch 1209/11038 | Loss: 0.1064 | LR: 6.003e-05\n",
      "Epoch 1/1 | Batch 1210/11038 | Loss: 0.1071 | LR: 6.003e-05\n",
      "Epoch 1/1 | Batch 1211/11038 | Loss: 0.0969 | LR: 6.003e-05\n",
      "Epoch 1/1 | Batch 1212/11038 | Loss: 0.1421 | LR: 5.981e-05\n",
      "Epoch 1/1 | Batch 1213/11038 | Loss: 0.1308 | LR: 5.981e-05\n",
      "Epoch 1/1 | Batch 1214/11038 | Loss: 0.1119 | LR: 5.981e-05\n",
      "Epoch 1/1 | Batch 1215/11038 | Loss: 0.1292 | LR: 5.981e-05\n",
      "Epoch 1/1 | Batch 1216/11038 | Loss: 0.1217 | LR: 5.959e-05\n",
      "Epoch 1/1 | Batch 1217/11038 | Loss: 0.0941 | LR: 5.959e-05\n",
      "Epoch 1/1 | Batch 1218/11038 | Loss: 0.1224 | LR: 5.959e-05\n",
      "Epoch 1/1 | Batch 1219/11038 | Loss: 0.1117 | LR: 5.959e-05\n",
      "Epoch 1/1 | Batch 1220/11038 | Loss: 0.1209 | LR: 5.937e-05\n",
      "Epoch 1/1 | Batch 1221/11038 | Loss: 0.1079 | LR: 5.937e-05\n",
      "Epoch 1/1 | Batch 1222/11038 | Loss: 0.1571 | LR: 5.937e-05\n",
      "Epoch 1/1 | Batch 1223/11038 | Loss: 0.1228 | LR: 5.937e-05\n",
      "Epoch 1/1 | Batch 1224/11038 | Loss: 0.1319 | LR: 5.914e-05\n",
      "Epoch 1/1 | Batch 1225/11038 | Loss: 0.1088 | LR: 5.914e-05\n",
      "Epoch 1/1 | Batch 1226/11038 | Loss: 0.1085 | LR: 5.914e-05\n",
      "Epoch 1/1 | Batch 1227/11038 | Loss: 0.1176 | LR: 5.914e-05\n",
      "Epoch 1/1 | Batch 1228/11038 | Loss: 0.1123 | LR: 5.892e-05\n",
      "Epoch 1/1 | Batch 1229/11038 | Loss: 0.1180 | LR: 5.892e-05\n",
      "Epoch 1/1 | Batch 1230/11038 | Loss: 0.1072 | LR: 5.892e-05\n",
      "Epoch 1/1 | Batch 1231/11038 | Loss: 0.0983 | LR: 5.892e-05\n",
      "Epoch 1/1 | Batch 1232/11038 | Loss: 0.1131 | LR: 5.870e-05\n",
      "Epoch 1/1 | Batch 1233/11038 | Loss: 0.1339 | LR: 5.870e-05\n",
      "Epoch 1/1 | Batch 1234/11038 | Loss: 0.1405 | LR: 5.870e-05\n",
      "Epoch 1/1 | Batch 1235/11038 | Loss: 0.0983 | LR: 5.870e-05\n",
      "Epoch 1/1 | Batch 1236/11038 | Loss: 0.1225 | LR: 5.848e-05\n",
      "Epoch 1/1 | Batch 1237/11038 | Loss: 0.1028 | LR: 5.848e-05\n",
      "Epoch 1/1 | Batch 1238/11038 | Loss: 0.1661 | LR: 5.848e-05\n",
      "Epoch 1/1 | Batch 1239/11038 | Loss: 0.1268 | LR: 5.848e-05\n",
      "Epoch 1/1 | Batch 1240/11038 | Loss: 0.1010 | LR: 5.825e-05\n",
      "Epoch 1/1 | Batch 1241/11038 | Loss: 0.1415 | LR: 5.825e-05\n",
      "Epoch 1/1 | Batch 1242/11038 | Loss: 0.1214 | LR: 5.825e-05\n",
      "Epoch 1/1 | Batch 1243/11038 | Loss: 0.1196 | LR: 5.825e-05\n",
      "Epoch 1/1 | Batch 1244/11038 | Loss: 0.1399 | LR: 5.803e-05\n",
      "Epoch 1/1 | Batch 1245/11038 | Loss: 0.1190 | LR: 5.803e-05\n",
      "Epoch 1/1 | Batch 1246/11038 | Loss: 0.1312 | LR: 5.803e-05\n",
      "Epoch 1/1 | Batch 1247/11038 | Loss: 0.1307 | LR: 5.803e-05\n",
      "Epoch 1/1 | Batch 1248/11038 | Loss: 0.0928 | LR: 5.781e-05\n",
      "Epoch 1/1 | Batch 1249/11038 | Loss: 0.1421 | LR: 5.781e-05\n",
      "Epoch 1/1 | Batch 1250/11038 | Loss: 0.1471 | LR: 5.781e-05\n",
      "Epoch 1/1 | Batch 1251/11038 | Loss: 0.1270 | LR: 5.781e-05\n",
      "Epoch 1/1 | Batch 1252/11038 | Loss: 0.1169 | LR: 5.759e-05\n",
      "Epoch 1/1 | Batch 1253/11038 | Loss: 0.1308 | LR: 5.759e-05\n",
      "Epoch 1/1 | Batch 1254/11038 | Loss: 0.1100 | LR: 5.759e-05\n",
      "Epoch 1/1 | Batch 1255/11038 | Loss: 0.1082 | LR: 5.759e-05\n",
      "Epoch 1/1 | Batch 1256/11038 | Loss: 0.1162 | LR: 5.736e-05\n",
      "Epoch 1/1 | Batch 1257/11038 | Loss: 0.1263 | LR: 5.736e-05\n",
      "Epoch 1/1 | Batch 1258/11038 | Loss: 0.1669 | LR: 5.736e-05\n",
      "Epoch 1/1 | Batch 1259/11038 | Loss: 0.1208 | LR: 5.736e-05\n",
      "Epoch 1/1 | Batch 1260/11038 | Loss: 0.1009 | LR: 5.714e-05\n",
      "Epoch 1/1 | Batch 1261/11038 | Loss: 0.1067 | LR: 5.714e-05\n",
      "Epoch 1/1 | Batch 1262/11038 | Loss: 0.1121 | LR: 5.714e-05\n",
      "Epoch 1/1 | Batch 1263/11038 | Loss: 0.1285 | LR: 5.714e-05\n",
      "Epoch 1/1 | Batch 1264/11038 | Loss: 0.1205 | LR: 5.691e-05\n",
      "Epoch 1/1 | Batch 1265/11038 | Loss: 0.1002 | LR: 5.691e-05\n",
      "Epoch 1/1 | Batch 1266/11038 | Loss: 0.1519 | LR: 5.691e-05\n",
      "Epoch 1/1 | Batch 1267/11038 | Loss: 0.1324 | LR: 5.691e-05\n",
      "Epoch 1/1 | Batch 1268/11038 | Loss: 0.1271 | LR: 5.669e-05\n",
      "Epoch 1/1 | Batch 1269/11038 | Loss: 0.1097 | LR: 5.669e-05\n",
      "Epoch 1/1 | Batch 1270/11038 | Loss: 0.1034 | LR: 5.669e-05\n",
      "Epoch 1/1 | Batch 1271/11038 | Loss: 0.1192 | LR: 5.669e-05\n",
      "Epoch 1/1 | Batch 1272/11038 | Loss: 0.1051 | LR: 5.647e-05\n",
      "Epoch 1/1 | Batch 1273/11038 | Loss: 0.1230 | LR: 5.647e-05\n",
      "Epoch 1/1 | Batch 1274/11038 | Loss: 0.1148 | LR: 5.647e-05\n",
      "Epoch 1/1 | Batch 1275/11038 | Loss: 0.1127 | LR: 5.647e-05\n",
      "Epoch 1/1 | Batch 1276/11038 | Loss: 0.0986 | LR: 5.624e-05\n",
      "Epoch 1/1 | Batch 1277/11038 | Loss: 0.1352 | LR: 5.624e-05\n",
      "Epoch 1/1 | Batch 1278/11038 | Loss: 0.0981 | LR: 5.624e-05\n",
      "Epoch 1/1 | Batch 1279/11038 | Loss: 0.1044 | LR: 5.624e-05\n",
      "Epoch 1/1 | Batch 1280/11038 | Loss: 0.1529 | LR: 5.602e-05\n",
      "Epoch 1/1 | Batch 1281/11038 | Loss: 0.1054 | LR: 5.602e-05\n",
      "Epoch 1/1 | Batch 1282/11038 | Loss: 0.0976 | LR: 5.602e-05\n",
      "Epoch 1/1 | Batch 1283/11038 | Loss: 0.1365 | LR: 5.602e-05\n",
      "Epoch 1/1 | Batch 1284/11038 | Loss: 0.1257 | LR: 5.579e-05\n",
      "Epoch 1/1 | Batch 1285/11038 | Loss: nan | LR: 5.579e-05\n",
      "Epoch 1/1 | Batch 1286/11038 | Loss: 0.1112 | LR: 5.579e-05\n",
      "Epoch 1/1 | Batch 1287/11038 | Loss: 0.1120 | LR: 5.579e-05\n",
      "Epoch 1/1 | Batch 1288/11038 | Loss: 0.1118 | LR: 5.557e-05\n",
      "Epoch 1/1 | Batch 1289/11038 | Loss: 0.1010 | LR: 5.557e-05\n",
      "Epoch 1/1 | Batch 1290/11038 | Loss: 0.0937 | LR: 5.557e-05\n",
      "Epoch 1/1 | Batch 1291/11038 | Loss: 0.1035 | LR: 5.557e-05\n",
      "Epoch 1/1 | Batch 1292/11038 | Loss: 0.1002 | LR: 5.534e-05\n",
      "Epoch 1/1 | Batch 1293/11038 | Loss: 0.1089 | LR: 5.534e-05\n",
      "Epoch 1/1 | Batch 1294/11038 | Loss: 0.1274 | LR: 5.534e-05\n",
      "Epoch 1/1 | Batch 1295/11038 | Loss: 0.1276 | LR: 5.534e-05\n",
      "Epoch 1/1 | Batch 1296/11038 | Loss: 0.1221 | LR: 5.512e-05\n",
      "Epoch 1/1 | Batch 1297/11038 | Loss: 0.0998 | LR: 5.512e-05\n",
      "Epoch 1/1 | Batch 1298/11038 | Loss: 0.1047 | LR: 5.512e-05\n",
      "Epoch 1/1 | Batch 1299/11038 | Loss: 0.1001 | LR: 5.512e-05\n",
      "Epoch 1/1 | Batch 1300/11038 | Loss: 0.1028 | LR: 5.490e-05\n",
      "Hamming Loss: 0.0199\n",
      "\n",
      "Checkpoint saved at epoch 1 batch 1300\n",
      "Epoch 1/1 | Batch 1301/11038 | Loss: 0.1115 | LR: 5.490e-05\n",
      "Epoch 1/1 | Batch 1302/11038 | Loss: 0.1054 | LR: 5.490e-05\n",
      "Epoch 1/1 | Batch 1303/11038 | Loss: 0.1126 | LR: 5.490e-05\n",
      "Epoch 1/1 | Batch 1304/11038 | Loss: 0.1101 | LR: 5.467e-05\n",
      "Epoch 1/1 | Batch 1305/11038 | Loss: 0.1288 | LR: 5.467e-05\n",
      "Epoch 1/1 | Batch 1306/11038 | Loss: nan | LR: 5.467e-05\n",
      "Epoch 1/1 | Batch 1307/11038 | Loss: 0.1151 | LR: 5.467e-05\n",
      "Epoch 1/1 | Batch 1308/11038 | Loss: 0.1379 | LR: 5.445e-05\n",
      "Epoch 1/1 | Batch 1309/11038 | Loss: 0.1014 | LR: 5.445e-05\n",
      "Epoch 1/1 | Batch 1310/11038 | Loss: 0.1018 | LR: 5.445e-05\n",
      "Epoch 1/1 | Batch 1311/11038 | Loss: 0.1147 | LR: 5.445e-05\n",
      "Epoch 1/1 | Batch 1312/11038 | Loss: 0.1104 | LR: 5.422e-05\n",
      "Epoch 1/1 | Batch 1313/11038 | Loss: 0.0987 | LR: 5.422e-05\n",
      "Epoch 1/1 | Batch 1314/11038 | Loss: 0.1032 | LR: 5.422e-05\n",
      "Epoch 1/1 | Batch 1315/11038 | Loss: 0.0970 | LR: 5.422e-05\n",
      "Epoch 1/1 | Batch 1316/11038 | Loss: 0.1051 | LR: 5.400e-05\n",
      "Epoch 1/1 | Batch 1317/11038 | Loss: 0.1632 | LR: 5.400e-05\n",
      "Epoch 1/1 | Batch 1318/11038 | Loss: 0.1031 | LR: 5.400e-05\n",
      "Epoch 1/1 | Batch 1319/11038 | Loss: 0.1100 | LR: 5.400e-05\n",
      "Epoch 1/1 | Batch 1320/11038 | Loss: 0.0899 | LR: 5.377e-05\n",
      "Epoch 1/1 | Batch 1321/11038 | Loss: 0.1076 | LR: 5.377e-05\n",
      "Epoch 1/1 | Batch 1322/11038 | Loss: 0.1193 | LR: 5.377e-05\n",
      "Epoch 1/1 | Batch 1323/11038 | Loss: 0.1093 | LR: 5.377e-05\n",
      "Epoch 1/1 | Batch 1324/11038 | Loss: 0.1324 | LR: 5.355e-05\n",
      "Epoch 1/1 | Batch 1325/11038 | Loss: 0.1022 | LR: 5.355e-05\n",
      "Epoch 1/1 | Batch 1326/11038 | Loss: 0.1227 | LR: 5.355e-05\n",
      "Epoch 1/1 | Batch 1327/11038 | Loss: 0.1124 | LR: 5.355e-05\n",
      "Epoch 1/1 | Batch 1328/11038 | Loss: 0.1286 | LR: 5.332e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m model = ECGPatchCombined(d_input=\u001b[32m12\u001b[39m, d_model=\u001b[32m128\u001b[39m, num_classes=\u001b[32m94\u001b[39m, nhead=\u001b[32m4\u001b[39m, num_encoder_layers=\u001b[32m2\u001b[39m, dim_feedforward=\u001b[32m256\u001b[39m, num_patches=\u001b[32m100\u001b[39m).to(device)\n\u001b[32m      2\u001b[39m trainer = Trainer(model, device, accum_steps=\u001b[32m4\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, train_dataloader, test_dataloader, num_epochs, save_path)\u001b[39m\n\u001b[32m     74\u001b[39m inputs, labels = inputs.to(\u001b[38;5;28mself\u001b[39m.device), labels.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autocast(device_type=\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m, dtype=torch.float16):\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.loss(outputs, labels) / \u001b[38;5;28mself\u001b[39m.accum_steps\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dl-proj/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dl-proj/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mECGPatchCombined.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      9\u001b[39m x = \u001b[38;5;28mself\u001b[39m.splitter(x)\n\u001b[32m     10\u001b[39m x = \u001b[38;5;28mself\u001b[39m.patch_position(x)   \u001b[38;5;66;03m# Already (batch_size, seq_len, d_model)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dl-proj/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dl-proj/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mECGTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     encoded = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# encoded shape: (batch_size, seq_len, d_model)\u001b[39;00m\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Pick out only the last in the sequence for classification\u001b[39;00m\n\u001b[32m     18\u001b[39m     encoded = encoded[:, -\u001b[32m1\u001b[39m, :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dl-proj/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dl-proj/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dl-proj/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:438\u001b[39m, in \u001b[36mTransformerEncoder.forward\u001b[39m\u001b[34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    436\u001b[39m str_first_layer = \u001b[33m\"\u001b[39m\u001b[33mself.layers[0]\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    437\u001b[39m batch_first = first_layer.self_attn.batch_first\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m is_fastpath_enabled = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmha\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_fastpath_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fastpath_enabled:\n\u001b[32m    441\u001b[39m     why_not_sparsity_fast_path = (\n\u001b[32m    442\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtorch.backends.mha.get_fastpath_enabled() was not True\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    443\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dl-proj/.venv/lib/python3.12/site-packages/torch/backends/mha/__init__.py:9\u001b[39m, in \u001b[36mget_fastpath_enabled\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      6\u001b[39m _is_fastpath_enabled: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_fastpath_enabled\u001b[39m() -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m     10\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns whether fast path for TransformerEncoder and MultiHeadAttention\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m    is enabled, or ``True`` if jit is scripting.\u001b[39;00m\n\u001b[32m     12\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m \u001b[33;03m        ``True`` unless all conditions on inputs are met.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting():\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = ECGPatchCombined(d_input=12, d_model=128, num_classes=94, nhead=4, num_encoder_layers=2, dim_feedforward=256, num_patches=100).to(device)\n",
    "trainer = Trainer(model, device, accum_steps=4)\n",
    "trainer.train(train_dataloader, test_dataloader, num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume from a checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_from = \"training_progress/checkpoint_ep0_b1199.pt\"\n",
    "\n",
    "model = ECGCombined(d_input=12, d_model=128, num_classes=94, nhead=4, num_encoder_layers=2, dim_feedforward=256).to(device)\n",
    "trainer = Trainer(model, device, accum_steps=4, resume_checkpoint=resume_from, lr=1e-6)\n",
    "trainer.train(train_dataloader, test_dataloader, num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnYAAAHWCAYAAAD6oMSKAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAApPNJREFUeJzs3XdYk9fbB/BvEkjCHrKHojgQBygojjra0qJYq9ZBW62KfW1rpQvbKj/3Hq1Wq9ZVba3aaqtoW604cI+KgloVFRWRoSwXO4Tkef+IeUhIAkkIBMj9ua5cTU7Oc3ISsLk54z4chmEYEEIIIYSQRo9r7A4QQgghhBDDoMCOEEIIIaSJoMCOEEIIIaSJoMCOEEIIIaSJoMCOEEIIIaSJoMCOEEIIIaSJoMCOEEIIIaSJoMCOEEIIIaSJoMCOEEIIIaSJoMCOECMaP348fHx89Lp2zpw54HA4hu0Q0cuJEyfA4XBw4sQJY3eFEGLiKLAjRA0Oh6PVzVS/yMePHw9ra2tjd6NR++GHH8DhcBASEmLsrhBCmhAzY3eAkIZo27ZtSo9/+eUXHDlyRKW8ffv2tXqdTZs2QSqV6nXtjBkzMG3atFq9PjGeHTt2wMfHBwkJCbh79y5at25t7C4RQpoACuwIUWPMmDFKj//9918cOXJEpbyqkpISWFpaav065ubmevUPAMzMzGBmRv+EG6P79+/j3LlziI2NxYcffogdO3Zg9uzZxu6WWsXFxbCysjJ2NwghWqKpWEL01L9/f3Ts2BGJiYno27cvLC0t8b///Q8A8Oeff2LQoEHw8PCAQCCAr68v5s+fD4lEotRG1TV2aWlp4HA4+Pbbb7Fx40b4+vpCIBCgW7duuHjxotK16tbYcTgcREVFYd++fejYsSMEAgE6dOiAuLg4lf6fOHECwcHBEAqF8PX1xYYNGwy+bu+PP/5AUFAQLCws4OTkhDFjxiArK0upTnZ2NiIjI+Hl5QWBQAB3d3cMGTIEaWlpbJ1Lly4hLCwMTk5OsLCwQMuWLTFhwoQaX1/bn4P8Z5mcnIyXX34ZlpaW8PT0xLJly1TazMzMxNChQ2FlZQUXFxd88cUXEIlEOn0uO3bsgIODAwYNGoQRI0Zgx44daus9e/YMX3zxBXx8fCAQCODl5YWxY8ciPz+frVNWVoY5c+agbdu2EAqFcHd3x1tvvYV79+4B0Lz+T/679vPPP7Nl8in2e/fuITw8HDY2Nhg9ejQA4PTp0xg5ciSaN28OgUAAb29vfPHFFygtLVXp961btzBq1Cg4OzvDwsIC7dq1w/Tp0wEAx48fB4fDwd69e1Wu+/XXX8HhcHD+/HmdPk9CSCX6c5+QWnj8+DEGDhyIt99+G2PGjIGrqysA4Oeff4a1tTWio6NhbW2NY8eOYdasWSgoKMA333xTY7u//vorCgsL8eGHH4LD4WDZsmV46623kJqaWuMo35kzZxAbG4uPP/4YNjY2+P777zF8+HCkp6ejWbNmAIDLly9jwIABcHd3x9y5cyGRSDBv3jw4OzvX/kN54eeff0ZkZCS6deuGxYsXIycnB6tWrcLZs2dx+fJl2NvbAwCGDx+OGzdu4JNPPoGPjw9yc3Nx5MgRpKens49ff/11ODs7Y9q0abC3t0daWhpiY2O16oO2P4enT59iwIABeOuttzBq1Cjs3r0bU6dORadOnTBw4EAAQGlpKV599VWkp6fj008/hYeHB7Zt24Zjx47p9Nns2LEDb731Fvh8Pt555x2sW7cOFy9eRLdu3dg6RUVF6NOnD27evIkJEyaga9euyM/Px19//YXMzEw4OTlBIpHgjTfeQHx8PN5++2189tlnKCwsxJEjR3D9+nX4+vrq1C8AqKioQFhYGF566SV8++237Aj0H3/8gZKSEkyaNAnNmjVDQkICVq9ejczMTPzxxx/s9f/99x/69OkDc3NzfPDBB/Dx8cG9e/fw999/Y+HChejfvz+8vb2xY8cODBs2TOVz8fX1Rc+ePXXuNyHkBYYQUqPJkyczVf+59OvXjwHArF+/XqV+SUmJStmHH37IWFpaMmVlZWzZuHHjmBYtWrCP79+/zwBgmjVrxjx58oQt//PPPxkAzN9//82WzZ49W6VPABg+n8/cvXuXLbt69SoDgFm9ejVbNnjwYMbS0pLJyspiy+7cucOYmZmptKnOuHHjGCsrK43Pl5eXMy4uLkzHjh2Z0tJStnz//v0MAGbWrFkMwzDM06dPGQDMN998o7GtvXv3MgCYixcv1tivqrT9Och/lr/88gtbJhKJGDc3N2b48OFs2cqVKxkAzO+//86WFRcXM61bt2YAMMePH6+xT5cuXWIAMEeOHGEYhmGkUinj5eXFfPbZZ0r1Zs2axQBgYmNjVdqQSqUMwzDMli1bGADMihUrNNY5fvy42r7Jf9d++ukntmzcuHEMAGbatGkq7an7LBcvXsxwOBzmwYMHbFnfvn0ZGxsbpTLF/jAMw8TExDACgYB59uwZW5abm8uYmZkxs2fPVnkdQoj2aCqWkFoQCASIjIxUKbewsGDvFxYWIj8/H3369EFJSQlu3bpVY7sRERFwcHBgH/fp0wcAkJqaWuO1oaGhSiM1nTt3hq2tLXutRCLB0aNHMXToUHh4eLD1WrduzY5M1dalS5eQm5uLjz/+GEKhkC0fNGgQ/Pz8cODAAQCyz4nP5+PEiRN4+vSp2rbkI3v79++HWCzWqR+6/Bysra2V1lDy+Xx0795d6TP/559/4O7ujhEjRrBllpaW+OCDD7Tu044dO+Dq6oqXX34ZgGz6PCIiAjt37lSaIt6zZw8CAgJURrXk18jrODk54ZNPPtFYRx+TJk1SKVP8LIuLi5Gfn49evXqBYRhcvnwZAJCXl4dTp05hwoQJaN68ucb+jB07FiKRCLt372bLdu3ahYqKihrXsRJCqkeBHSG14OnpCT6fr1J+48YNDBs2DHZ2drC1tYWzszP7hfX8+fMa2636pSgP8jQFP9VdK79efm1ubi5KS0vV7sI01M7MBw8eAADatWun8pyfnx/7vEAgwNKlS3Hw4EG4urqib9++WLZsGbKzs9n6/fr1w/DhwzF37lw4OTlhyJAh+Omnn7Ra16bLz8HLy0slGFL83OTvq3Xr1ir11L1PdSQSCXbu3ImXX34Z9+/fx927d3H37l2EhIQgJycH8fHxbN179+6hY8eO1bZ37949tGvXzqCbaMzMzODl5aVSnp6ejvHjx8PR0RHW1tZwdnZGv379AFR+lvIguKZ++/n5oVu3bkprC3fs2IEePXrQ7mBCaokCO0JqQXEUQ+7Zs2fo168frl69innz5uHvv//GkSNHsHTpUgDQKr0Jj8dTW84wTJ1eawyff/45UlJSsHjxYgiFQsycORPt27dnR4E4HA52796N8+fPIyoqCllZWZgwYQKCgoJQVFSksV1dfw718bkdO3YMjx49ws6dO9GmTRv2NmrUKADQuImiNjSN3FXdQCInEAjA5XJV6r722ms4cOAApk6din379uHIkSPsxgt9UvaMHTsWJ0+eRGZmJu7du4d///2XRusIMQDaPEGIgZ04cQKPHz9GbGws+vbty5bfv3/fiL2q5OLiAqFQiLt376o8p65MHy1atAAA3L59G6+88orSc7dv32afl/P19cWUKVMwZcoU3LlzB4GBgVi+fDm2b9/O1unRowd69OiBhQsX4tdff8Xo0aOxc+dO/N///Z/aPtTFz6FFixa4fv06GIZRCphu376t1fU7duyAi4sL1q5dq/JcbGws9u7di/Xr18PCwgK+vr64fv16te35+vriwoULEIvFGjfVyEd7nz17plQuHzXVxrVr15CSkoKtW7di7NixbPmRI0eU6rVq1QoAauw3ALz99tuIjo7Gb7/9htLSUpibmyMiIkLrPhFC1KMRO0IMTD7yozjSU15ejh9++MFYXVLC4/EQGhqKffv24eHDh2z53bt3cfDgQYO8RnBwMFxcXLB+/XqlKdODBw/i5s2bGDRoEABZ3r+ysjKla319fWFjY8Ne9/TpU5VRs8DAQACodjq2Ln4O4eHhePjwodLasJKSEmzcuLHGa0tLSxEbG4s33ngDI0aMULlFRUWhsLAQf/31FwDZbuGrV6+qTQsif0/Dhw9Hfn4+1qxZo7FOixYtwOPxcOrUKaXndfkc1H2WDMNg1apVSvWcnZ3Rt29fbNmyBenp6Wr7I+fk5ISBAwdi+/bt2LFjBwYMGAAnJyet+0QIUY9G7AgxsF69esHBwQHjxo3Dp59+Cg6Hg23btjWoqdA5c+bg8OHD6N27NyZNmgSJRII1a9agY8eOuHLlilZtiMViLFiwQKXc0dERH3/8MZYuXYrIyEj069cP77zzDpvuxMfHB1988QUAICUlBa+++ipGjRoFf39/mJmZYe/evcjJycHbb78NANi6dSt++OEHDBs2DL6+vigsLMSmTZtga2uL8PBwjf2ri5/DxIkTsWbNGowdOxaJiYlwd3fHtm3btEpK/ddff6GwsBBvvvmm2ud79OgBZ2dn7NixAxEREfjqq6+we/dujBw5kp16fvLkCf766y+sX78eAQEBGDt2LH755RdER0cjISEBffr0QXFxMY4ePYqPP/4YQ4YMgZ2dHUaOHInVq1eDw+HA19cX+/fvR25urtbv28/PD76+vvjyyy+RlZUFW1tb7NmzR+2az++//x4vvfQSunbtig8++AAtW7ZEWloaDhw4oPK7NXbsWHYjyvz587XuDyGkGkbYiUtIo6Mp3UmHDh3U1j979izTo0cPxsLCgvHw8GC+/vpr5tChQyppJzSlO1GX/gOAUioITelOJk+erHJtixYtmHHjximVxcfHM126dGH4fD7j6+vL/Pjjj8yUKVMYoVCo4VOoJE+Loe7m6+vL1tu1axfTpUsXRiAQMI6Ojszo0aOZzMxM9vn8/Hxm8uTJjJ+fH2NlZcXY2dkxISEhSulEkpKSmHfeeYdp3rw5IxAIGBcXF+aNN95gLl26VGM/tf05aPpZVv35MAzDPHjwgHnzzTcZS0tLxsnJifnss8+YuLi4GtOdDB48mBEKhUxxcbHGOuPHj2fMzc2Z/Px8hmEY5vHjx0xUVBTj6enJ8Pl8xsvLixk3bhz7PMPI0pBMnz6dadmyJWNubs64ubkxI0aMYO7du8fWycvLY4YPH85YWloyDg4OzIcffshcv35dbboTTWlskpOTmdDQUMba2ppxcnJiJk6cyKbSUWyDYRjm+vXrzLBhwxh7e3tGKBQy7dq1Y2bOnKnSpkgkYhwcHBg7OzultDiEEP1xGKYBDSMQQoxq6NChuHHjBu7cuWPsrhATUFFRAQ8PDwwePBibN282dncIaRJojR0hJqrqUVB37tzBP//8g/79+xunQ8Tk7Nu3D3l5eUobMgghtUMjdoSYKHd3d4wfPx6tWrXCgwcPsG7dOohEIly+fBlt2rQxdvdIE3bhwgX8999/mD9/PpycnJCUlGTsLhHSZNDmCUJM1IABA/Dbb78hOzsbAoEAPXv2xKJFiyioI3Vu3bp12L59OwIDA9lceIQQw6ARO0IIIYSQJoLW2BFCCCGENBEU2BFCCCGENBEmt8auoqICly9fhqurq8p5iIQQQghp+KRSKXJyctClSxeYmZlcKFMtk/s0Ll++jO7duxu7G4QQQgippYSEBHTr1s3Y3WhQTC6wc3V1BSD7ZXB3dzdybwghhBCiq0ePHqF79+7sdzqpZHKBnXz61d3dHV5eXkbuDSGEEEL0RUuqVNEnQgghhBDSRFBgRwghhBDSRFBgRwghhBDSRFBgRwghhBDSRFBgRwghhBDSRFBgRwghhBDSRFBgRwghhBDSRFBgRwghhBDSRFBgRwghhBDSRDSIwG7t2rXw8fGBUChESEgIEhISNNbt378/OByOym3QoEH12GNCCCGEkIbH6IHdrl27EB0djdmzZyMpKQkBAQEICwtDbm6u2vqxsbF49OgRe7t+/Tp4PB5GjhxZzz0nhBBCCGlYjB7YrVixAhMnTkRkZCT8/f2xfv16WFpaYsuWLWrrOzo6ws3Njb0dOXIElpaWFNgRQgghRCNdZgfFYjHmzZsHX19fCIVCBAQEIC4uTqmORCLBzJkz0bJlS1hYWMDX1xfz588HwzB1/VaqZdTArry8HImJiQgNDWXLuFwuQkNDcf78ea3a2Lx5M95++21YWVmpfV4kEqGgoIC9FRYWGqTvhBBCCGkcdJ0dnDFjBjZs2IDVq1cjOTkZH330EYYNG4bLly+zdZYuXYp169ZhzZo1uHnzJpYuXYply5Zh9erV9fW21DIz5ovn5+dDIpHA1dVVqdzV1RW3bt2q8fqEhARcv34dmzdv1lhn8eLFmDt3bq37qo1jt3LQ3NEKrV2s6+X1CDGIkhLg4UNAKpU9ZhjZTfF+1ce63tflGkJI09K5M2Bra9QuKM4OAsD69etx4MABbNmyBdOmTVOpv23bNkyfPh3h4eEAgEmTJuHo0aNYvnw5tm/fDgA4d+4chgwZwq7x9/HxwW+//VbtSGB9MGpgV1ubN29Gp06d0L17d411YmJiEB0dzT7OysqCv7+/wftyNeMZJm1PgsCMiz2TeqGNq43BX4OQWhGLgTt3gOvXK2/XrgH37lFQRQipO2fPAr161UnThYWFKCgoYB8LBAIIBAKlOvLZwZiYGLasptlBkUgEoVCoVGZhYYEzZ86wj3v16oWNGzciJSUFbdu2xdWrV3HmzBmsWLHCEG9Nb0YN7JycnMDj8ZCTk6NUnpOTAzc3t2qvLS4uxs6dOzFv3rxq61X9ISv+AhiSl4MF2rra4FrWc6w7eQ8rRgXWyesQUiOpFHjwQDmAu34duHULKC9Xf42VFWBmBnA4ssccTvX3ta2n7zWEkKajSoBkSFUHambPno05c+YolekzOxgWFoYVK1agb9++8PX1RXx8PGJjYyGRSNg606ZNQ0FBAfz8/MDj8SCRSLBw4UKMHj3aMG9OT0YN7Ph8PoKCghAfH4+hQ4cCAKRSKeLj4xEVFVXttX/88QdEIhHGjBlTDz2tWTNrAcb38sGUP64ir1Bk7O4QU8AwQG6ubNRNMYC7cQMoKlJ/jbU10LGj8q1TJ8DFpX77TgghBpCcnAxPT0/2cdXROn2tWrUKEydOhJ+fHzgcDnx9fREZGam0sfP333/Hjh078Ouvv6JDhw64cuUKPv/8c3h4eGDcuHEG6Yc+jD4VGx0djXHjxiE4OBjdu3fHypUrUVxczM6Djx07Fp6enli8eLHSdZs3b8bQoUPRrFkzY3RbLRuh7OMsElUYuSekyXn+XBawVQ3i8vPV1+fzgfbtVYO45s0BrtE3wxNCiEHY2NjAtob1e/rMDjo7O2Pfvn0oKyvD48eP4eHhgWnTpqFVq1Zsna+++grTpk3D22+/DQDo1KkTHjx4gMWLF5t2YBcREYG8vDzMmjUL2dnZCAwMRFxcHDtkmp6eDm6VL6Lbt2/jzJkzOHz4sDG6rJG1PLAro8CO6Km0VDZlKl//Jg/gMjLU1+dwgNatZaNuigFc69aAuXn99p0QQhqg2swOCoVCeHp6QiwWY8+ePRg1ahT7XElJiUp8wuPxIJVvRDMSowd2ABAVFaXxwz1x4oRKWbt27YyeJ0YdG4Hsi5RG7IhWUlKA//5TDuLu3q3cnVqVl1fl1Kk8gGvfHrCwqN9+E0JII6Pr7OCFCxeQlZWFwMBAZGVlYc6cOZBKpfj666/ZNgcPHoyFCxeiefPm6NChAy5fvowVK1ZgwoQJRnmPcg0isGsqrAQ8ABTYkRpIpcAHHwCa0vQ4OlYGb/L/dugA2NvXazcJIaSp0HV2sKysDDNmzEBqaiqsra0RHh6Obdu2wV7h/8OrV6/GzJkz8fHHHyM3NxceHh748MMPMWvWrPp+e0o4TEMc+qpDmZmZ8Pb2RkZGBry8vAzadm5hGbovjAcArHm3C97o7GHQ9kkTwDDAZ58Bq1cDPB4QFKS8iaFjR8DVlXaHEkJINeryu7yxoxE7A7IWVH6cUb9eRjcfR7ja1t02b9IIzZkjC+o4HGDrVsDI2+IJIYQ0LbQ9zoAszHlKj1Ny6PgyouC77wB53sU1ayioI4QQYnAU2BkQp8r02f38YiP1hDQ4W7YA8hNQFi4EPv7YuP0hhBDSJFFgV4doEwUBAOzeDUycKLv/1VeAwrE2hBBCiCFRYFeHSkSSmiuRpu3QIeDdd2U7YSdOBJYupY0RhBBC6gwFdnWouJxG7EzauXPAW28BYjEwahSwbh0FdYQQQuoUBXZ1iEbsTNjVq0B4OFBSAgwcCGzbJktvQgghhNQhCuwM7PcPe7L3d13KQHmFcY8WIUaQkgK8/rrsfNeXXpKtsePzjd0rQgghJoACOwPr3tIR08Pbs4/P3tNwSDtpmjIygNdeA3JzgS5dgP37AUtLY/eKEEKIiaDArg48Ky1n79/Iem7EnpB6lZsrC+rS04F27YC4OMDOzti9IoQQYkIosKsD73Rvzt7PeFJqxJ6QevP8OTBgAHD7NtC8OXDkCODiYuxeEUIIMTEU2NUBLwdLLB8ZAADIeFpi5N6QOldSArzxBnD5siyYO3IE8PY2dq8IIYSYIArs6oibneyM2PwikZF7QupUeTkwYgRw5oxs2vXwYaBtW2P3ihBCiImiwK6OWPBlqS1KyinlSZMlkQDvvQccPCjbIHHgABAQYOxeEUIIMWEU2NURyxeBXZmYArsmiWGASZOA338HzM2B2Figd29j94oQQoiJo8CujliY04hdk8UwwNSpwKZNAJcL/PorEBZm7F4RQgghFNjVFflUbKlYAoZhjNwbYlBLlgDffCO7v2mTbI0dIYQQ0gBQYFdH5CN2DAOI6PSJpmPdOuB//5PdX74cmDDBuP0hhBBCFFBgV0cs+Wbs/SfF5dXUJI3Gjh3A5Mmy+zNnAtHRxu0PIYQQUgUFdnWEx+Ww93stOYZTKXlG7A2ptb//BsaNkw3BfvIJMHeusXtECCGEqKDArp5M3pFk7C4QfZ04AYwcWZneZOVKgMOp6SpCCCGk3lFgV08KRRXG7gLRx8WLwODBgEgEDBkCbNki2wlLCCGENED0DVWHvhnRmb3fr62zEXtC9HLjhuz816Ii4JVXgJ07ATOzmq8jhBBCjIQCuzo0MtgbMwa1BwBYCXhG7g3Ryf37wOuvA0+eAN27A/v2AUKhsXtFCCGEVIsCuzpmKzQHAIjElPKk0Xj0CAgNBR4+BDp2lB0ZZmNj7F4RQgghNaLAro4JzGUfcVkFnUDRKDx5IhupS00FWrUCDh8GHB2N3StCCCFEKxTY1TGBmewjphG7RqCwEBg4ELh+HfDwAI4eBdzdjd0rQgghRGsU2NUxwYsTKC49eGrknpBqlZUBQ4cCCQmyEbrDh4GWLY3dK0IIIUQnFNjVMfmIHQBcSH1sxJ4QjSoqgLffBo4dA6ytgbg4oEMHY/eKEEII0RkFdnXMnFf5EdOoXQMklcrOe/3zT0AgkJ0w0a2bsXtFCCGE6IUCuzpWXlG5to7Po4+7QWEY4PPPgW3bAB4P+OMPoH9/Y/eKEEII0RtFGnWse8vKHZX5xSIj9oSomD0bWL1adjzY1q2yEyYIIYSQRowCuzpmzuNi6gA/AEBeAQV2DcaKFcD8+bL7a9YAo0cbtz+EEEKIAVBgVw9cbAQAgNN3843cEwJAdt7rlCmy+wsXAh9/bNz+EEIIIQZCgV09cH4R2OUVinAt87mRe2Pidu8GJk6U3f/qKyAmxrj9IYQQUi/Wrl0LHx8fCIVChISEICEhQWNdsViMefPmwdfXF0KhEAEBAYiLi1Oq4+PjAw6Ho3KbPHlyXb+VatGJ5vXA0YrP3v/n+iN08rIzYm9MhFQKZGQAKSnKt/h42XMTJwJLl8rW1xFCCGnSdu3ahejoaKxfvx4hISFYuXIlwsLCcPv2bbi4uKjUnzFjBrZv345NmzbBz88Phw4dwrBhw3Du3Dl06dIFAHDx4kVIJJWnSl2/fh2vvfYaRo4cWW/vSx0OwzCMUXtQzzIzM+Ht7Y2MjAx4eXnVy2tWSKRoPf0gAGBC75aYNtAPfDMaLK01hgHy8lSDt5QU4O5dQKRhTWNEBLBjh2wnLCGEkEZH1+/ykJAQdOvWDWvWrAEASKVSeHt745NPPsG0adNU6nt4eGD69OlKo2/Dhw+HhYUFtm/frvY1Pv/8c+zfvx937twBx4iDBjRiVw/MeFzMfbMDZv91A1vO3seJlFwc+aIfeFwaLdJKQQFw545y4CZ//LyaqW1zc6B1a6BtW9mtTRvA3x/o1YtG6gghpAkoLCxEQUEB+1ggEEAgECjVKS8vR2JiImIUlt5wuVyEhobi/PnzatsViUQQCoVKZRYWFjhz5oza+uXl5di+fTuio6ONGtQBFNjVm6AWDuz91LxiPC0ph5O1oJorTIxIBNy7pxq4paQA2dmar+NwgBYtKgM3eRDXti3QvDlgRr/ihBDSVPn7+ys9nj17NubMmaNUlp+fD4lEAldXV6VyV1dX3Lp1S227YWFhWLFiBfr27QtfX1/Ex8cjNjZWaepV0b59+/Ds2TOMHz9e7/diKPStV0/audkoPS4Tq//laNIkEiA9XTVwS0kBHjyQrX3TxMVFOWiT33x9gSp/VRFCCDENycnJ8PT0ZB9XHa3T16pVqzBx4kT4+fmBw+HA19cXkZGR2LJli9r6mzdvxsCBA+Hh4WGQ168NCuzqiXmVUydKy00ksHv6FPj0UyApSbburbxcc10bG/XBW5s2gB1tOCGEEKLMxsYGtra21dZxcnICj8dDTk6OUnlOTg7c3NzUXuPs7Ix9+/ahrKwMjx8/hoeHB6ZNm4ZWrVqp1H3w4AGOHj2K2NhY/d+IAVFgZyTFphLYxcYCigtN+XzldW+KU6iurrT2jRBCiEHx+XwEBQUhPj4eQ4cOBSDbPBEfH4+oqKhqrxUKhfD09IRYLMaePXswatQolTo//fQTXFxcMGjQoLrovs4osKtH68d0xUfbkwAAJaIKI/emnqSlyf47fDjwzTeydW+0G5UQQkg9io6Oxrhx4xAcHIzu3btj5cqVKC4uRmRkJABg7Nix8PT0xOLFiwEAFy5cQFZWFgIDA5GVlYU5c+ZAKpXi66+/VmpXKpXip59+wrhx42DWQNZ0N4xemIgBHd3RztUGt3MKUWIqI3bp6bL/BgcDLVsaty+EEEJMUkREBPLy8jBr1ixkZ2cjMDAQcXFx7IaK9PR0cLmVS6bKysowY8YMpKamwtraGuHh4di2bRvs7e2V2j169CjS09MxYcKE+nw71aLArp41s+YDOUBxuYmM2MkDu+bNjdsPQgghJi0qKkrj1OuJEyeUHvfr1w/Jyck1tvn666+joaUDNnqWXF2O+ACAZ8+eYfLkyXB3d4dAIEDbtm3xzz//1FNva8+SL5uGNJkRu4wM2X+9vY3bD0IIIcQEGHXETtcjPsrLy/Haa6/BxcUFu3fvhqenJx48eKAyNNqQCcxkgV15RTWpPZoK+bFeAI3YEUIIIfXAqIHdihUrMHHiRHbx4vr163HgwAFs2bJF7REfW7ZswZMnT3Du3DmYm5sDkB3C25jIjxIzicAuN1eW3oTLBRpAbh9CCCGkqTPaVKz8iI/Q0NDKztRwxMdff/2Fnj17YvLkyXB1dUXHjh2xaNEijZmgAdmxIAUFBeytsLDQ4O9FF/wX+exEFSYwFSsfrXN3lx3vRQghhJA6ZbTArrojPrI1HCGVmpqK3bt3QyKR4J9//sHMmTOxfPlyLFiwQOPrLF68GHZ2duyt6vEj9U0+Yvft4RQcTc6poXYjRxsnCCGEkHpl9M0TupBKpXBxccHGjRsRFBSEiIgITJ8+HevXr9d4TUxMDJ4/f87etNnlUpd43MoEvP/3yyUj9qQeUGBHCCGE1CujrbHT54gPd3d3mJubg6eQ4LZ9+/bIzs5GeXk5+Hy+yjUCgUDp7LiCggIDvQP9SKQNa1t0naIdsYQQQki9MtqIneIRH3LyIz569uyp9prevXvj7t27kCocFp+SkgJ3d3e1QV1DVGFKgR2N2BFCCCH1yqhTsdHR0di0aRO2bt2KmzdvYtKkSSpHfMTExLD1J02ahCdPnuCzzz5DSkoKDhw4gEWLFmHy5MnGegs6q5BUBqVW/CZ+tBYFdoQQQki9Mmq6E12P+PD29sahQ4fwxRdfoHPnzvD09MRnn32GqVOnGust6ExxKra4XAKplAGX20QPvqepWEIIIaRecZiGdhZGHcvMzIS3tzcyMjLg5eVV76//6W+X8dfVh+zjBUM7YkyPFvXejzonEgFCoex+Xh7g5GTc/hBCCGkyjP1d3pA1ql2xTUHVzRMz9l3HnRzj5tarE5mZsv9aWADNmhm3L4QQQoiJoMCuntkIVWe/n5aIjdCTOqY4DctpolPNhBBCSANDgV09i369Lbr5OCiViSVN8Hgx2jhBCCGE1DsK7OqZi40Qf3zUC77OVmxZaXkTPF6MAjtCCCGk3lFgZyQ/juvG3i8VN8HAjnbEEkIIIfWOAjsjaelkhZfbOQNoooEdjdgRQggh9Y4COyOyeJGgWESBHSGEEEIMgAI7IxKaywK7JjdixzCVgR1NxRJCCCH1hgI7I7KQB3blTWxX7PPnQFGR7D4FdoQQQki9ocDOiCya6oidfLTOyQmwtDRuXwghhBATQoGdEcnX2K0/eQ/PS5tQkmLaEUsIIYQYBQV2RiRfYwcAuy6mG7EnBkYbJwghhBCjoMDOiBQDu2JRE5qOpcCOEEIIMQoK7IzIQiGwK2tK6+xoKpYQQggxCgrsjMiMx2HvbziV2nSmY2nEjhBCCDEKCuyMiVF+uPb4PeP0w9AosCOEEEKMggI7I5IyypHdo+elRuqJAUkkQFaW7D5NxRJCCCH1igI7I5JWGbETSxj1FRuT7GygogLg8QB3d2P3hhBCCDEpFNgZUWsXa2N3wfDk07BeXrLgjhBCCCH1hgI7I+re0hGh7V2VyqRVh/EaG9oRSwghhBgNBXZGNryrp9LjgrJGfgIFbZwghBBCjIYCOyPjcJQfn7idZ5yOGAoFdoQQQojRUGBnZL7OyuvsPt91xTgdMRSaiiWEEEKMhgI7I2vjaoMt44ON3Q3DoRE7QgghDdDatWvh4+MDoVCIkJAQJCQkaKwrFosxb948+Pr6QigUIiAgAHFxcSr1srKyMGbMGDRr1gwWFhbo1KkTLl26VJdvo0YU2DUAr/i5YuYb/uzj8gqpEXtTSxTYEUIIaWB27dqF6OhozJ49G0lJSQgICEBYWBhyc3PV1p8xYwY2bNiA1atXIzk5GR999BGGDRuGy5cvs3WePn2K3r17w9zcHAcPHkRycjKWL18OBweH+npbanEYhmnk2zB1k5mZCW9vb2RkZMDLy8vY3WGViSUIWRSP56ViTA9vj4l9Wxm7S7orLQUsLWX3nzwBjPzLTQghpGnS9bs8JCQE3bp1w5o1awAAUqkU3t7e+OSTTzBt2jSV+h4eHpg+fTomT57Mlg0fPhwWFhbYvn07AGDatGk4e/YsTp8+baB3ZRg0YtdACM15+DKsHQBgT1KmkXujJ/n6OmtrwN7eqF0hhBDS9BUWFqKgoIC9iUQilTrl5eVITExEaGgoW8blchEaGorz58+rbVckEkEoFCqVWVhY4MyZM+zjv/76C8HBwRg5ciRcXFzQpUsXbNq0yUDvTH8U2DUggzrJTmq4lV3YONOeKE7DVt3uSwghhBiYv78/7Ozs2NvixYtV6uTn50MikcDVVTlvrKurK7Kzs9W2GxYWhhUrVuDOnTuQSqU4cuQIYmNj8ejRI7ZOamoq1q1bhzZt2uDQoUOYNGkSPv30U2zdutWwb1JHZkZ9daLE0YoPKz4PxeUSPC4qh63Q3Nhd0g3tiCWEEFKPkpOT4elZmQ9WIBAYpN1Vq1Zh4sSJ8PPzA4fDga+vLyIjI7Flyxa2jlQqRXBwMBYtWgQA6NKlC65fv47169dj3LhxBumHPmjEroGx4MuO4dp/9aGRe6IH2jhBCCGkHtnY2MDW1pa9qQvsnJycwOPxkJOTo1Sek5MDNzc3te06Oztj3759KC4uxoMHD3Dr1i1YW1ujVavK9e/u7u7w9/dXuq59+/ZIl38XGgkFdg1MflE5AGD5kRQj90QPFNgRQghpYPh8PoKCghAfH8+WSaVSxMfHo2fPntVeKxQK4enpiYqKCuzZswdDhgxhn+vduzdu376tVD8lJQUtWrQw7BvQEU3FEsOhqVhCCCENUHR0NMaNG4fg4GB0794dK1euRHFxMSIjIwEAY8eOhaenJ7tG78KFC8jKykJgYCCysrIwZ84cSKVSfP3112ybX3zxBXr16oVFixZh1KhRSEhIwMaNG7Fx40ajvEc5CuwamND2Ljh6U31enQaPRuwIIYQ0QBEREcjLy8OsWbOQnZ2NwMBAxMXFsRsq0tPTweVWTmKWlZVhxowZSE1NhbW1NcLDw7Ft2zbYK2R86NatG/bu3YuYmBjMmzcPLVu2xMqVKzF69Oj6fntKKI9dA5OU/hRv/XAO3o4WOP31K8bujvYYBrCykuWyu3sX8PU1do8IIYQ0UQ39u9yYaI1dAyM0k22eyHhSitb/+wcrjqSgUcTeT57IgjoAoH9khBBCiFFQYNfACMwrfyQVUgbfx9/BhftPjNgjLcmnYV1dAQNtNyeEEEKIbiiwa2CE5jyVsgpJIxixo/V1hBBCiNFRYNfACMxUfyQ8biM4xYF2xBJCCCFGR4FdA6NuxE4skRqhJzqiETtCCCHE6Ciwa2DUjdhRYEcIIYQQbVBg18CY81R/JF/suoLyigYe3NFULCGEEGJ0FNg1AgVlFfgjMcPY3agejdgRQgghRkeBXQP028QeKmX5heVG6ImWKiqAhw9l9ymwI4QQQoyGArsGqKdvM5UyTkPeGPvwISCVAubmgIuLsXtDCCGEmCwK7BqJhhzXsdOw3t4Al36lCCGEEGOhb+FGgsMBCsrExu6GerS+jhBCCNFJv3798Msvv6BUfhyngVBg10h8ezgFneccxtWMZ8buiiraEUsIIYTopEuXLvjyyy/h5uaGiRMn4t9//zVIuw0isFu7di18fHwgFAoREhKChIQEjXV//vlncDgcpZtQKKzH3hrXhlP3jN0FVTRiRwghhOhk5cqVePjwIX766Sfk5uaib9++8Pf3x7fffoucnBy92zV6YLdr1y5ER0dj9uzZSEpKQkBAAMLCwpCbm6vxGltbWzx69Ii9PXjwoB57XD9Gh6gPkswa4ho2CuwIIYQQnZmZmeGtt97Cn3/+iczMTLz77ruYOXMmvL29MXToUBw7dkznNo0eJaxYsQITJ05EZGQk/P39sX79elhaWmLLli0ar+FwOHBzc2Nvrq6u9djj+jH3zQ5wsRGolKtLYGx0NBVLCCGE6C0hIQGzZ8/G8uXL4eLigpiYGDg5OeGNN97Al19+qVNbRo0SysvLkZiYiNDQULaMy+UiNDQU58+f13hdUVERWrRoAW9vbwwZMgQ3btzQWFckEqGgoIC9FRYWGvQ91BUzHhfTB7VXKTfnNcD9sTRiRwghhOgkNzcXy5cvR8eOHdGnTx/k5eXht99+Q1paGubOnYsff/wRhw8fxvr163Vq16yO+quV/Px8SCQSlRE3V1dX3Lp1S+017dq1w5YtW9C5c2c8f/4c3377LXr16oUbN27Ay8tLpf7ixYsxd+7cOul/XRsS6ImScgliYq+xZWYNLbArKgKePpXdpxE7QgghRCteXl7w9fXFhAkTMH78eDg7O6vU6dy5M7p166ZTu0YN7PTRs2dP9OzZk33cq1cvtG/fHhs2bMD8+fNV6sfExCA6Opp9nJWVBX9//3rpqyG82l454W+Dm4qVT8Pa2QG2tsbtCyGEENJIxMfHo0+fPtXWsbW1xfHjx3Vq16hRgpOTE3g8nsruj5ycHLi5uWnVhrm5Obp06YK7d++qfV4gEMDW1pa92djY1Lrf9cnFRnnHrzmPi7u5hbiU9sRIPaqCpmEJIYQQnXl5eeHOnTsq5Xfu3EFaWpre7Ro1sOPz+QgKCkJ8fDxbJpVKER8frzQqVx2JRIJr167B3d29rrppdMtHBrD3N55KReiKUxix/jyynhk2qaFeKLAjhBBCdDZ+/HicO3dOpfzChQsYP3683u0afV4vOjoamzZtwtatW3Hz5k1MmjQJxcXFiIyMBACMHTsWMTExbP158+bh8OHDSE1NRVJSEsaMGYMHDx7g//7v/4z1Furc8CAvWAtUZ813JqQboTdV0I5YQgghRGeXL19G7969Vcp79OiBK1eu6N2u0dfYRUREIC8vD7NmzUJ2djYCAwMRFxfHbqhIT08HVyF329OnTzFx4kRkZ2fDwcEBQUFBOHfuXKNaN6ePcb1aYO1x5eTEq4/dRU5BGT7o2wqtXYw0xUwjdoQQQojOOByO2kwdz58/h0Qi0b9dhmGY2nSsscnMzIS3tzcyMjLU7qJtqKbvvYYdF9SP0DWz4iNx5mv13KMXXnkFOH4c2L4dGD3aOH0ghBBiUhrrd7miwYMHw8LCAr/99ht4PB4A2fKyiIgIFBcX4+DBg3q1a/QRO6KdYlGFxuceF5fXY0+qoKlYQgghRGdLly5F37590a5dO3Z37OnTp1FQUKDXiRNyRl9jR7RTWKY5sDMaqbQysKOpWEIIIURr/v7++O+//zBq1Cjk5uaisLAQY8eOxa1bt9CxY0e926URu0aisJoRO6PJywNEIoDDATw9jd0bQgghpFHx8PDAokWLDNomBXaNRESwNxLuN5DcdXLy0Tp3d8Dc3Lh9IYQQQhqhkpISpKeno7xceVlV586d9WqPArtG4q2unmjtYo0ha88auyuVaEcsIYQQope8vDxERkZq3CSh785YWmPXSHA4HAR42xu7G8rkgR1tnCCEEEJ08vnnn+PZs2e4cOECLCwsEBcXh61bt6JNmzb466+/9G6XRuyI/mjjBCGEEKKXY8eO4c8//0RwcDC4XC5atGiB1157Dba2tli8eDEGDRqkV7s0YtfIWPJ5xu5CJZqKJYQQQvRSXFwMFxcXAICDgwPy8vIAAJ06dUJSUpLe7VJg18i42QnVll/JeIZ6zzVNU7GEEEKIXtq1a4fbt28DAAICArBhwwZkZWVh/fr1cHd317tdCuwamQ/7tlJbPnTtWRy/nVu/naGpWEIIIY3E2rVr4ePjA6FQiJCQECQkJGisKxaLMW/ePPj6+kIoFCIgIABxcXFKdebMmQMOh6N08/Pz07o/n332GR49egQAmD17Ng4ePIjmzZvj+++/r1UKFJ0Du61bt+LAgQPs46+//hr29vbo1asXHjx4oHdHiHZGBXvj/Zdaqn3urysP668jIhHw4heSAjtCCCEN2a5duxAdHY3Zs2cjKSkJAQEBCAsLQ26u+gGRGTNmYMOGDVi9ejWSk5Px0UcfYdiwYbh8+bJSvQ4dOuDRo0fs7cyZM1r3acyYMRg/fjwAICgoCA8ePMDFixeRkZGBiIgIvd+rzoHdokWLYGFhAQA4f/481q5di2XLlsHJyQlffPGF3h0h2uFwOJj5hj9SF4VjQm/lAK9CWo9TsVlZsv8KhYCTU/29LiGEEKKjFStWYOLEiYiMjIS/vz/Wr18PS0tLbNmyRW39bdu24X//+x/Cw8PRqlUrTJo0CeHh4Vi+fLlSPTMzM7i5ubE3Jy2/D8ViMXx9fXHz5k22zNLSEl27dtW6DU10DuwyMjLQunVrAMC+ffswfPhwfPDBB1i8eDFOnz5dq84Q7XG5HMSEKw/5VkjqMbBTPCOWw6m/1yWEEEJ0UF5ejsTERISGhrJlXC4XoaGhOH/+vNprRCIRhELlNe0WFhYqI3J37tyBh4cHWrVqhdGjRyNdvva8Bubm5igrK9PxnWhH58DO2toajx8/BgAcPnwYr732GgBAKBSitLTUsL0j1TLnKf/4KqTS+ntx2hFLCCHEyAoLC1FQUMDeRCKRSp38/HxIJBK4uroqlbu6uiI7O1ttu2FhYVixYgXu3LkDqVSKI0eOIDY2ll0TBwAhISH4+eefERcXh3Xr1uH+/fvo06cPCgsLter75MmTsXTpUlRUGPbIUJ3z2L322mv4v//7P3Tp0gUpKSkIDw8HANy4cQM+Pj4G7RzRzdGbuXhSXA5HK37dvxjtiCWEEGJk/v7+So9nz56NOXPm1LrdVatWYeLEifDz8wOHw4Gvry8iIyOVpm4HDhzI3u/cuTNCQkLQokUL/P7773j//fdrfI2LFy8iPj4ehw8fRqdOnWBlZaX0fGxsrF591zmwW7t2LWbMmIGMjAzs2bMHzZo1AwAkJibinXfe0asTxHBGrD+HY1P61/0L0Y5YQgghRpacnAxPT0/2sUAgUKnj5OQEHo+HnJwcpfKcnBy4ubmpbdfZ2Rn79u1DWVkZHj9+DA8PD0ybNg2tWqnPTAEA9vb2aNu2Le7evatV3+3t7TF8+HCt6upC58DO3t4ea9asUSmfO3euQTpEaic1r7h+XoimYgkhhBiZjY0NbG1tq63D5/MRFBSE+Ph4DB06FAAglUoRHx+PqKioaq8VCoXw9PSEWCzGnj17MGrUKI11i4qKcO/ePbz33nta9f2nn37Sqp6udF5jFxcXp7R4cO3atQgMDMS7776Lp0+fGrRzRD8VknpYa0dTsYQQQhqJ6OhobNq0CVu3bsXNmzcxadIkFBcXIzIyEgAwduxYxMTEsPUvXLiA2NhYpKam4vTp0xgwYACkUim+/vprts6XX36JkydPIi0tDefOncOwYcPA4/GMPnup84jdV199haVLlwIArl27hilTpiA6OhrHjx9HdHR0nUWgRL1X/Fxw7JZyHp51J+7h/T4tYcmvw6OAaSqWEEJIIxEREYG8vDzMmjUL2dnZCAwMRFxcHLuhIj09HVxu5VhXWVkZZsyYgdTUVFhbWyM8PBzbtm2Dvb09WyczMxPvvPMOHj9+DGdnZ7z00kv4999/4ezsrFWfWrZsCU41WSVSU1P1eq8cRsdzqKytrXH9+nX4+Phgzpw5uH79Onbv3o2kpCSEh4dr3GHSUGRmZsLb2xsZGRnw8vIydndq7dy9fLy76YJK+Xs9WmD+0I5186LPnwPyX+6iIqDKgk9CCCGkLjWF7/JVq1YpPRaLxbh8+TLi4uLw1VdfYdq0aXq1q/OQDp/PR0lJCQDg6NGjGDt2LADA0dERBQUFenWC6K+9m/q1BQevZ9ddYCefhnV0pKCOEEII0cNnn32mtnzt2rW4dOmS3u3qvMbupZdeQnR0NObPn4+EhAQMGjQIAJCSktJoo+bGzMGKj5/Gd1PzTB0mK6ZpWEIIIaRODBw4EHv27NH7ep0DuzVr1sDMzAy7d+/GunXr2G3GBw8exIABA/TuCNFfaxdrlTLdJth1RDtiCSGEkDqxe/duODo66n29zlOxzZs3x/79+1XKv/vuO707QWrHztJcpUxal5Ed7YglhBBCaqVLly5KmycYhkF2djby8vLwww8/6N2uXtsmJRIJ9u3bxx5e26FDB7z55pvg8Xh6d4Toz1Zojo3vBeGDbYlsWZ2eGktTsYQQQkityHPqyXG5XDg7O6N///7w8/NTf5EWdA7s7t69i/DwcGRlZaFdu3YAgMWLF8Pb2xsHDhyAr6+v3p0h+nu9gxv2f/IS3lgtyzFYUi6puxejqVhCCCGkVmbPnl0n7eq8xu7TTz+Fr68vMjIykJSUhKSkJKSnp6Nly5b49NNP66KPREsdPe3g5WABACivkOJublHdvBBNxRJCCCG18s8//+DQoUMq5YcOHcLBgwf1blfnwO7kyZNYtmyZ0sK+Zs2aYcmSJTh58qTeHSGGcfzL/uz9bw7dMvwLSCRAVpbsPo3YEUIIIXqZNm0aJBLV2TWGYfTOYQfoEdgJBAIUFhaqlBcVFYHP5+vdEWIY5jwuzHmyxZjSulhol5MDiMUAjwe4u9fBCxBCCCFN3507d+Dv769S7ufnh7t37+rdrs6B3RtvvIEPPvgAFy5cAMMwYBgG//77Lz766CO8+eabeneEGM7yUYEAgOelYsM3Lp+G9fAAzOrwyDJCCCGkCbOzs1N7bNjdu3dhVYvk/zoHdt9//z18fX3Rs2dPCIVCCIVC9O7dG61bt8bKlSv17ggxHHc7IQAg+3mZ4RunHbGEEEJIrQ0ZMgSff/457t27x5bdvXsXU6ZMqdVAmc5DLvb29vjzzz9x9+5dNt1J+/bt0bp1a707QQzLzVYW2KU/KcH1rOfo6GlnuMZpRywhhBBSa8uWLcOAAQPg5+fHntyVmZmJPn364Ntvv9W7Xb3n0lq3bq0UzP33338IDg5GeXm53p0hhuH6IrADgPc2X8DlWa8brnHaEUsIIYTUmp2dHc6dO4cjR47g6tWrsLCwQOfOndG3b99atWuwRVIMw6jd3UHqH9+scob9WZV1dnsSM5FTWIaP++s5wkpTsYQQQohBcDgcvP7663j9dcMNwOi8xo40Dh/2bQUA6OihPA075Y+rWBZ3Gyk5qjubtUJTsYQQQkitffrpp/j+++9VytesWYPPP/9c73YpsGuiBgd4AAByCspwL68IWc9KwSicH6v3jlmaiiWEEEJqbc+ePejdu7dKea9evbB7926929V6KragoKDa59XltiPG42AlyymYWyjCq8tliaNjP+7FPs9Re1UNSkuBvDzZfRqxI4QQQvT2+PFj2Nmpbm60tbVFfn6+3u1qHdjZ29uDw9EcDjAMU+3zpH7ZW5irlL31wzn2/uKDtzCxT0sM6KhDkuHMTNl/rawAB4fadpEQQggxWa1bt0ZcXByioqKUyg8ePIhWrVrp3a7Wgd3x48f1fhFS/yz5vGqfT3zwFIkPniJtySDtG1WchqUgnhBCCNFbdHQ0oqKikJeXh1deeQUAEB8fj+XLl9cqL7DWgV2/fv30fhFS/+pk9JR2xBJCCCEGMWHCBIhEIixcuBDz588HAPj4+GDdunUYO3as3u3SmVBEe7QjlhBCCDGYSZMmYdKkScjLy4OFhQWsra0BAE+ePIGjo6NebdKu2CbsyqzX0N7dtto6EilT7fNKaEcsIYQQYnDOzs6wtrbG4cOHMWrUKHh6eurdFgV2TZi9JR8HP+tTbZ2S8grtG6SpWEIIIcSgHjx4gNmzZ8PHxwcjR44El8vFL7/8ond7FNiZgMkv+2p8rrRch9NCaCqWEEIIqbXy8nLs3LkToaGh8PPzQ1JSEjIzM3HmzBns3LkTI0eO1LttCuxMwFdhfpgxqL3a54q1DewYhqZiCSGEkFr65JNP4OHhgVWrVmHYsGHIzMzE33//DQ6HAx6v+owW2tB588SwYcPU7rjkcDgQCoVo3bo13n33XbRr167WnSOG07WF+rxzxSItp2KfPgVKSmT3vbwM1CtCCCHEtKxbtw5Tp07FtGnTYGNjY/D2dR6xs7Ozw7Fjx5CUlAQOhwMOh4PLly/j2LFjqKiowK5duxAQEICzZ89q3ebatWvh4+MDoVCIkJAQJCQkaHXdzp07weFwMHToUF3fhsnp2twBr/m7qpQ/Li7XrgH5aJ2LC2BhYcCeEUIIIaZj27ZtSEhIgLu7OyIiIrB//35IJDosi6qBzoGdm5sb3n33XaSmpmLPnj3Ys2cP7t27hzFjxsDX1xc3b97EuHHjMHXqVK3a27VrF6KjozF79mwkJSUhICAAYWFhyM3Nrfa6tLQ0fPnll+jTp/rNAaTSkrc6qZRlPS3V7mKahiWEEEJq7Z133sGRI0dw7do1+Pn5YfLkyXBzc4NUKkVycnKt29c5sNu8eTM+//xzcLmVl3K5XHzyySfYuHEjOBwOoqKicP36da3aW7FiBSZOnIjIyEj4+/tj/fr1sLS0xJYtWzReI5FIMHr0aMydO7dWx26YmmbWApWyS2lPwDBapDyhHbGEEEKIwbRs2RJz585FWloatm/fjuHDh2PMmDHw8vLCp59+qne7Ogd2FRUVuHXrlkr5rVu32KFEoVCo1ckH5eXlSExMRGhoaGWHuFyEhobi/PnzGq+bN28eXFxc8P777+vafZPnZitUehx7OQvLDt2u+ULaEUsIIYQYHIfDQVhYGH7//Xc8fPgQX375JU6ePKl3ezoHdu+99x7ef/99fPfddzhz5gzOnDmD7777Du+//z57BMbJkyfRoUOHGtvKz8+HRCKBq6vy2i9XV1dkZ2ervebMmTPYvHkzNm3apFV/RSIRCgoK2FthYaFW1zVVm8cHq5StO3EPhWXi6i+kqVhCCCGkTjk6OuLzzz/H1atX9W5D512x3333HVxdXbFs2TLk5OQAkAViX3zxBbuu7vXXX8eAAQP07pQmhYWFeO+997Bp0yY4OTlpdc3ixYsxd+5cg/elsergYae2vNOcw9g3uTcCve3VX0hTsYQQQkiDp3Ngx+PxMH36dEyfPh0FBQUAAFtb5WOrmmv55e/k5AQej8cGiHI5OTlwc3NTqX/v3j2kpaVh8ODBbJlUKgUAmJmZ4fbt2/D1VU7GGxMTg+joaPZxVlYW/P39teqfqfnuSAq2Tuiu/kmaiiWEEEIavFolKLa1tVUJ6nTB5/MRFBSE+Ph4tkwqlSI+Ph49e/ZUqe/n54dr167hypUr7O3NN9/Eyy+/jCtXrsBbzTShQCBg+2lra1snOWMamxFB6vPQnUzJU7+RoqICyMqS3aepWEIIIaTB0jmwy8nJwXvvvQcPDw+YmZmBx+Mp3XQVHR2NTZs2YevWrbh58yYmTZqE4uJiREZGAgDGjh2LmJgYALJNGR07dlS62dvbw8bGBh07dgSfz9f59U3RwmEd8cdHqoEzALSM+Qc+0w7gxG2FdDOPHgFSKWBuDqgZSSWEEEIaOl1y5orFYsybNw++vr4QCoUICAhAXFycxvpLliwBh8PB559/Xgc9143OU7Hjx49Heno6Zs6cCXd3d612v1YnIiICeXl5mDVrFrKzsxEYGIi4uDh2Q0V6erpSahVSewIzHrr5OGLL+GBM3XMNeYUilTrjf7qItCWDZA/k07BeXgD9LAghhDQy8py569evR0hICFauXImwsDDcvn0bLi4uKvVnzJiB7du3Y9OmTfDz88OhQ4cwbNgwnDt3Dl26dFGqe/HiRWzYsAGdO3fWqU///fef2nL5SV7NmzeHQKCapqwmHEarJGaVbGxscPr0aQQGBur8Yg1BZmYmvL29kZGRAS86Ggvn7uXj3U0X1D53f3G4LHD/7Tfg3XeBvn2BWmzBJoQQQgxB1+/ykJAQdOvWDWvWrAEgW/bl7e2NTz75BNOmTVOp7+HhgenTp2Py5Mls2fDhw2FhYYHt27ezZUVFRejatSt++OEHLFiwAIGBgVi5cqVW74HL5VY7OGZubo6IiAhs2LABQqFQYz2VdrWu+YK3t7d2CW1Jo2AjMNf43LOSFylQaEcsIYSQRkqfnLkikUglmLKwsMCZM2eUyiZPnoxBgwYpta2tvXv3ok2bNti4cSO7b2Djxo1o164dfv31V2zevBnHjh3DjBkzdGpX56nYlStXYtq0adiwYQN8fHx0vZw0MD5Olhqfyy4og4MVH6LUNAgAMN7eqN3EOyGEEGI4hYWFbIYOQLZhsur0ZXU5c9UduAAAYWFhWLFiBfr27QtfX1/Ex8cjNjZW6UzXnTt3IikpCRcvXtSr7wsXLsSqVasQFhbGlnXq1AleXl6YOXMmEhISYGVlhSlTpuDbb7/Vul2dR+wiIiJw4sQJ+Pr6wsbGBo6Ojko30rjYCM1x6quXEaAmf11BqRhlYgkST8sSJSZJreu5d4QQQohm/v7+sLOzY2+LFy82SLurVq1CmzZt4OfnBz6fj6ioKERGRrJr/jMyMvDZZ59hx44dOk2TKrp27RpatGihUt6iRQtcu3YNABAYGIhHjx7p1K5eI3akaWnezBJutgJUzXMdsfFfAMCBfNkpIMdLLRBUz30jhBBCNElOToanpyf7WN1mA11z5gKAs7Mz9u3bh7KyMjx+/BgeHh6YNm0aez59YmIicnNz0bVrV/YaiUSCU6dOYc2aNRCJRDVmCvHz88OSJUuwceNGNquHWCzGkiVL4OfnB0CWe7fqSGNNdA7sxo0bp+slpBGobtmkR0EeAOC5s3s99YYQQgipmY2NTY35dBVz5g4dOhRAZc7cqKioaq8VCoXw9PSEWCzGnj17MGrUKADAq6++yo6qyUVGRsLPzw9Tp07VKv3b2rVr8eabb8LLy4vdUXvt2jVIJBLs378fAJCamoqPP/64xrYUaRXYFRQUsB+c4ly2OrVJWEyMx1pY+avwmr8rjiTL/rKxKC+DQ5nsfN0CJ8phRwghpPGJjo7GuHHjEBwcjO7du2PlypUqOXM9PT3ZqdwLFy4gKysLgYGByMrKwpw5cyCVSvH1118DAJs/V5GVlRWaNWumUq5Jr169cP/+fezYsQMpKSkAgJEjR+Ldd99lD1N47733dH6vWgV2Dg4OePToEVxcXGBvb692ey7DMOBwOEoLC0nj8VVYO1zPeo73evpg+/kHbLlHoWy0roBviQobCtoJIYQ0PrrmzC0rK8OMGTOQmpoKa2trhIeHY9u2bbC3tzdov2xsbPDRRx8ZtE2tArtjx46xGyOOHz9u0A6QhsHdzgKHv+gHAFgdf4ctl0/DPrR1hjmP9sQSQghpnKKiojROvZ44cULpcb9+/ZCcnKxT+1Xb0MadO3dw/Phx5ObmQiqVKj03a9YsndsDtAzs+vXrp/Y+aZoWDuuEib9cAlAZ2D2ydYIZj06dIIQQQgxh06ZNmDRpEpycnODm5qY0G8rhcOo2sKvq2bNnSEhIUBthjh07Vq+OkIbjNX9XrB/TFR9tT4JHQT4AGrEjhBBCDGnBggVYuHAhpk6datB2dQ7s/v77b4wePRpFRUWwtbVViTApsGsa5D9X+Yhdlq0LuAo/64wnJfj28G1M7NMKHT3tjNJHQgghpLF6+vQpRo4cafB2dZ5bmzJlCiZMmICioiI8e/YMT58+ZW9PnjwxeAeJcXkU5gIAHtk4QVRROTob+fNF/HnlIT7clmisrhFCCCGN1siRI3H48GGDt6vziF1WVhY+/fRTWFpqPoqKNB3uClOxrhVS/H31IVLzinE3twgAkPWs1JjdI4QQQhql1q1bY+bMmfj333/RqVMnmJsrn93+6aef6tWuzoFdWFgYLl26xGZfJk0Yw8CTnYp1hkOFFJ/8dlmpioOl7BdRLJHih+P30KetE7o2d6j3rhJCCCGNycaNG2FtbY2TJ0/i5MmTSs9xOJz6C+wGDRqEr776CsnJyWojzDfffFOvjpCGpY2LNZqVPIdAIgbD4SDHphkyb2Sr1LO1kP38t//7AN8dTcF3R1OQtmRQfXeXEEIIaVTu379fJ+3qHNhNnDgRADBv3jyV5yhBcdPRytkaW151A9YAFS6uEPPM1daz4st+hW4+qv5EEkIIIYTUPZ0Du6rpTUjTFcDIgjVznxbo3boZzt59rFKHx5XtlJXQrwUhhBBSrejoaMyfPx9WVlaIjo6utu6KFSv0eg298tgRE5GeLvuvtzdshepH7MrEshFaKcPUV68IIYSQRuny5csQi8XsfU3UHd2qLa0Cu++//x4ffPABhEIhvv/++2rr6rvYjzRAGRmy/zZvrpTDTtGj52VY9M9N7L2cVY8dI4QQQhofxWNZ6+qIVq0Cu++++w6jR4+GUCjEd999p7FebXZxkAZIPmLXvDnc7YRqqxSJKrDxVGo9dooQQgghmmgV2Cnu3KirXRykAVKYio16pTVScovQxdseq+LvGLdfhBBCSCNXVlaG1atX4/jx42qPaE1KStKrXVpjRzRTmIq1t+TjlwndAQB8My4u3H+CUyl5ai+bvvcaFg7rpFR27l4+npeIMbCTe512mRBCCGkM3n//fRw+fBgjRoxA9+7da7WuThGHYXRf9Z6ZmYm//voL6enpKC8vV3pO310c9SUzMxPe3t7IyMiAl5eXsbvTcJWXA0IhwDBATg7g4qJSxWfaAY2XK+ayYxgGLWP+AQCcm/YKPOwtDN9fQgghJqMpfJfb2dnhn3/+Qe/evQ3ars4jdvHx8XjzzTfRqlUr3Lp1Cx07dkRaWhoYhkHXrl0N2jliRFlZsqBOIACcndVWCW3viqM3czQ2IZUyOHozB4He9mzZsxIxBXaEEEJMnqenJ2xsbAzeLlfXC2JiYvDll1/i2rVrEAqF2LNnDzIyMtCvXz+MHDnS4B0kRiKfhvX2BjQMD387sjP2f/ISLvzvVQS1UD1GbE9SJj7Yloi31p1jy+R57xQxDIPcwjLD9JsQQghpBJYvX46pU6fiwYMHBm1X58Du5s2bGDt2LADAzMwMpaWlsLa2xrx587B06VKDdo4YkcKOWE3sLfno6GkHV1sh9kzqpfScVMrg1wRZG5lPS9ny8grVTMYLDtxE94Xx+OfaIwN0nBBCCGn4goODUVZWhlatWsHGxgaOjo5KN33pPBVrZWXFrqtzd3fHvXv30KFDBwBAfn6+3h0hDYzCjlhtDeviyeazS80vQs5z1VE4UYXqkXObz8h2Wi/Yn4xw2lxBCCHEBLzzzjvIysrCokWL4OrqarDNEzoHdj169MCZM2fQvn17hIeHY8qUKbh27RpiY2PRo0cPg3SKNAAKO2K1tWJUAP66+hASKYPQFafU1hFVSBH1axKyn5dh5hv+CFBYf1cipnOGCSGEmIZz587h/PnzCAgIMGi7Ok/FrlixAiEhIQCAuXPn4tVXX8WuXbvg4+ODzZs3G7RzxIi0mIqtisPhwJLPq7ZOmViC61nPcenBU8z88zpuZxeyz5WWKwd2D5+VYtSG8zRFSwghpMnx8/NDaWlpzRV1pNOInUQiQWZmJjp37gxANi27fv16g3eKNAB6TMUCQDMrPgrLKjQ+//7WS+z9/zKfY/SPF9jHoirr7747koKE+0+QcP+JUvoUQgghpLFbsmQJpkyZgoULF6JTp04wN1c+k93W1lavdnUK7Hg8Hl5//XXcvHkT9vb2er0gaST0mIoFgObNrJD2uETr+vlFIo3PVUh1TrFICCGENAoDBgwAALz66qtK5QzDgMPhQCLRb3mSzmvsOnbsiNTUVLRs2VKvFySNwPPnshug84idp736M2W1tTTuFrp42+P1Dm5wsRWw5WKJFOY8nVcOEEIIIQ3S8ePH66RdnQO7BQsW4Msvv8T8+fMRFBQEKysrpef1HTokDYh8tM7BAbC21unSZlaCmispsDDngQGDMrFsGnbdiXsAZCdXWPMrfz1LyiWws6DAjhBCSNPQr1+/OmlX68Bu3rx5mDJlCsLDwwEAb775ptLW3NoOHZIGRM9pWAAY06MFtv37AM9LxVrVd7YRoEIixUM1qVGWH0lh71dIKtffPS4SYeOpVIwM9kZrF90CT0IIIaShKCsrw3///Yfc3FxIpcrrzN9880292tQ6sJs7dy4++uijOhs6JA2IHjti5dzshDg37RVUSBncySlESbkEY7ckaKwvkTKQqDmuOL3KOj2xpLLO1D3XcPRmDn45/wA35w/QuY+EEEKIscXFxWHs2LFqcwDXyxo75sWXb10NHZIGRM8dsXJWAtmvVbCPI65nPWfLWzSzxIMqAVuRSP0O2oIy5RE/scKIXVL6UwBAKeW9I4QQ0kh98sknGDlyJGbNmgVXV1eDtavTGjtDZUUmDVwtpmKrsrOo3L5tI1T9ddM0ZVtQqjmwY9SM8BFCCCGNSU5ODqKjow0a1AE6BnZt27atMbh78uRJrTpEGoBaTMVWZSusDOx4XO03P8TfylV6rJj6hMI6Qgghjd2IESNw4sQJ+Pr6GrRdnQK7uXPnws7OzqAdIA1QLadiFdkIzeDrbIXScgn6tXXG1YxnAIDWLtYoE0uQ+VR91m35+bFy5RWKI3a17hYhhBBiVGvWrMHIkSNx+vRptQmKP/30U73a1Smwe/vtt+Hi4qLXC5FGQioFMjNl9w0wYsflcnDws74QS6R4+KwU38ffAQBsGhuM93++qHU7YokU+UUiOFkLaCqWEEJIo/fbb7/h8OHDEAqFOHHihNKMKIfDqfvAjtbXmYicHEAsBrhcwMPDIE3yzbjgm3HRxtUGx6b0g7ONADZCczSz5iM1v1irNlYcScHpO/lYNrwzjdgRQghp9KZPn465c+di2rRp4OqwVKkmWrdEoyQmQj4N6+EBmOmcv7pGrZytYfNi3d2CoZ20vu70Hdl28K/3/Edr7AghhDR65eXliIiIMGhQB+gQ2EmlUpqGNQUG3BFbk3ZuNnirqycAYGVEoNbXaUqRQgghhGiydu1a+Pj4QCgUIiQkBAkJmnOsisVizJs3D76+vhAKhQgICEBcXJxSnXXr1qFz586wtbWFra0tevbsiYMHD2rdn3HjxmHXrl16vx9NDD8kQxo3A+6I1cY3IwLwRWhbeDta4vNdV+rlNQkhhJiWXbt2ITo6GuvXr0dISAhWrlyJsLAw3L59W+2g1YwZM7B9+3Zs2rQJfn5+OHToEIYNG4Zz586hS5cuAAAvLy8sWbIEbdq0AcMw2Lp1K4YMGYLLly+jQ4cONfZJIpFg2bJlOHToEDp37qyyeWLFihV6vVc6fJMoM+COWG3wuBx4O1oCAFo6WdVQW1XC/SeI+jUJuQWqR5IRQgghgCxImjhxIiIjI+Hv74/169fD0tISW7ZsUVt/27Zt+N///ofw8HC0atUKkyZNQnh4OJYvX87WGTx4MMLDw9GmTRu0bdsWCxcuhLW1Nf7991+t+nTt2jV06dIFXC4X169fx+XLl9nblStX9H6vNGJHlNXjVGxV297vjt8S0tHaxRpf7Lqq1TWjNpwHIEt07OtsjauZz7D23a5wsOTDgs+ry+4SQghpBMrLy5GYmIiYmBi2jMvlIjQ0FOfPn1d7jUgkglAoVCqzsLDAmTNn1NaXSCT4448/UFxcjJ49e2rVr7o6opUCO6KsnqdiFXk5WOKrMD/2yDAA4PO4KJdIq7lK5vSdfHaDRa8lxwAAt+YPgNCchwupj5FXJMIbnQ2zy5cQQkjDUFhYiIKCAvaxQCCAQCBQqpOfnw+JRKJywoOrqytu3bqltt2wsDCsWLECffv2ha+vL+Lj4xEbG6tyfuu1a9fQs2dPlJWVwdraGnv37oW/v7+B3p1+GsRUrC4LGmNjYxEcHAx7e3tYWVkhMDAQ27Ztq8feNnHyEbt6mopVR6pwyoTATP9f0UnbEwEAERv/RdSvl3Evr6jWfSOEENJw+Pv7w87Ojr0tXrzYIO2uWrUKbdq0gZ+fH/h8PqKiohAZGamyg7Vdu3a4cuUKLly4gEmTJmHcuHFITk7W+nUuXbqEr7/+Gm+//TbeeustpZu+jB7YyRc0zp49G0lJSQgICEBYWBhyc3PV1nd0dMT06dNx/vx5/Pfff4iMjERkZCQOHTpUzz1vgsrKZHnsAKOM2Mm52FQOfwvM9f8VPX47T+nxo2e0Do8QQpqS5ORkPH/+nL0pTrfKOTk5gcfjIUf+/fZCTk4O3Nzc1Lbr7OyMffv2obi4GA8ePMCtW7dgbW2NVq1aKdXj8/lo3bo1goKCsHjxYgQEBGDVqlVa9X3nzp3o1asXbt68ib1790IsFuPGjRs4duxYrU75Mnpgp+uCxv79+2PYsGFo3749fH198dlnn6Fz584a572JDuQnTlhaAo6ORutG82aWWPtuV+z8oAd6t3aqVVuK+RcpxzYhhDQtNjY2bLoRW1tblWlYQBZ8BQUFIT4+ni2TSqWIj4+vcT2cUCiEp6cnKioqsGfPHgwZMqTa+lKpFCKRSKu+L1q0CN999x3+/vtv8Pl8rFq1Crdu3cKoUaPQvBaDK0YN7OQLGkNDQ9mymhY0KmIYBvHx8bh9+zb69u1bl101DYrTsEaOggZ1dkePVs0wZ3DNW8aro7g+T/6WKiRSZD+n0TtCCDEV0dHR2LRpE7Zu3YqbN29i0qRJKC4uRmRkJABg7NixSqN9Fy5cQGxsLFJTU3H69GkMGDAAUqkUX3/9NVsnJiYGp06dQlpaGq5du4aYmBicOHECo0eP1qpP9+7dw6BBgwDIgs/i4mJwOBx88cUX2Lhxo97v1aibJ/RZ0AgAz58/h6enJ0QiEXg8Hn744Qe89tprauuKRCKl6LmwsNAwnW+KjLhxQhMHKz7c7YR4pGcgVl6hENhBFtlN3XMNe5Iy8fuHPdG9pfFGJgkhhNSPiIgI5OXlYdasWcjOzkZgYCDi4uLY+CM9PV1p/VxZWRlmzJiB1NRUWFtbIzw8HNu2bYO9vT1bJzc3F2PHjsWjR49gZ2eHzp0749ChQxrjkaocHBzYmMTT0xPXr19Hp06d8OzZM5SUlOj9XhvlrlgbGxtcuXIFRUVFiI+PR3R0NFq1aoX+/fur1F28eDHmzp1b/51sjOo5h522tk7ojg9+uYS0x7r/oqc/Ub1mT5JsynnJwZuI/bh3rftHCCGk4YuKikJUVJTa506cOKH0uF+/fjVugti8eXOt+tO3b18cOXIEnTp1wsiRI/HZZ5/h2LFjOHLkCF599VW92zVqYKfPgkZANl3bunVrAEBgYCBu3ryJxYsXqw3sYmJiEB0dzT7Oysoy+lbkBsuIOeyq09bVBse/7I9fE9Ixfe91na6d93flP0xplfOO7+cXG6R/hBBCiK7WrFmDsjLZbNT06dNhbm6Oc+fOYfjw4ZgxY4be7Ro1sFNc0Dh06FAAlQsaNUXV6lS3WLFqThvFfDekigY4FSvH4XAwOqSFzoHdhftP2PviKvnwnpaIkf64BM2bWRqkj4QQQoi2HBU2KXK5XEybNs0g7Rp9KjY6Ohrjxo1DcHAwunfvjpUrV6osaPT09GRz0yxevBjBwcHw9fWFSCTCP//8g23btmHdunXGfBtNQwOdijUUsYRRKVtwIBkbxwYboTeEEEJMkbYDTLa2tnq1b/TATtcFjcXFxfj444+RmZkJCwsL+Pn5Yfv27YiIiDDWW2gaGKbBTsUq2v5+CBIfPMWr7V3A43IwcNVpra+tUHOCRdVRPEIIIaQu2dvbg1NN5gmGYcDhcFROudAWh2EY1WGMJiwzMxPe3t7IyMiAl5eXsbvTcDx9Wpm7rqQEsLAwbn+0tOPCA62nZ1/1c8Hm8d3gM+0AWxba3hU/jqMRO0IIaUwa83f5yZMn2fsMwyA8PBw//vgjPD09ler169dPr/aNPmJHGgj5NKyTU6MJ6gAgItibDezaulojJUfzsWHxt3JxLfO5UpkZl4N7eUX46ex9fNy/NdxshRj94wU42wjw/Ttd6rTvhBBCTE/VgI3H46FHjx4qp1roy+gnT5AGohFMw6pjxqv8FV4wtBNCashL99a6s0qPeVwOIjb8i+3/pqPXkmNIflSA86mP8dfVhzCxwWxCCCFNAAV2RKYB74ityYxB7TGxT0t083HA6ne7wN7SXGPdqhso8opEyC+q3FF9NfMZe/9xcTkO3chWSnJMCCGENGQU2BGZRrwj9v/6tML0Qf7gcDhwsRHi9w+rP/tPUYJCOhQAOJ2Sz96P2HAeH25LxA8n7hqsr4QQQkhV1W2m0BWtsSMyjXQqVp0WzSxhLTBDkahC52vjbmSz9+/lyRIY/3XlIT4PbWuw/hFCCDFdb731ltLjsrIyfPTRR7CyslIqj42N1at9CuyITCOeiq1KYMbDpRmh8JsZZ5D2DPiHFCGEEBNnZ2en9HjMmDEGbZ8COyLTiKdi1RGa8wzW1r28YkT9moRVb3cBj6s5yst4UoLkRwV43d/VoMPqhBBCmo6ffvqpTtunNXYEkEiArCzZ/SYwYldVRLA3jn/ZH71bN9O7jf3/PcLJlNxq6/RZdhwfbkvEIYXpXEIIIaQ+UWBHgEePZMGdmRng5mbs3hjM7o96IiLYG9MG+qGlkxV2/F8PuNgIar5Qg9Jy1d2xa4/fxWc7L0MqrdxtezIlT+/XIIQQQmqDAjtSOQ3r6QnwDDeFaWzBPo5YOqIzHKz4bJm0Smq6sA6uWrfHU/Ov5ZtDt/HnlYf49/5jtqxMLEV+kQiDV5/BtvNpunabEEII0RsFdqRJ7YitSdWkwxxwcOLL/lpeC0zd/R/WHldNf/Lupgvs/TKxBMsPp+Ba1nPM/PMGRBX6nfdHCCGE6Io2T5AmtSO2JtIqgZ1YIoWPkxVGBXvh90uZ1V772a4rbLLibw7dhhVf/ehmmViCp8Xl7ONlcbcx8w1/ALKzbbeeS8NPkd3had94jm4jhBDSONCIHWlyO2Kr816PFkqPyyWyQG1Mjxaw4vOq3WBR9QSK4nL1I3GiCqnSKN2uixns/el7ryMlpwiL/7mpc98JIYSQmtCIHTGpqdhPXm2Dri0c0MnTDlwOBwJz2d82nb3scWPeAACAz7QDtXqNMrFywCepurAPwInbeTh2Kwev+Gm/xo8QQgipCY3YEZOaijXncdG/nQuaWQvgYMWHJd/wf9vIRuwqR/fUBXZFogpM+PkScgvKDP76hBBCTBcFdsSkpmLrQ5lYojRtK2FUAzu53EJRfXSJEEKIiaDAztSVlACPX6TqMIERO12Y81RPj2juaFnjdWVi5TV2EimjshuXEEIIqQsU2Jk6+fo6Gxugyvl1pm5IoCcCvJQ/EytBzVO3hWVipalYAJjyx1W1dcUvNm9sPHUPg74/jWcl5WrrAbJULc9LxTW+PiGEENNFgZ2pU5yGpfNNAQA/jO6K/u2cMWNQe+z6sKfSc07WfA1XVSooq8CDxyVKZbFJWVgad0ulbqlYgiJRBRb9cws3HhZg+r7rGtuN/v0qAuYexpWMZ9q9EUIIISaHAjtTZ0I7YrUV3skdP0d2h70lH0JzHqJebg0A+OzVNpg20E/vdteduKdSViaWICb2Gvv4wH+PcO5evlKd61nP8VtCOvZelp3nu/GUajuEEEIIQOlOiAntiNVX9GttMSTQA77O1uByOeju44iEtCcGabukXIK/rz5UKvvjUiZ6+Tqxj99YfUbpeS6NrBJCCNGAAjtTRztia8TlctDG1YZ97GRT83SstkrVJDmuKW7jcSmwI4QQoh5NxZo6morVWaC3vcHaKhJVqJRVTXBcFY9G7AghhGhAI3amjqZidTaulw8s+GY4eO0Rzt17XKu25v6drFL2z7VsSKUMuBpG5mjEjhBCiCY0YmfKGIamYvUgMOPhvR4tENzCoc5eQ1QhhVTKYNXROyrPSRgGm8/cx+3sQpXnHj4rVXvSBSGEENNAgZ0pe/wYKHtxpJWXl3H70gh90M+3ztoeuOoUjt7MwXdHU1Sei03Kwvz9yQhbeUqp/MTtXPRacgzjtiTg2K0cNkceIYQQ00GBnSmTj9a5uQECgXH70ghZC8zw7cgA8HmG/2eU9rgEH2xL1Kru9aznePisFJvP3AcAnLmbjwk/X8JKNUEhIYSQpo0CO1NG07C1NiLIC9fnhhnt9dPyi/HG6jPoteQYrPjKS2Z3JmTU+etvPZeGQzey6/x1CCGEaIc2T5gy2hFrEHwz4/19dFEhn56lgKf0nKSOz6e9lV2A2X/dAACkLRlUp69FCCFEOzRiZ8poR6zBWJgrB1X2lub4YXRXzBnsz5b1aeNU9bJaK1VIjVJ1xE5ax5so8gpFddo+IYQY0tq1a+Hj4wOhUIiQkBAkJCRorCsWizFv3jz4+vpCKBQiICAAcXFxSnUWL16Mbt26wcbGBi4uLhg6dChu375d12+jRhTYmTKaijWYrRO6w8NOiNXvdMG8IR3wd9RLCO/kjvG9W2JlRCDe69EC0we1N/jrKubBS7ivfBoGbY4lhBCZXbt2ITo6GrNnz0ZSUhICAgIQFhaG3NxctfVnzJiBDRs2YPXq1UhOTsZHH32EYcOG4fLly2ydkydPYvLkyfj3339x5MgRiMVivP766yguLq6vt6UWh2HqeL6mgcnMzIS3tzcyMjLgZeo7QXv1As6fB3bvBoYPN3ZvTMLCA8nYdPq+wdqLerk11hy/q/Y5Sz4PyfMGGOy1qjp9Jw/vbZb9xUtTsYSQ+qTrd3lISAi6deuGNWvWAACkUim8vb3xySefYNq0aSr1PTw8MH36dEyePJktGz58OCwsLLB9+3a1r5GXlwcXFxecPHkSffv21fOd1R6N2Jkymoqtd9MH+ddcSQel1ZxSoU0+u9vZhdiTmInq/r57XCRCYZm42nbO3Mmv8bUIIcQYysvLkZiYiNDQULaMy+UiNDQU58+fV3uNSCSCUChUKrOwsMCZM2fU1geA58+fAwAcHR0N0Gv9UWBnqsRi4OGLw+dpKrZe7ZnUE29388blma/Vui1pNQGZNmPxYStPYcofV3E4OUft84VlYgQtOIpOcw5X2/6YzRdUnl97/C4GrDyFZyXlNXeEEEL0UFhYiIKCAvYmEqmu/c3Pz4dEIoGrq6tSuaurK7Kz1e/qDwsLw4oVK3Dnzh1IpVIcOXIEsbGxePTokdr6UqkUn3/+OXr37o2OHTvW/o3VAgV2purhQ9k3M58PuLgYuzcmJaiFI5YM7wwHK36t26qQaI7edNkVe/NRgdry1LzKtSIMw4BhGBy7lYPMpyWo2nrVzRrfHLqNW9mFWHfyntb9qMrEVooQQnTk7+8POzs79rZ48WKDtLtq1Sq0adMGfn5+4PP5iIqKQmRkJLhc9WHT5MmTcf36dezcudMgr18blO7EVClunNDwi0oahlHBXuja3AEl5RLM2698tmyxwuaJqqobzVtx+DZ+u1iZ585cQ5JlxXNpyyVSnH2R/BgAhObK1+y9nIXhQaprXZ4W6zdit+roHfya8AD7JveGu52FXm0QQpq25ORkeHp6so8FapLtOzk5gcfjISdHeWYiJycHbm5uatt1dnbGvn37UFZWhsePH8PDwwPTpk1Dq1atVOpGRUVh//79OHXqVINYu0/f6KaKdsQ2CB/0bYVAb3sMDfTADA27Zod18cLb3ZvDqkqeOgCIvZylse3qBrueloiV0pXweVyUV0hxJ6dQaZSMy1EI7Cqk+De1cudtmVj5yDJNiYqLyzWvA6zOd0dTkFMgUnteLiGEAICNjQ1sbW3Zm7rAjs/nIygoCPHx8WyZVCpFfHw8evbsWW37QqEQnp6eqKiowJ49ezBkyBD2OYZhEBUVhb179+LYsWNo2bKl4d5YLdCInami5MQNwv/ClYO5BQduAgC6+TjgYtpTAADzYtJTaK4a2NXkbm4R7uQUYkBHN3A4HDwtLgcDoKRKsGXG4+DjHYk4ejMXKyMCMbSL7C9gxcHcHovi0bu17rn4qhtV1EZ1I4+EEKKN6OhojBs3DsHBwejevTtWrlyJ4uJiREZGAgDGjh0LT09Pdir3woULyMrKQmBgILKysjBnzhxIpVJ8/fXXbJuTJ0/Gr7/+ij///BM2Njbsej07OztYWBhvloECO1NFO2IbpHWju+KX8w+w8u1AhCyS/XVZXiEbGROY6R7Yha44CQD4KbIbOnnaof83J2DB5yHAy06pnjmPi6M3ZfmcNp1ORWsXa/yakI6erZqxdYrLJRo3WQBQWXMnVyJSP2K37d8H8LQX4hU/V7XPs+1SXEcIqaWIiAjk5eVh1qxZyM7ORmBgIOLi4tgNFenp6Urr58rKyjBjxgykpqbC2toa4eHh2LZtG+zt7dk669atAwD0799f6bV++uknjB8/vq7fkkYU2JkqmoptkAZ2csfATu5KZdYC2T/TqmvadBH500X2fpGoAqn5ygk0FdfSSaQM3lgt29L/64V0vV9T8fWqSn5YgJn7rgOgHHiEkPoRFRWFqKgotc+dOHFC6XG/fv2QnJystq5cQ93cRYGdqaKp2Abv25EBuJdXhKAWDgBUjy2rDcXdrgBQIalcL6fv1Oe5u/k4kpyD1/yVR+AqpFKVurmFZVq32zD/10kIIQ0TbZ4wVTQV2+CNCPLC1AF+4LzYwKDPGjttiRXSphRrmDqtSXG5BBN/uaQStNX2aLPdiZl49Ly0do0QQoiJoMDOFBUWAs+eye7TVGyjYcHXHNhNHeCHBUM7ontLR4wK1n27/YojKez9/CLVBJ+6WLD/plJS4qr57QCwwaq2Fv9zq1Z9IoQQU0FTsaZIPg1rbw/Y2Bi1K0R7QoXNEzPf8EfW01JsOSs7d9bDXoghgZ4Y06MFAMDJWoAfTmifGFhxHZyoQnXqVBd/XX0IUUXlqJ+6qd3YpMzK56UMuNzqA72ajjQjhBAiQ4GdKaJp2EZJcfNESEtHdHzJjg3sCsqUNyiYaUg4XF/O3n3M3pcyQPrjEhxOzoaXgyU+2p6oVLdCyoBfQ2Cn6wgfIYSYKgrsTBHtiG2UBApr7FxsZEk4e/k2w4X7TxDWQXnDQidP5XQm9U0xTpMyDAauOqUxUbFEzVTt4SrJjimsqySWSPHHpUz08m0GHycrY3eHENLAUGBnimhHbKNkZ2GOd7o3B5cDuNgKAQDb3w9BiVjCpkSRC23vggVDO6K9uy2GrztX731VnFplmOpPnxBLpbCA8vrBz3ZeUXpMA3aVfjp7H4terDmkVDGEkKoaxOaJtWvXwsfHB0KhECEhIUhISNBYd9OmTejTpw8cHBzg4OCA0NDQausTNWgqttFa/FYnLBzWiX3M5XJUgjpANnU5pkcLNlVKfeMpRGJZz6rf0Sp5sSP3UtoT9liyquvyjt7MxYPHshQtj56XYs5fN5CaV6RUJyb2PwxcdVppfV9TdEHhWDdCCKnK6IHdrl27EB0djdmzZyMpKQkBAQEICwtDbm6u2vonTpzAO++8g+PHj+P8+fPw9vbG66+/jqwszWdmkipoKtakuNvJRveaWfExIshLr12zuqppM4SiihdTsSPWn8eH2xKR/rhE7WkTMbHXAACTdyTh53NpGLXhX6Xnf0vIwM1HBTh+K0//jjcCNHpJCKmO0QO7FStWYOLEiYiMjIS/vz/Wr18PS0tLbNmyRW39HTt24OOPP0ZgYCD8/Pzw448/sof5Ei3RVKxJWTSsE8b1bIFjX/bHtyMD4GDJr/PX5OkQfeQWliklSD59Nw/lEtWduc9LZTtjk9KfAaguLQulNCaEmC6jBnbl5eVITExEaGgoW8blchEaGorz589r1UZJSQnEYjEcHR3VPi8SiVBQUMDeCgsLDdL3RksqpcDOxLzs54K5QzrCzsJcVqAm5vJ4MapnKDoM2GHQ92dw5m4++3j63usG7QshhJgSowZ2+fn5kEgk7CG8cq6ursjOztZwlbKpU6fCw8NDKThUtHjxYtjZ2bE3f3//Wve7UcvNBcrLZfM5Hh7G7g1pIE59/TLWjwmqDP7q2drjd2usY8hjGRmGwcc7EvHlH1cN12i9oblYQohmRp+KrY0lS5Zg586d2Lt3L4RC9SMOMTExeP78OXur6VDfJk8+WufhAZgb50ucGJeztUClzIzHxYCObnC2UX2uPlTU8twx5cO4aw58sp6V4p9r2didmIkyseE3W5y+k4fJO5LwuJaneKjTlNbY3c0twrAfzuL4bfVrqgkhujNqYOfk5AQej4ecnByl8pycHLi5uVV77bfffoslS5bg8OHD6Ny5s8Z6AoEAtra27M3G1E9aoB2xJm9MjxYYHKB+tFYxQDLTZT61iofPy2qupEDdsWO6UDzrVtfAR916vtp6b3MCDlx7hIX/3NT6GlGFpNafQ2MT9WsSLqc/Q+RPF43dFUKaDKMGdnw+H0FBQUobH+QbIXr27KnxumXLlmH+/PmIi4tDcHBwfXS16aAdsSZPaM7D6ne6aHxOrj5DDEkt51krpJXBGcMAn/52Wen8W0UMw4CrEP2V1/IItepkaxngFpaJETD3MEasrznnYBMasMOT4vKaKxFCdGL0qdjo6Ghs2rQJW7duxc2bNzFp0iQUFxcjMjISADB27FjExMSw9ZcuXYqZM2diy5Yt8PHxQXZ2NrKzs1FUVKTpJYgi2jhBquHlYMHe59fjsWTaDJoVqDkvdt9lWZojxancy+lP8dfVh/g+/o5K/TKxBANWnsbnu66wZZO2JyK3ULcRRm3xtBz1PHfvMcrEUnbHLyGE6MvogV1ERAS+/fZbzJo1C4GBgbhy5Qri4uLYDRXp6el49OgRW3/dunUoLy/HiBEj4O7uzt6+/fZbY72FxoWmYskLAd72AICWCsdSffl6O/C4HIwK9sKW8d3Yo8vqWoUWkV3m01I2gbGcPECrUJiKvf7wucY2jt/Kxe2cQiTcr0zyezHtKf4XWzc7cbUN7LhNaeGcDkxr4pmQ+tEgjhSLiopCVFSU2udOnDih9DgtLa3uO9SU0VQseWHje0H4+VwaRodUBvltXG1wZdZrsBaYgcPhIGF6KH48nYoFB9SvFfvy9bYY1NkDL397olZ9uZOr3Yj73L9uqC1XDAzP3n3M3mcYBmIJg5ScQnTwsEWpho0St3MKanztIlEFLMx5WgdrgPb5/HQZHDXRGNBoysQSfHPoNl5t74Jevk7G7g4hNTL6iB2pZzQVS15wtRVi6gA/eDlYKpXbCM3BUYgexvb0waq3A7F+TFe2rG9bZ3A4wKhu3kojfnVN3aaMjCclEGvYdCCRMoj6NQlvrD6Dn8+lQaRhPV1pefUjhrkFZeg4+xAiNqjm11x5NAWTf02CRE0ftD2BQ/HzrmkDBaceVtklPyzAF7uuIONJSZ2/VkO35ex9bD5zH+9uumDsrhCilQYxYkfqiUgEyPMDUmBHtMQ342JIoCcYhsH290PQwcMWdhbmEFVIYcHn1dxAHeuz7Dj6tXVW+1yFlMHhZNmu+x9P38cHfVuprVdTypO4F1PAlx48ZcsYhgGHw8HKo7K1fG918cSr7ZVzctY0YieqkEBgxlOqJ5ZKIeAa93MdvOYMJFIGt7ILcfCzPnX2OobMTVhX0h9TcEsaFxqxMyWZmbL/CoVAs2bG7QtpdDgcDl5q4wQHKz64XI5SUDewY/XpierayRT158OKJYq7ZRmNO2A1TdHKmXGV/1e56VQqghccxdK4W2yZutFEHk85sPvp7H3EXZetGX7wuBh+M+MQE3tNaY2d4nrBusQwDNYcu6P2s5OPPqbkmPhJPdDt3GNCGgIasTMlitOwtFCHGND373RBWn4xbC3MEbKo4ZzbfFbhqLLHxeVIfqR+LZ3iNGpJeQUePitFa5fKnJdVc/rJ89OtO3GPLXtSpJq6Q3Ek7lZ2Aeb+LUuQnrZkEDacSgXDAL8lpGNwgDtbT1zDRhJ9/+kyDIMiUQVOpeSjVCyBtcAM3x5OYfuj6Zq61fCH7HQ595iQhoACO1NCO2JJHTHncdHGVRYI3Zo/ADsupGP+ft1PednwXhAYBvhoe6JB+vXR9iT2vqhCir0v0qOok/jgKYJaOGDImrO4k1uEnR/0QI9WspFtxQ0TmgKvJ8Wqp0woXve4SuCnuJZOcd2cuI5G7L7a/R9ikzIhf9n3X2pZJ6/T1NCAHWlsaCrWlNCOWFIPhOY8eNpX5sN7uZ369W9VjenRHKHtXcE3M8436cRfLgGo3KG7TyEINFOYUtW0Hk/dxozq0phIFUbDFO/X1Yjd7sTKoA4A8rU47qzhj6fVPQ6N2JFGhgI7U0I7Ykk9CW3vgv7tnDH5ZV9EdNPuD4kFQzuBx+XAvB4TIyuqegrCzosZausVllWoLZcnSVacvlR8K4qzmgzDKCVlVpwKrmmNnT67YtWtLSwtr/mM3MawuaGuaZvepqBMTBstSINAU7GmhKZiST0x43Hxc2R3ALIgZvdHPdG8mSUWHriJ1/xd8cu5B0hIe1JDK/XvXp5yPj2GYZD8qEDppIrXvzul9lr51KpikMbjqg9SJVJGKQBUPFJN17NrGYbBrD9voJ2bDcb0aKG2jrqju2raMEJktA3sghccRXmFFCe/6o8WzeovBRAhVdGInSmhqVhiBBwOB8E+jnCxEWLV213wRmcPrBndBR/0bQV7S3OV+lWnIls5q35JLn6rU5309b0flXOViSUM3lxzVmnkqkikfsROHpwprpFTHLFTnNFLSHuiPBWrOGKncO7t8xIxYpMyUazhNSf+cgmn7uRj278PMGOf5tMz1E3v1pTipT40hhFBbWdi5aOi/6Y+rqEmIXWLAjtTwTA0YkcaDBcbIf4X3h49Wqqm3SmvUP62/+PDnip1wjrUTXqVqilLyiVStYmH1ZFPxSpOe2raUfnupgvYd+Uh+1jxNcQv3n9JeQUC5h1G9O9XMS32WuXFCk0eSc7B4SrHrFXXN0UX056qqWk4NSVa1saplDxM33vNqEGorse9NYZglTRtFNiZiufPgaIX00w0YkcaiDlvdkBoexf8MqE7W+Zsw1eq42il/PjvqJdUyuqKprx36khejNQpTqVqu/BeafPEixG7jadS2bK/rz5UuUauRIu1ctqcxSuVMlptqNBGYZkYLy09hqm7/6tVO2O3JGDHhXSlz0JbEiljkOBS13QndRXXPS8VY/OZ+8gpUM2XSIgiCuxMhXy0rlkzwNKy+rqE1BM3OyF+HNcNfRVOjuja3AGv+LmwjzkcDoJaOLCPO3nZ1Vv/dAnsKqQMVh5Nwe+XKjddPCupXNsWfzNX47WKcZe4Qorjt3LZEy0U7UnMxIH/Hin3UeHi/CIRJu9IwqkqSYe1SaES9VsSghccNchU4r7LWXj4vAy7LqnfgCKnbRCk69FmA1aegu///sHljGc6XaeOoRIUP3xWqtWGFU1iYv/D/P3JGP2j4Y82k0oZjUsMLqY9wY+nU+shpyExFArsTAXtiCWNBIfDweSXfZXKFAO7+tRjsfbJlo/ezMHKo3fwzaHbbNm+Kw/ZacQtZ+9rvLakvPJLtULKIPLni2rrTfnjqkqZWCH4nP3XDRy49ghjtyQo1VFct6fJP9dkU7qb9BgdU2GAFCE3FZJJ1zTwduPhc1xRCOLkGx4KysR6v/7PZ++j/zfHkaMwPX/ubj42nrqnc5CTmleEXkuOod83x/Xuz9Fk2R8Gd3OLaqipuwlbL6Lj7ENIyy9WeW7k+vNYcOAmDt3IMfjrkrpBgZ2poPV1pBHp4u2A0PaubBJddVNqW8YHKz1+q4snNr4XVC/908WNhwU15qabuqdyylLTRglNFNs+f0/9aFtNI3aKgUr8Lc0ji9qqaZBLImUQveuK2t26copJqplqxvYkUgaDvj+DoWvPsoGcrVC2KaegVP/Abs7fyUh7XKI06vjujxew6J9bOFrN6Ku6mO/Yi880t7AWU911mE7vxG3ZCO/v1Yyw3lcT9JGGiQI7U0E7YkkjwuVy8OO4YMx8wx+AcjoQuVf8XJUezx3SAa+2d8WgTu4qdY1p+LpzaDP9YLV1FOPWZyW6BSPHb1dOuyqO3ikGwzVtANFmKdqhG9l4b/MF5GqxxqumXHvHbuUitppTQADgqULQF5ukua5iYCs/1s3WQpbJq0BDzsHaevBYtyBHca3lz9WM3FZHcVnAo+elerVREwMsSSQNAAV2poKmYkkjpmnmq5evbFdt/3bOsBGag8flYO3orvXYM8N7WqJ5FKsmNsLK1KSPFQKjmjZPaLPz98NtiTh9Jx/Tq0mrIlfTTKzi1LOim48K2FHHqhtP5v59A5eq5D786ex9vLr8JPtY/i4MMWKnr+pGFwHZSKCuqq4x/OH4PQ01a6e6vtMBHI0HBXamgqZiSSP2f31aQmjOxZgeyr+/G8cGY8prbTF1gJ9S+bEp/fDtyACc+url+uymQTypRWCneKzZw2eyUR2GYSCuIXDTZg2e3JHkHIz58QJS8zSv9appKlZgpv6rZ+Cq03hn07/IeqY6IvXT2TSMWH9eqWzu38lKdb+Pl204sbV4EdjVYo2dLmrafVubmCjrWanKhom6Or+2uqWDTSGuW7t2LXx8fCAUChESEoKEhASNdcViMebNmwdfX18IhUIEBAQgLi5Oqc6pU6cwePBgeHh4gMPhYN++fXX8DrRDgZ2poKlY0oh5OVjiv9lhWDBUOTGxtcAMn7zaBu3dbZXKWzlbY0SQFywFPLasv5Zn1hqbfDpRH4o7GyM2nsfGU/fQ95vjiFY4OUMd/1mHdHqdM3fzEfXrZY3PK07FqttoUNOxcemPSzQGL1VH7RTtvZyF29mF7Ijdcx2ntfWlbqmAIl1Hu3ZceIB+3xxHWn4xxvx4AelVRuz0Pb/2Xl4RXll+AnsSM9U+b4j0MA3Vrl27EB0djdmzZyMpKQkBAQEICwtDbq769ZIzZszAhg0bsHr1aiQnJ+Ojjz7CsGHDcPly5e99cXExAgICsHbt2vp6G1qhwM4USCRA1os1KjRiRxopvoZRnupYmFcGdr7O1irPN7T1eACQrWENmzY7MRVH7MrEUiz65xYynpQqTcsaiqZ8agVlYqXhHXXTvDXFDwwYjcFLdQGl/PU9HSwAABlPlQOi0nKJVmsEdaX4HtX9mKq+k5o200zfex0PHpdg1l831G5a4HE5KBNLEBN7Dcdva7/ZJSb2GlLzitXurgbqLgdfQ7BixQpMnDgRkZGR8Pf3x/r162FpaYktW7aorb9t2zb873//Q3h4OFq1aoVJkyYhPDwcy5cvZ+sMHDgQCxYswLBhw+rrbWiFAjtTkJ0NVFQAPB7g3vC+yAipK0KFwM7eQvX4MoG59v8LHB3SHDMGtTdIv6qjKWDS9gQMAPC0tzBUdwAA5+7lq5RxuRyIJVK89cNZfPUiUEjNK0Lg3MP4WiExsbpTL9TlB1QcLXpSXK5xx6w2g1UtnWS5Ou/nKQdFfZYdR/dF8ew0taEoBXZa1BdpmR9R0w5pHpeDjadS8VtCOiJ/Up8aR52aTvCodipWw+cedz0b17Oea90HYygvL0diYiJCQ0PZMi6Xi9DQUJw/f17tNSKRCEKhUKnMwsICZ86cqdO+GgIFdqZAPg3r6SkL7ggxETwuB5P6+yIi2BuT+vtiaKAHFgztiOAXefFGBXsjvJN2x5ON6dECvi6qo36GlpKjfu1aiQ7Har3a3qXmSjp4d5NqUlweh4OL958gKf0Z/kjMBMMw+PVCusponDyw+y/zGWbsu4anxeUol6i+F8UAsLpROW2O+HKxkX0hV12vKD9Z4+xd1UC1NhSnYssrpCpTmlVHH7U9Ik1dUAzIPoOsp4bfGVvTxo/JO5IQ/fsV9vG1zOf4aHsi3lhtvGCnsLAQBQUF7E0kUk0pk5+fD4lEAldX5Z30rq6uyM5WfyRfWFgYVqxYgTt37kAqleLIkSOIjY3Fo0eP1NZvSMxqrkIaPdoRS0yY4saKlW93AQCMCPLCw2elaOVsjR6tmoFhZFN/5+7m41mpGE9LynHx/hOl81zNeVydj5cypM5zDmtdV1NAYEjZBWV4V2FRf0pOEX48o5rKY29SJgYHeODNNWcBAKfv5OPBY9WTJLQdkeTWMBzBMIAZj1Ntm9JanqJQ9XLFkcH5+5Ox/7+H2Ptxb43XaxvYSTRsauFxaw7C1KlpbV51H0tugQgHrsmCmnlDOsJaYIY7uYU698HQ/P39lR7Pnj0bc+bMqXW7q1atwsSJE+Hn5wcOhwNfX19ERkZqnLptSCiwMwW0I5YQJUJzHloprLmTf+H1au3Elo0OaYH5Qzui04uASmDG1flAeGP59UK6wdq6lqndNNvigzfVls/884ZSzjp1QR2g/c7cmn4GDMPA7EX0J5YwbNCuSIujc3UyZO1ZpceX058pPa7aZW2nYis0JJbmcTjVBmGa1PTbq7iOs1hUAUt+5QyP4tFq4gopIND99etCcnIyPD092ccCgWrHnJycwOPxkJOjfHpGTk4O3NzUj9g7Oztj3759KCsrw+PHj+Hh4YFp06ahVatWhn0DdYCmYk0B7YglRC82QnOsfqcL/hfuB29HS/aoKlMyeI1202wlIs2jUFUDHXU0BTFVaRNcmyn8nOSDdopBS027WA3paXE5Zv15Q6lM2xE7TceH6Xt+bU2XyT+r1fF30GH2IZxUOHNY8dKaNn/UJxsbG9ja2rI3dYEdn89HUFAQ4uMrjwiUSqWIj49Hz549q21fKBTC09MTFRUV2LNnD4YMGWLw92BoNGJnCmgqlhC9DQ7wYO93bWGPFs0s0dzREjZCM/Z81aqWDu+ECimD6XtrTuZbnetzw9BtwVGU6rC+zlgSqklDog1tp49riuskDMNOxcraleKPS1lY+E/liGJZuYQdySsTS7Dm2F280t4FXZvL1l7eyal+ilGXgdufzqWplIkqpJBIGRxJzkagtwPc7ITYmZCOhLQnWDa8s0LfqxmxU3i8/uQ9SBkGFuY8jOvpozHwq3Eq9kWry4+kAJCdPSynOH2t7YhjQxIdHY1x48YhODgY3bt3x8qVK1FcXIzIyEgAwNixY+Hp6YnFixcDAC5cuICsrCwEBgYiKysLc+bMgVQqxddff822WVRUhLt377KP79+/jytXrsDR0RHNjfh9S4GdKaCpWEIMQmDGQ3x0P/C4HEikDNKfnMX1rMrD6iN7+6CbjyMGdnTD4WTtDk33sBPi4XP1O2GtBWYmM0qo7Ro7DoDdiZkaN0DcelSIfIVcgKl5xZgWe02pzsJ/buJESi52/F8PbDl7H2uO38Wa43eRtmQQcgrK8Np3p6rtg2KQU1O/1a2TKxNLsDsxA1P3XIPQnIvLM19n+/imwh8SmnC5ylOxSw7eYu87WvExJLByarJIVIFnJeXwcrCscSq26luR5wOs+lxjDOwiIiKQl5eHWbNmITs7G4GBgYiLi2M3VKSnp4OrsICzrKwMM2bMQGpqKqytrREeHo5t27bB3t6erXPp0iW8/HJlEvTo6GgAwLhx4/Dzzz/Xy/tShwI7U0BTsYQYjNmL5LpmPA5aO1uzgV3SzNfgaMVn63nYaZdy5FzMq9h7ORNf7FKfW6zcgNNeNkIzFNbR+am1dSRZ/ehnVVwOB19qyMMGAPP2Kx/ZNXDVabX1zt59jDKxRGm6M/lhAd5Yrb6+IsUdqepStwCy9C1cLkfthhtRhRSn78gC0zKxFCkKI4Ty8upwORyNmyeqTt/2XXYcT4rLcfKr/mpHGm88rFxD+euFdES/1pZ9bG+pGNgpjtg1/BFkdaKiohAVFaX2uRMnTig97tevH5KTqz/+rX///lrll6xvtMauqSstBfJf/I+CRuwIMSjFpMmKQR0ANkkuAKx5twvCOrhi9mDlHXzy0bhhXbxwY24Ypoer5snTFDjow7/KCR0Nycwq69A0MeT+lTs5RUrr8d7bfKHG5MkAsPX8A/b+dYXASFF2QRnyCkVqp0VFYgkEZpUbE0rKKwOlzWp2FldV3cEdZlW2DcvzAZ5MyVOZir2e9RyDvldeQxm84Ch7X3E9o2IKF/mIXSPZS2RyKLBr6uTr66ytAYUhZEJI7dmpSXos56Aw2tGjVTNseC8Ykb1boktze7b82JR+7H0rgRkm9lXdcVc1YNTGnknqF4Q3hS/iIgOOOMZezkRsUuWOXX1O6Bi5Xn2C215LjqHbwqNqA0VRhRR7kiqP9SoV6/aeuByOxkzI5mbqf8il5RKVqdgjNSwXUNzkoXjesEhcfYJpYlwU2DV1itOwTeH/6oQ0IJNfbo1OnnaY+Ya/ynMcDgfHpvTDn5N7w8m6cqeeUGGkpkUzqxpfY1Sw7ksoujZ3UDpODQAGdHBTOsPVEMx5HPRo5WjQNmuSU6iagFZfP51Nq/Ocfzlq1k/mVXkPpeW6j8pq6jWfx8V/mc8w68/reKoQqJaUS1R2FD8tqT6QVVxLV6GwJEA+FXs/vzJ1zQfbLmnbdVLHKLBr6mhHLCF1xt6Sj78/eQnvv9RS7fOtnK0R4G2vVDZ/aEd42ltg0bBO1bYtDwYn9ffFqGAvlef7tXVWe93kl33B4XBUdtK2cbVW+dvuw361y8nVydMOK0YF1qoNXelytFpdyniiPh9fVbsuZaiUPS8VKz3WdddzSk6hxrVd5jwuvvrjP/xy/gHe2fQvW14mlqj8/DUd2yanGNgpLgkQVcjWBX4ff4ctO3ozt0GuNzNFFNg1dbQjlpAGpbWLNc5OewXvhqj/Nzl/SAd4OVjgt4khAGTTvfOGdFSpt+itTmjppDriJx8RfNWv8lgxOwtzjO3po/LFHjOwPXu8Wk34ahZ2dW/ZDB5VzqXt2aqZVu01dpqmYLVRXCXnn6YzYTX5/VKmxhE7Mx4HD57ITsK4lV25KaO0SmAnlTIqAWZVipskxBLlNXYnbueqqd/4dss2RRTYNXW0I5aQRuW9nj44M/UVtHG1YcuE5jzsmdRLqZ6FOU/tVJr5iw0d344MwNLhnXB9bhguzQiFs41AbXLf3z7ooVW/FHdIyoVWOZO2s5cdLPimcR51doH6FDXaKBIpB1T6rO37U+G4O0XmPC66t1QNrmVr7Cp//n8kZtSYaDhV4ag0xd3ZxaIKtdP6DXXHtamhwK6po6lYQpoEgcIO3LAOrnCwNMezEs0jLg5WfER0aw5rgRnMq9lGWd1ziqoGdkej+yHYR3l9nRXfDOY8Wstbk4JS5QDoSbHh1g2CUV4PJ/dHYqZSILf/v0c6TWsnPXjK3n+kIe9iYVn1I4CkflBg19TRVCwhTUJrF9nZto5WfGx4LxgcDgc2QlkqUsWMGup2LMrVdPJAdewtlHfnyvujKKiFg8qooE8zS71fs6kqrDJil/jgmcHaFkulGtfOXbhfeTqI0Jyn08YRxVHFh89K1ebRKxJVoKBMjMifEjB5R1KDWQ9paiiwa8oYhqZiCWkihOY8JM8Lw/mYV9iynyO7IaiFA/Z+3JstK6smeezXYe30fn0HK82pXf6K6o2ol1sj6pXWSsGCwIyr9YigIj6Pi7+iemP2YH+M6dFw/yjd/5/66dCaVB2xu/moQENN3U3fe11pbZ0mFuY8vVOU5GrYmVxUVoGE1Cc4fjsPB649wm0t+kEMjwK7puzJE1mCYgDwUt1VRwhpXCz5ZkqJbYNaOGLPpF5KO2+rO2C+o6cdJvX3VSm3FshG/qJebs2WDQ30UEqZ8k53zQFWZy97fBnWDsIqwUJ1IzbLRnTGmakvq33u58hu6Oxlj8jeLeFiI9TYhrFF/XpZr+sawpSlwIyrd6qX0vIKPHymOh1bKKpQ2uGbU4t1iER/dKRYUyYfrXN1BYQN93+OhBDDqWln4qevtMGTonIM6OjGlp34qj9uZxeil28zfBnWDs9LxbCzMIdEyqC8QorsgjK42gqqabWS4uaJCqnyhN2qtwOx5OAtrBsThEBvezx8VqraAIBerZ3Y+4MDPPD7pQyUV0g1jhQZk43ADIWiCnzYrxU2nEzV6pq0x9qlSqlL5mZcvadKL6Y9xcW0pyrlwS0ccOxW5W7Z2mwwIfqjEbumjKZhCTE5bdSsfVNkwedh6YjOeFkhHYqTtQC9Wzuxa/DkJ2rwuBxY8Hlo6WSltHbu+3e6aGxfvu4PAF73d8XcNzsAAD59tQ2GBHrifMyrCHwxwqhNYNHSyQpnpr6CMT1aaKyzbERnNHc0zlq+whepSqz4jWucxEZoZtA1cFZ8HppZC5T+sMhvgIG4KaDArimjHbGEmIy/o17CV2HtMDpEcwBUGwIzLrr5OMDf3RaDOrlrrKcY4HwzMgC9WzvhxtwwpcPl5YTmqqlReGrOVgWUz1OtakRXr2oTPvu52Wh8Tlct1GwGcbUVYHxvH7XB5YKhqjkIG4I7OUW4k1tksPbkfxSIqiQyJvWPArumjEbsCDEZnbzsMPnl1uCb1c3/1jkcDn7/sCf2f/KSxuALAEa8OCUjwNueHfmzEqgfzXK2EWDDe0FKZZ5VEh7LVTcix+Vy4Gyjeaq4uvWBulJ3PnAnTzvYCs1x8qv+SuUz3/CvdqTRmBSnTA1JcY1neQ158kjdoMCuKaNUJ4QQA+JwOOBWE9QBgJ+bLc5NewW7tEx8HNbBDXcXDkSX5vYAgB9Gd1Vbb2SwF74IbYtt73dX+7xLlcBufC8fnI95BXsm9cSwrp5s+Yf9WkFgxsV7egZctkLVwM72RbBXNZ2MVS2TNVdNSl0fNAXWNZG/c03HkJH607gWBRDd0FQsIcQIqh4zVhMzHlcpZYs65jwuPgttA0AWtP18Lg0A8OPYYADKCZSHd/VCRDdvuNtZwN3OQikxb/+2Lvjq9XYQSxjserEpQ65fW2dwOcDx23ka+6Eu8FEX7AGApYaRSm21crKCTzNLpD0uQXNHS6RreT5tbTha8ZGlYVNLtV5EdorHkNGInXFQYNeU0VQsIaQJign3Q792zghp6QjLF2v6OBwO3ujsjmtZz7FgaEel3bmKufSkDAMzHhdmPCDhf68icN4RAMArfi7YMr4bnpWU45fzD2DJ5yHA217lTFhbC9WvzWwNJzFYC3Qbseva3B5J6c/YxzweB0ei+6FULIGt0Bxh353C7Zy6zQ1npufJIYVlFXj9u5NKZ8rSiJ1xUGDXVFVUAA9fJM+kETtCSBMiMOPh5XYuKuVr3u0KqZSpdrpYcTTR3rLyNA35qJ69JR+fviobGWQY1V2jbnaqI3YCc/WrmuRB5y8TumPslgSNfZJztFI+3YPH4cCcV5nkWaqmP4ZmVsNUe3VScpQ3Y1BgZxy0xq6pevgQkEoBc3NZHjtCCDEBmoK62I97YcN7QWjpZKX2eXVBCIfDQeKMUHwRWrmjd2SwF7a/H6JUT/F5RfIdwn3bOmvVd5XArsp7UQzsHCw1nwRSG2Zcw4UFFNgZBwV2TZV8GtbLCzDgP1RCCGmMujZ3QFgHN43PazqFoZm1AG8GegAAfJ2tYCs0x0ttnNCnTWUSZR+FYPG3iZWbRqx0nIqtbrcxIDslUu7El+pP7agtfadi1aE1dsZh9G/8tWvXwsfHB0KhECEhIUhI0DxcfePGDQwfPhw+Pj7gcDhYuXJl/XW0saEdsYQQorWKaoKQlk5WOP31y/gr6iW2TKyhvmLaFU1pXtTp2aqZUq6+MT2aq+T5kyhEdnYKI3afv9hUoo0P+rZCwv9eRf926kcRFadiz0x9GTED/bRuu6pjt3LVTmeTumXUwG7Xrl2Ijo7G7NmzkZSUhICAAISFhSE3V31+nZKSErRq1QpLliyBm5vmv7wIaEcsIYTooFxSfQDi7WipFKh98oosmBoRpHwOt+J0qaXCBg6fF4mNq6Zlae9uiyuzXsO297sj82nlbtQFQ1UTLlddYycf4Xune3O1R74pngIi93F/X7jYCuFmq/6YScWULV4OlujW0lFtPW2N3ZJQbdBMDM+ogd2KFSswceJEREZGwt/fH+vXr4elpSW2bNmitn63bt3wzTff4O2334ZAoN25hSaLdsQSQkiN/N1tAQDDunjodF3v1k64OD0Uy4Z3VipXXFdmqXAKx9YJ3TEyyAu/TuyhFAzyzbiwt+TDjMdVG4gp4lbJkycfXJNIGfg0U107GNKymUqZfCOGpinXqqW1PXbs9J38GqeYiWEZbVdseXk5EhMTERMTw5ZxuVyEhobi/Pnz1VypG5FIBJGo8ry6wsK63SreYNBULCGE1Oi3D3rgSsYz9PZVDYJqou60Cz83G/i52cDVVqgU0LRoZoVvRgYAAL4Z0Rm7EzNlTyiMws0Z3AFzcAMf9fNV+3qr3u6CCT9fxLQX06OyQI+BRMpg+agAfHPoNt5/qSXSHpegvEKKKxlPVdqQB3SaNklUiR1hrWY6+dqc19FpzmG116tvkwK7+mS0wC4/Px8SiQSuVXZsurq64tatWwZ7ncWLF2Pu3LkGa6/RoKlYQgipkZ2FOfppuWtVG2Y8Lv75tI9KgKRIMdBRHA/zcbLCz5HqT9YAgEBveyTOCGWvlweOUoZBC0crrHq7CwCgs5c9AOCOmpx35i8CuqpHzwV42eGrMD828bNce3dbzBnsjyJRBTafuY/o19vBRkNCZnU2VjkyjtS9Jp/HLiYmBtHR0ezjrKws+Pv7G7FH9YSmYgkhxChqOnYNAFo5WyE1rxiDO+s2BawYFG7/P1naFVcN6+WaWfNVyuR9m9C7JX5LSEdhWQW4HODHcd3gbCPA1vNpKteM790SADD55dbs6+/4vxCM/vECW6eDhy1uPCwAINsIsnR4ZzSz5uu0gYQYhtE+cScnJ/B4POTk5CiV5+TkGHRjhEAgUFqPV1BQYLC2G6yiIuDpiyF4GrEjhJAGJ3ZSL1xOf6aUNkVXXZs7VPv8mB4tcCRZ9h17MU15WtbNTojEGa/BnMeBqELK7sCtLiRVDCp7t3bCrfkD8PBZKY7dysXgAA+ELIoHAFRIpWj+YrMIqX9G2zzB5/MRFBSE+Ph4tkwqlSI+Ph49e/Y0VreaBvk0rJ0dYGv7/+3de1BU5f8H8PfCuuzSAqsgu4AgNw1NE5JL2MWcmEAty5wu/ijJ+lkWlmYFFlp9bUwnG8fMstsUM2JiTmpFRRHaxYlAEVQi0dLAiIVKkYsXLvv5/eGPExvo13TZhd33a2Zn5JyHZ5/nPXLOZ56z56xjx0JERD0YPDWYFOUPtXvfnYY9NWpsnjsBy6b3vMMWOHs5VqVS9XisyoXSDnJH+FA9/ve6cKtVw+DBLOocyaFrpAsXLkRaWhpiY2MRHx+P1atXo7W1FbNnzwYAzJo1C0FBQVi+fDmAszdcVFZWKv+ura1FeXk59Ho9IiMjHTaPfoeXYYmI6P+NNHph7f/EnPOSbXd3xgbjy8p6jAr494sCOQ8k4P2Sajw9ZdTFDJNsRCUOfnrg2rVrsXLlSpjNZkRHR2PNmjVISDj7uYEbbrgBoaGhyM7OBgD8+uuvCAsL69HHxIkT8fXXX1/Q+/32228IDg7G0aNHMWzYsP/+CwPR8ePAnj1AZydw002OHg0REQ0gVeZmhAzxhE5zcSt59uAS5/KL5PDCzt74n4GIiGhg47n83Bz+lWJEREREZBss7IiIiIicBAs7IiIiIifBwo6IiIic3muvvYbQ0FBotVokJCSgpKTknG3b29uxdOlSREREQKvVYty4ccjPz7+kPu2FhR0RERE5tU2bNmHhwoV47rnnsGfPHowbNw7JycloaGjotf3ixYvx5ptv4tVXX0VlZSXmzp2L6dOno6ys7KL7tBfeFUtEREQDyr89lyckJCAuLg5r164FcPYLEYKDg/Hoo49i0aJFPdoHBgYiKysL6enpyrYZM2ZAp9MhJyfnovq0F67YERER0YDU3NyMpqYm5XXmzJkebdra2lBaWoqkpCRlm5ubG5KSklBUVNRrv2fOnIFWa/1AZ51Oh507d150n/bCwo6IiIgGpNGjR8PHx0d5dX1TVXd//vknOjs7YTQarbYbjUaYzeZe+01OTsaqVatw6NAhWCwWFBQUYMuWLairq7voPu3FoV8pRkRERHSxKisrERQUpPzs4eFhk35feeUVzJkzB1FRUVCpVIiIiMDs2bPx7rvv2qT/vsQVOyIiIhqQvLy84O3trbx6K+z8/Pzg7u6O+vp6q+319fUwmUy99jt06FBs27YNra2tqK6uxoEDB6DX6xEeHn7RfdoLCzsiIiJyWhqNBuPHj0dhYaGyzWKxoLCwEImJief9Xa1Wi6CgIHR0dODDDz/Erbfeesl99jVeiiUiIiKntnDhQqSlpSE2Nhbx8fFYvXo1WltbMXv2bADArFmzEBQUpHxGr7i4GLW1tYiOjkZtbS2ef/55WCwWZGRkXHCfjsLCjoiIiJzaXXfdhT/++APPPvsszGYzoqOjkZ+fr9z8UFNTAze3vy9inj59GosXL8bhw4eh1+sxZcoUrF+/HgaD4YL7dBQ+x46IiIgGFJ7Lz83lVuwsFgsAKLcsExER0cDSdQ7vOqfT31yusOu6gyU+Pt7BIyEiIqJLUV9fj5CQEEcPo19xuUuxHR0dKCsrg9FotLqebgvNzc0YPXo0Kisr4eXlZdO+BzLm0hMz6R1z6R1z6R1z6clVMrFYLKivr0dMTAzUapdbozovlyvs+lJTUxN8fHxw4sQJeHt7O3o4/QZz6YmZ9I659I659I659MRMiM+xIyIiInISLOyIiIiInAQLOxvy8PDAc889Z7PvqnMWzKUnZtI75tI75tI75tITMyF+xo6IiIjISXDFjoiIiMhJsLAjIiIichIs7IiIiIicBAs7G3nttdcQGhoKrVaLhIQElJSUOHpIfWr58uWIi4uDl5cX/P39cdttt6GqqsqqzenTp5Geng5fX1/o9XrMmDFD+eaPLjU1NZg6dSo8PT3h7++Pp556Ch0dHfacSp9ZsWIFVCoVFixYoGxz1Uxqa2txzz33wNfXFzqdDmPHjsXu3buV/SKCZ599FgEBAdDpdEhKSsKhQ4es+jh27BhSU1Ph7e0Ng8GABx54AC0tLfaeis10dnZiyZIlCAsLg06nQ0REBF544QV0/9izK+Ty7bff4pZbbkFgYCBUKhW2bdtmtd9WGezbtw/XXXcdtFotgoOD8dJLL/X11C7a+TJpb29HZmYmxo4di8suuwyBgYGYNWsWfv/9d6s+nC0T+heELllubq5oNBp599135ccff5Q5c+aIwWCQ+vp6Rw+tzyQnJ8t7770nFRUVUl5eLlOmTJGQkBBpaWlR2sydO1eCg4OlsLBQdu/eLVdffbVMmDBB2d/R0SFjxoyRpKQkKSsrk88++0z8/Pzk6aefdsSUbKqkpERCQ0PlyiuvlPnz5yvbXTGTY8eOyfDhw+W+++6T4uJiOXz4sHzxxRfy888/K21WrFghPj4+sm3bNtm7d69MmzZNwsLC5NSpU0qblJQUGTdunPzwww/y3XffSWRkpMycOdMRU7KJZcuWia+vr+Tl5cmRI0dk8+bNotfr5ZVXXlHauEIun332mWRlZcmWLVsEgGzdutVqvy0yOHHihBiNRklNTZWKigrZuHGj6HQ6efPNN+01zX/lfJk0NjZKUlKSbNq0SQ4cOCBFRUUSHx8v48ePt+rD2TKhC8fCzgbi4+MlPT1d+bmzs1MCAwNl+fLlDhyVfTU0NAgA+eabb0Tk7MFn0KBBsnnzZqXNTz/9JACkqKhIRM4evNzc3MRsNitt1q1bJ97e3nLmzBn7TsCGmpubZcSIEVJQUCATJ05UCjtXzSQzM1Ouvfbac+63WCxiMplk5cqVyrbGxkbx8PCQjRs3iohIZWWlAJBdu3YpbT7//HNRqVRSW1vbd4PvQ1OnTpX777/fatvtt98uqampIuKaufyziLFVBq+//roMHjzY6m8oMzNTLr/88j6e0aXrrdj9p5KSEgEg1dXVIuL8mdD58VLsJWpra0NpaSmSkpKUbW5ubkhKSkJRUZEDR2ZfJ06cAAAMGTIEAFBaWor29narXKKiohASEqLkUlRUhLFjx8JoNCptkpOT0dTUhB9//NGOo7et9PR0TJ061WrugOtm8vHHHyM2NhZ33HEH/P39ERMTg7ffflvZf+TIEZjNZqtcfHx8kJCQYJWLwWBAbGys0iYpKQlubm4oLi6232RsaMKECSgsLMTBgwcBAHv37sXOnTsxefJkAK6bS3e2yqCoqAjXX389NBqN0iY5ORlVVVU4fvy4nWbTd06cOAGVSgWDwQCAmbg6fnPuJfrzzz/R2dlpdSIGAKPRiAMHDjhoVPZlsViwYMECXHPNNRgzZgwAwGw2Q6PRKAeaLkajEWazWWnTW25d+wai3Nxc7NmzB7t27eqxz1UzOXz4MNatW4eFCxfimWeewa5du/DYY49Bo9EgLS1NmVdv8+6ei7+/v9V+tVqNIUOGDNhcFi1ahKamJkRFRcHd3R2dnZ1YtmwZUlNTAcBlc+nOVhmYzWaEhYX16KNr3+DBg/tk/PZw+vRpZGZmYubMmcp3w7p6Jq6OhR1dsvT0dFRUVGDnzp2OHopDHT16FPPnz0dBQQG0Wq2jh9NvWCwWxMbG4sUXXwQAxMTEoKKiAm+88QbS0tIcPDrH+eCDD7Bhwwa8//77uOKKK1BeXo4FCxYgMDDQpXOhC9fe3o4777wTIoJ169Y5ejjUT/BS7CXy8/ODu7t7jzsb6+vrYTKZHDQq+5k3bx7y8vKwY8cODBs2TNluMpnQ1taGxsZGq/bdczGZTL3m1rVvoCktLUVDQwOuuuoqqNVqqNVqfPPNN1izZg3UajWMRqPLZQIAAQEBGD16tNW2UaNGoaamBsDf8zrf35DJZEJDQ4PV/o6ODhw7dmzA5vLUU09h0aJFuPvuuzF27Fjce++9ePzxx7F8+XIArptLd7bKwBn/rrqKuurqahQUFCirdYDrZkJnsbC7RBqNBuPHj0dhYaGyzWKxoLCwEImJiQ4cWd8SEcybNw9bt27F9u3beyzpjx8/HoMGDbLKpaqqCjU1NUouiYmJ2L9/v9UBqOsA9c9CYCC48cYbsX//fpSXlyuv2NhYpKamKv92tUwA4JprrunxKJyDBw9i+PDhAICwsDCYTCarXJqamlBcXGyVS2NjI0pLS5U227dvh8ViQUJCgh1mYXsnT56Em5v1Idjd3R0WiwWA6+bSna0ySExMxLfffov29nalTUFBAS6//PIBecmxq6g7dOgQvvrqK/j6+lrtd8VMqBtH373hDHJzc8XDw0Oys7OlsrJSHnzwQTEYDFZ3Njqbhx9+WHx8fOTrr7+Wuro65XXy5Emlzdy5cyUkJES2b98uu3fvlsTERElMTFT2dz3a46abbpLy8nLJz8+XoUOHDuhHe/xT97tiRVwzk5KSElGr1bJs2TI5dOiQbNiwQTw9PSUnJ0dps2LFCjEYDPLRRx/Jvn375NZbb+31kRYxMTFSXFwsO3fulBEjRgyox3r8U1pamgQFBSmPO9myZYv4+flJRkaG0sYVcmlubpaysjIpKysTALJq1SopKytT7vC0RQaNjY1iNBrl3nvvlYqKCsnNzRVPT89++2iP82XS1tYm06ZNk2HDhkl5ebnV8bf7Ha7OlgldOBZ2NvLqq69KSEiIaDQaiY+Plx9++MHRQ+pTAHp9vffee0qbU6dOySOPPCKDBw8WT09PmT59utTV1Vn18+uvv8rkyZNFp9OJn5+fPPHEE9Le3m7n2fSdfxZ2rprJJ598ImPGjBEPDw+JioqSt956y2q/xWKRJUuWiNFoFA8PD7nxxhulqqrKqs1ff/0lM2fOFL1eL97e3jJ79mxpbm625zRsqqmpSebPny8hISGi1WolPDxcsrKyrE7OrpDLjh07ej2WpKWliYjtMti7d69ce+214uHhIUFBQbJixQp7TfFfO18mR44cOefxd8eOHUofzpYJXTiVSLfHnBMRERHRgMXP2BERERE5CRZ2RERERE6ChR0RERGRk2BhR0REROQkWNgREREROQkWdkREREROgoUdERERkZNgYUdERETkJFjYEZHLy87OhsFgcPQwiIguGQs7Iuo37rvvPqhUKuXl6+uLlJQU7Nu374L7eP755xEdHd13gyQi6sdY2BFRv5KSkoK6ujrU1dWhsLAQarUaN998s6OHRUQ0ILCwI6J+xcPDAyaTCSaTCdHR0Vi0aBGOHj2KP/74AwCQmZmJkSNHwtPTE+Hh4ViyZAna29sBnL2k+p///Ad79+5VVv2ys7MBAI2NjXjooYdgNBqh1WoxZswY5OXlWb33F198gVGjRkGv1ysFJhHRQKJ29ACIiM6lpaUFOTk5iIyMhK+vLwDAy8sL2dnZCAwMxP79+zFnzhx4eXkhIyMDd911FyoqKpCfn4+vvvoKAODj4wOLxYLJkyejubkZOTk5iIiIQGVlJdzd3ZX3OnnyJF5++WWsX78ebm5uuOeee/Dkk09iw4YNDpk7EdHFYGFHRP1KXl4e9Ho9AKC1tRUBAQHIy8uDm9vZCwyLFy9W2oaGhuLJJ59Ebm4uMjIyoNPpoNfroVarYTKZlHZffvklSkpK8NNPP2HkyJEAgPDwcKv3bW9vxxtvvIGIiAgAwLx587B06dI+nSsRka2xsCOifmXSpElYt24dAOD48eN4/fXXMXnyZJSUlGD48OHYtGkT1qxZg19++QUtLS3o6OiAt7f3efssLy/HsGHDlKKuN56enkpRBwABAQFoaGiwzaSIiOyEn7Ejon7lsssuQ2RkJCIjIxEXF4d33nkHra2tePvtt1FUVITU1FRMmTIFeXl5KCsrQ1ZWFtra2s7bp06n+6/vO2jQIKufVSoVROSS5kJEZG9csSOifk2lUsHNzQ2nTp3C999/j+HDhyMrK0vZX11dbdVeo9Ggs7PTatuVV16J3377DQcPHjzvqh0R0UDHwo6I+pUzZ87AbDYDOHspdu3atWhpacEtt9yCpqYm1NTUIDc3F3Fxcfj000+xdetWq98PDQ3FkSNHlMuvXl5emDhxIq6//nrMmDEDq1atQmRkJA4cOACVSoWUlBRHTJOIqE/wUiwR9Sv5+fkICAhAQEAAEhISsGvXLmzevBk33HADpk2bhscffxzz5s1DdHQ0vv/+eyxZssTq92fMmIGUlBRMmjQJQ4cOxcaNGwEAH374IeLi4jBz5kyMHj0aGRkZPVb2iIgGOpXwQyREREREToErdkREREROgoUdERERkZNgYUdERETkJFjYERERETkJFnZEREREToKFHREREZGTYGFHRERE5CRY2BERERE5CRZ2RERERE6ChR0RERGRk2BhR0REROQkWNgREREROYn/A+Oh9gQMFYsyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"training_progress/checkpoint_ep0_b1299.pt\")\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(checkpoint['loss_history'], label='Loss')\n",
    "ax1.set_xlabel('Batch')\n",
    "ax1.set_ylabel('Training Loss')\n",
    "ax1.set_title('Training Loss and Accuracy')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot([x[0] for x in checkpoint['acc_history']], [1 - x[1] for x in checkpoint['acc_history']], color='red', label='Accuracy')\n",
    "ax2.set_ylabel('Hamming Accuracy')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
