{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Transformer class\n",
    "Given an input of \n",
    "\n",
    "`(batch_size, seq_length, d_model)`, \n",
    "\n",
    "generate \n",
    "\n",
    "`(batch_size, out_seq_length, d_model)`.\n",
    "\n",
    "During training, target is also \n",
    "\n",
    "`(batch_size, out_seq_length, d_model)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Transformer(nhead=8, num_encoder_layers=2, num_decoder_layers=2, batch_first=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "src = torch.rand((32, 10, 512))\n",
    "tgt = torch.rand((32, 20, 512))\n",
    "out = model(src, tgt)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A basic Transformer that could work for ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGTransformer(nn.Module):\n",
    "    def __init__(self, d_model, num_classes=63, nhead=8, num_encoder_layers=2, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "\n",
    "        # Encoder stack\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.transformer(x)\n",
    "        # encoded shape: (batch_size, seq_len, d_model)\n",
    "        # Pick out only the last in the sequence for classification\n",
    "        encoded = encoded[:, -1, :]\n",
    "        result = self.classifier(encoded)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGTransformer(\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=12, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=12, bias=True)\n",
      "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=63, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ECGTransformer(d_model=12, nhead=4, num_classes=63, num_encoder_layers=6, dim_feedforward=512)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81723\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 81723 parameters\n",
      "transformer: 80904 parameters\n",
      "transformer.layers: 80904 parameters\n",
      "transformer.layers.0: 13484 parameters\n",
      "transformer.layers.0.self_attn: 624 parameters\n",
      "transformer.layers.0.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.0.linear1: 6656 parameters\n",
      "transformer.layers.0.dropout: 0 parameters\n",
      "transformer.layers.0.linear2: 6156 parameters\n",
      "transformer.layers.0.norm1: 24 parameters\n",
      "transformer.layers.0.norm2: 24 parameters\n",
      "transformer.layers.0.dropout1: 0 parameters\n",
      "transformer.layers.0.dropout2: 0 parameters\n",
      "transformer.layers.1: 13484 parameters\n",
      "transformer.layers.1.self_attn: 624 parameters\n",
      "transformer.layers.1.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.1.linear1: 6656 parameters\n",
      "transformer.layers.1.dropout: 0 parameters\n",
      "transformer.layers.1.linear2: 6156 parameters\n",
      "transformer.layers.1.norm1: 24 parameters\n",
      "transformer.layers.1.norm2: 24 parameters\n",
      "transformer.layers.1.dropout1: 0 parameters\n",
      "transformer.layers.1.dropout2: 0 parameters\n",
      "transformer.layers.2: 13484 parameters\n",
      "transformer.layers.2.self_attn: 624 parameters\n",
      "transformer.layers.2.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.2.linear1: 6656 parameters\n",
      "transformer.layers.2.dropout: 0 parameters\n",
      "transformer.layers.2.linear2: 6156 parameters\n",
      "transformer.layers.2.norm1: 24 parameters\n",
      "transformer.layers.2.norm2: 24 parameters\n",
      "transformer.layers.2.dropout1: 0 parameters\n",
      "transformer.layers.2.dropout2: 0 parameters\n",
      "transformer.layers.3: 13484 parameters\n",
      "transformer.layers.3.self_attn: 624 parameters\n",
      "transformer.layers.3.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.3.linear1: 6656 parameters\n",
      "transformer.layers.3.dropout: 0 parameters\n",
      "transformer.layers.3.linear2: 6156 parameters\n",
      "transformer.layers.3.norm1: 24 parameters\n",
      "transformer.layers.3.norm2: 24 parameters\n",
      "transformer.layers.3.dropout1: 0 parameters\n",
      "transformer.layers.3.dropout2: 0 parameters\n",
      "transformer.layers.4: 13484 parameters\n",
      "transformer.layers.4.self_attn: 624 parameters\n",
      "transformer.layers.4.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.4.linear1: 6656 parameters\n",
      "transformer.layers.4.dropout: 0 parameters\n",
      "transformer.layers.4.linear2: 6156 parameters\n",
      "transformer.layers.4.norm1: 24 parameters\n",
      "transformer.layers.4.norm2: 24 parameters\n",
      "transformer.layers.4.dropout1: 0 parameters\n",
      "transformer.layers.4.dropout2: 0 parameters\n",
      "transformer.layers.5: 13484 parameters\n",
      "transformer.layers.5.self_attn: 624 parameters\n",
      "transformer.layers.5.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.5.linear1: 6656 parameters\n",
      "transformer.layers.5.dropout: 0 parameters\n",
      "transformer.layers.5.linear2: 6156 parameters\n",
      "transformer.layers.5.norm1: 24 parameters\n",
      "transformer.layers.5.norm2: 24 parameters\n",
      "transformer.layers.5.dropout1: 0 parameters\n",
      "transformer.layers.5.dropout2: 0 parameters\n",
      "classifier: 819 parameters\n",
      "classifier.0: 819 parameters\n",
      "classifier.1: 0 parameters\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    params = sum(p.numel() for p in module.parameters())\n",
    "    print(f\"{name}: {params} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/torch/nn/functional.py:5614: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /build/python-pytorch/src/pytorch-rocm/aten/src/ATen/Context.cpp:310.)\n",
      "  proj = linear(q, w, b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 63])\n",
      "tensor([0.3445, 0.6134, 0.3308, 0.5061, 0.6985, 0.6163, 0.4630, 0.4196, 0.3788,\n",
      "        0.3524, 0.5629, 0.4439, 0.5261, 0.7331, 0.4000, 0.7133, 0.3199, 0.2308,\n",
      "        0.7299, 0.4106, 0.6710, 0.6334, 0.6253, 0.6197, 0.4280, 0.7458, 0.3168,\n",
      "        0.5989, 0.4369, 0.5994, 0.5256, 0.4640, 0.7689, 0.5553, 0.6169, 0.4644,\n",
      "        0.3360, 0.7362, 0.6294, 0.6007, 0.5448, 0.7205, 0.3767, 0.3987, 0.2950,\n",
      "        0.4072, 0.6089, 0.4929, 0.2895, 0.5062, 0.6890, 0.2740, 0.5677, 0.4495,\n",
      "        0.3671, 0.4324, 0.3318, 0.4679, 0.3701, 0.5554, 0.7422, 0.2648, 0.6657],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.rand((2, 5000, 12)).to(device)\n",
    "out = model(inputs)\n",
    "print(out.shape)\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3445, 0.6134, 0.3308, 0.5061, 0.6985, 0.6163, 0.4630, 0.4196, 0.3788,\n",
      "         0.3524, 0.5629, 0.4439, 0.5261, 0.7331, 0.4000, 0.7133, 0.3199, 0.2308,\n",
      "         0.7299, 0.4106, 0.6710, 0.6334, 0.6253, 0.6197, 0.4280, 0.7458, 0.3168,\n",
      "         0.5989, 0.4369, 0.5994, 0.5256, 0.4640, 0.7689, 0.5553, 0.6169, 0.4644,\n",
      "         0.3360, 0.7362, 0.6294, 0.6007, 0.5448, 0.7205, 0.3767, 0.3987, 0.2950,\n",
      "         0.4072, 0.6089, 0.4929, 0.2895, 0.5062, 0.6890, 0.2740, 0.5677, 0.4495,\n",
      "         0.3671, 0.4324, 0.3318, 0.4679, 0.3701, 0.5554, 0.7422, 0.2648, 0.6657],\n",
      "        [0.4353, 0.4194, 0.3749, 0.4381, 0.5999, 0.6827, 0.2940, 0.3459, 0.5030,\n",
      "         0.5124, 0.5436, 0.6546, 0.4196, 0.4524, 0.6050, 0.6886, 0.3235, 0.3779,\n",
      "         0.5857, 0.6351, 0.8313, 0.7327, 0.6514, 0.5495, 0.3315, 0.5373, 0.4017,\n",
      "         0.5286, 0.6734, 0.3994, 0.6043, 0.3770, 0.6200, 0.6178, 0.5695, 0.7738,\n",
      "         0.6191, 0.6293, 0.5350, 0.4410, 0.8078, 0.7634, 0.3866, 0.6132, 0.6401,\n",
      "         0.2863, 0.6836, 0.4551, 0.3866, 0.3986, 0.6677, 0.4007, 0.4102, 0.5583,\n",
      "         0.5780, 0.4447, 0.6746, 0.3023, 0.4034, 0.7245, 0.6479, 0.2695, 0.3697],\n",
      "        [0.4438, 0.6271, 0.3404, 0.5770, 0.7200, 0.5863, 0.5172, 0.4870, 0.3864,\n",
      "         0.4506, 0.5469, 0.4141, 0.5624, 0.7258, 0.3828, 0.7586, 0.3905, 0.1993,\n",
      "         0.7155, 0.4243, 0.6695, 0.6626, 0.6492, 0.5445, 0.3964, 0.7278, 0.3548,\n",
      "         0.6221, 0.4268, 0.6061, 0.5433, 0.4591, 0.7124, 0.5524, 0.6219, 0.4288,\n",
      "         0.3573, 0.7031, 0.6300, 0.6332, 0.4880, 0.6912, 0.3677, 0.3638, 0.3210,\n",
      "         0.3616, 0.5256, 0.4660, 0.3105, 0.5335, 0.6864, 0.3458, 0.5884, 0.5479,\n",
      "         0.3382, 0.4460, 0.3057, 0.4239, 0.3992, 0.4669, 0.6808, 0.2749, 0.6140]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
