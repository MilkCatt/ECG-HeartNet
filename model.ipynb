{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import F1Score\n",
    "from sklearn.metrics import multilabel_confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "amdgpu.ids: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Define device for torch\n",
    "device = torch.device(\"cpu\")\n",
    "# MPS for Apple Silicon GPUs\n",
    "if torch.mps.is_available():\n",
    "   print(\"MPS is available\")\n",
    "   device = torch.device(\"mps\")\n",
    "\n",
    "# CUDA for Nvidia GPUs\n",
    "if torch.cuda.is_available():\n",
    "   print(\"CUDA is available\")\n",
    "   device = torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    for name, module in model.named_modules():\n",
    "        params = sum(p.numel() for p in module.parameters())\n",
    "        print(f\"{name}: {params} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader\n",
    "\n",
    "To investigate: Normalization or other transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeECG:\n",
    "    def __call__(self, tensor):\n",
    "        # Z-score normalization per lead\n",
    "        means = tensor.mean(dim=1, keepdim=True)\n",
    "        stds = tensor.std(dim=1, keepdim=True)\n",
    "        return (tensor - means) / (stds + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, path=\"data/ecg\", diagnoses='data/diagnoses.csv', transform=None):\n",
    "        # Load and prepare labels\n",
    "        self.labels_df = pd.read_csv(diagnoses)\n",
    "\n",
    "        self.labels_df['ID'] = self.labels_df['ID'].astype(str).str.replace(r'\\D', '', regex=True) # Remove the JS\n",
    "        self.labels_df.set_index('ID', inplace=True)\n",
    "        self.num_classes = self.labels_df.shape[1]\n",
    "        print(f'Number of classes: {self.num_classes}')\n",
    "\n",
    "        self.transform = transform\n",
    "        self.cache = {}\n",
    "\n",
    "    def get_pos_weights(self):\n",
    "        # Compute counts\n",
    "        pos_counts = self.labels_df.sum()\n",
    "        neg_counts = len(self.labels_df) - pos_counts\n",
    "\n",
    "        # Calculate pos_weight = #neg / #pos for each class\n",
    "        pos_weight = (neg_counts / pos_counts).values\n",
    "\n",
    "        # Move to device\n",
    "        pos_weight_tensor = torch.tensor(pos_weight, dtype=torch.float32, device=device)\n",
    "        return pos_weight_tensor\n",
    "    \n",
    "    def get_num_classes(self):\n",
    "        return self.num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.cache:\n",
    "            return self.cache[idx]\n",
    "        \n",
    "        try:\n",
    "            # Access the row through iloc of the index,\n",
    "            # Use the ID to make filepath\n",
    "            ID = self.labels_df.iloc[idx].name\n",
    "\n",
    "            file_path = f'data/ecg/{ID}.csv'\n",
    "            \n",
    "            # Load ECG data\n",
    "            df = pd.read_csv(file_path)\n",
    "            ecg_data = df.drop(columns=['time']).values\n",
    "            tensor = torch.tensor(ecg_data, dtype=torch.float32).T  # (leads, timesteps)\n",
    "            \n",
    "            if self.transform:\n",
    "                tensor = self.transform(tensor)\n",
    "                \n",
    "            # Get corresponding label\n",
    "\n",
    "            label_values = self.labels_df.loc[ID].values  # Get all label columns\n",
    "            label = torch.tensor(label_values, dtype=torch.float32)  # Use float for multi-label\n",
    "\n",
    "            return tensor, label\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {str(e)}\")\n",
    "            # Return zero tensor and -1 label placeholder\n",
    "            return torch.zeros((12, 5000), dtype=torch.float32), torch.full((self.num_classes,), -1, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 5\n",
      "torch.Size([12, 5000])\n",
      "torch.Size([5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.1199, 3.1205, 3.1199, 3.1205, 3.1205], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ECGDataset(path=\"data/ecg\", diagnoses='data/diagnoses_balanced.csv')\n",
    "data, label = dataset.__getitem__(23423)\n",
    "print(data.shape)\n",
    "print(label.shape)\n",
    "dataset.get_pos_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGTransformer(nn.Module):\n",
    "    def __init__(self, d_model, num_classes=63, nhead=8, num_encoder_layers=2, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "\n",
    "        # Encoder stack\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.transformer(x)\n",
    "        # encoded shape: (batch_size, seq_len, d_model)\n",
    "        # Pick out only the last in the sequence for classification\n",
    "        encoded = encoded[:, -1, :]\n",
    "        result = self.classifier(encoded)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGTransformer(\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=12, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=12, bias=True)\n",
      "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=12, out_features=63, bias=True)\n",
      ")\n",
      ": 81723 parameters\n",
      "transformer: 80904 parameters\n",
      "transformer.layers: 80904 parameters\n",
      "transformer.layers.0: 13484 parameters\n",
      "transformer.layers.0.self_attn: 624 parameters\n",
      "transformer.layers.0.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.0.linear1: 6656 parameters\n",
      "transformer.layers.0.dropout: 0 parameters\n",
      "transformer.layers.0.linear2: 6156 parameters\n",
      "transformer.layers.0.norm1: 24 parameters\n",
      "transformer.layers.0.norm2: 24 parameters\n",
      "transformer.layers.0.dropout1: 0 parameters\n",
      "transformer.layers.0.dropout2: 0 parameters\n",
      "transformer.layers.1: 13484 parameters\n",
      "transformer.layers.1.self_attn: 624 parameters\n",
      "transformer.layers.1.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.1.linear1: 6656 parameters\n",
      "transformer.layers.1.dropout: 0 parameters\n",
      "transformer.layers.1.linear2: 6156 parameters\n",
      "transformer.layers.1.norm1: 24 parameters\n",
      "transformer.layers.1.norm2: 24 parameters\n",
      "transformer.layers.1.dropout1: 0 parameters\n",
      "transformer.layers.1.dropout2: 0 parameters\n",
      "transformer.layers.2: 13484 parameters\n",
      "transformer.layers.2.self_attn: 624 parameters\n",
      "transformer.layers.2.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.2.linear1: 6656 parameters\n",
      "transformer.layers.2.dropout: 0 parameters\n",
      "transformer.layers.2.linear2: 6156 parameters\n",
      "transformer.layers.2.norm1: 24 parameters\n",
      "transformer.layers.2.norm2: 24 parameters\n",
      "transformer.layers.2.dropout1: 0 parameters\n",
      "transformer.layers.2.dropout2: 0 parameters\n",
      "transformer.layers.3: 13484 parameters\n",
      "transformer.layers.3.self_attn: 624 parameters\n",
      "transformer.layers.3.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.3.linear1: 6656 parameters\n",
      "transformer.layers.3.dropout: 0 parameters\n",
      "transformer.layers.3.linear2: 6156 parameters\n",
      "transformer.layers.3.norm1: 24 parameters\n",
      "transformer.layers.3.norm2: 24 parameters\n",
      "transformer.layers.3.dropout1: 0 parameters\n",
      "transformer.layers.3.dropout2: 0 parameters\n",
      "transformer.layers.4: 13484 parameters\n",
      "transformer.layers.4.self_attn: 624 parameters\n",
      "transformer.layers.4.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.4.linear1: 6656 parameters\n",
      "transformer.layers.4.dropout: 0 parameters\n",
      "transformer.layers.4.linear2: 6156 parameters\n",
      "transformer.layers.4.norm1: 24 parameters\n",
      "transformer.layers.4.norm2: 24 parameters\n",
      "transformer.layers.4.dropout1: 0 parameters\n",
      "transformer.layers.4.dropout2: 0 parameters\n",
      "transformer.layers.5: 13484 parameters\n",
      "transformer.layers.5.self_attn: 624 parameters\n",
      "transformer.layers.5.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.5.linear1: 6656 parameters\n",
      "transformer.layers.5.dropout: 0 parameters\n",
      "transformer.layers.5.linear2: 6156 parameters\n",
      "transformer.layers.5.norm1: 24 parameters\n",
      "transformer.layers.5.norm2: 24 parameters\n",
      "transformer.layers.5.dropout1: 0 parameters\n",
      "transformer.layers.5.dropout2: 0 parameters\n",
      "classifier: 819 parameters\n"
     ]
    }
   ],
   "source": [
    "model = ECGTransformer(d_model=12, nhead=4, num_classes=63, num_encoder_layers=6, dim_feedforward=512)\n",
    "print(model)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 63])\n",
      "tensor([-0.3114,  0.7817,  0.3441,  0.6371,  0.4557, -0.2647,  0.0785,  1.4021,\n",
      "        -0.5778,  0.5539, -1.1239, -0.5755, -0.3548, -1.1942, -0.7231, -0.9842,\n",
      "         0.3549,  0.3671,  0.0777, -1.0710, -0.1278, -0.1683,  0.3399, -0.8317,\n",
      "         1.1035,  0.4088,  0.4098, -0.8206, -0.8247, -0.5101,  0.8955, -0.2588,\n",
      "        -0.2010, -0.1479, -0.2765, -0.0554, -0.1968, -0.2181,  0.3201,  0.0775,\n",
      "         0.3281,  0.6800, -0.1614,  0.1403, -0.5414,  0.5355, -0.1686,  0.0174,\n",
      "        -0.4426, -0.6130,  1.1272, -1.0367,  0.0677, -0.2011, -0.3319,  1.0212,\n",
      "        -0.7615,  0.7125, -0.5321,  0.2784, -0.0158,  0.4694,  0.9192],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.rand((2, 5000, 12))\n",
    "out = model(inputs)\n",
    "print(out.shape)\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An embedding model \n",
    "that uses convolution\n",
    "\n",
    "Convolution turning 12 channels to 128, repeated to transfer forward 200ms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGEmbeddings_old(nn.Module):\n",
    "    def __init__(self, d_input, d_model, n_conv_layers=8):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(d_model if i>0 else d_input, d_model, 51, stride=1, padding='same')\n",
    "            for i in range(n_conv_layers)\n",
    "        ])\n",
    "        self.activation = nn.ReLU(inplace=False)  # Important for checkpointing\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.conv_layers)):\n",
    "            x = self.conv_layers[i](x)\n",
    "            if i < len(self.conv_layers) - 1:\n",
    "                x = self.activation(x)\n",
    "\n",
    "        if not x.requires_grad:\n",
    "            x = x.detach().requires_grad_(True)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGEmbeddings(nn.Module):\n",
    "    def __init__(self, d_input, d_model):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        in_channels = d_input\n",
    "        out_channels = d_input\n",
    "\n",
    "        # Dynamically adjust the number of channels to reach d_model\n",
    "        while out_channels < d_model:\n",
    "            out_channels = min(d_model, out_channels * 2)  # Double channels, but cap at d_model\n",
    "            self.conv_layers.append(nn.Conv1d(in_channels, out_channels, 1, stride=1, padding='same'))\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.activation = nn.ReLU(inplace=False)  # Important for checkpointing\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, conv in enumerate(self.conv_layers):\n",
    "            x = conv(x)\n",
    "            if i < len(self.conv_layers) - 1:  # Apply activation except for the last layer\n",
    "                x = self.activation(x)\n",
    "\n",
    "        if not x.requires_grad:\n",
    "            x = x.detach().requires_grad_(True)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGEmbeddings(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv1d(12, 24, kernel_size=(1,), stride=(1,), padding=same)\n",
      "    (1): Conv1d(24, 48, kernel_size=(1,), stride=(1,), padding=same)\n",
      "    (2): Conv1d(48, 96, kernel_size=(1,), stride=(1,), padding=same)\n",
      "    (3): Conv1d(96, 192, kernel_size=(1,), stride=(1,), padding=same)\n",
      "    (4): Conv1d(192, 256, kernel_size=(1,), stride=(1,), padding=same)\n",
      "  )\n",
      "  (activation): ReLU()\n",
      ")\n",
      ": 74248 parameters\n",
      "conv_layers: 74248 parameters\n",
      "conv_layers.0: 312 parameters\n",
      "conv_layers.1: 1200 parameters\n",
      "conv_layers.2: 4704 parameters\n",
      "conv_layers.3: 18624 parameters\n",
      "conv_layers.4: 49408 parameters\n",
      "activation: 0 parameters\n"
     ]
    }
   ],
   "source": [
    "embedding_model = ECGEmbeddings(d_input=12, d_model=256)\n",
    "print(embedding_model)\n",
    "count_parameters(embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining together embedding with transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGCombined(nn.Module):\n",
    "    def __init__(self, d_input, d_model, num_classes=63, nhead=8, num_encoder_layers=2, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.embedding_model = ECGEmbeddings(d_input, d_model)\n",
    "        self.transformer = ECGTransformer(d_model, num_classes, nhead, num_encoder_layers, dim_feedforward)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_model(x)\n",
    "        x = x.permute(0, 2, 1)       # Reshape to (batch_size, seq_len, d_model)\n",
    "        x = self.transformer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, device, pos_weights=None, accum_steps=4, checkpoint_interval=256, lr=1e-4,\n",
    "                 resume_checkpoint=None):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.accum_steps = accum_steps\n",
    "        self.checkpoint_interval = checkpoint_interval\n",
    "        \n",
    "        # Initialize essential components\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.f1 = F1Score(task='multilabel', num_labels=self.model.num_classes, average=None)\n",
    "        self.loss = nn.BCEWithLogitsLoss(pos_weight=pos_weights)   \n",
    "        self.accum_loss = 0.0\n",
    "        self.loss_history = []\n",
    "        self.acc_history = []\n",
    "        self.batch_count = 0\n",
    "        self.start_epoch = 0\n",
    "        self.start_batch = 0\n",
    "\n",
    "        if resume_checkpoint:\n",
    "            self._load_checkpoint(resume_checkpoint)\n",
    "\n",
    "    def _load_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"Load training state from checkpoint\"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)\n",
    "        \n",
    "        # Essential parameters\n",
    "        self.model.load_state_dict(checkpoint['model_state'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        \n",
    "        # Training progress\n",
    "        self.loss_history = checkpoint['loss_history']\n",
    "        self.acc_history = checkpoint['acc_history']\n",
    "        self.batch_count = checkpoint.get('batch_count', 0)\n",
    "        self.start_epoch = checkpoint['epoch']\n",
    "        self.start_batch = checkpoint.get('batch', 0) + 1\n",
    "        \n",
    "        # Configurations\n",
    "        self.checkpoint_interval = checkpoint.get('checkpoint_interval', \n",
    "                                                 self.checkpoint_interval)\n",
    "        \n",
    "        print(f\"Resuming from epoch {self.start_epoch} batch {self.start_batch}\")\n",
    "\n",
    "    def train(self, train_dataloader, test_dataloader, num_epochs, save_path=\"training_progress\"):\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(self.start_epoch, num_epochs):\n",
    "            for batch_idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "                if batch_idx < self.start_batch:\n",
    "                    continue\n",
    "                \n",
    "                # Forward pass\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.loss(outputs, labels) / self.accum_steps\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Every batch\n",
    "                self.accum_loss += loss.item()\n",
    "                self.batch_count += 1\n",
    "                \n",
    "                # Every accum_steps\n",
    "                if (batch_idx + 1) % self.accum_steps == 0:\n",
    "                    self._update_parameters()\n",
    "                    \n",
    "                    # Save loss\n",
    "                    avg_loss = self.accum_loss\n",
    "                    self.loss_history.append([self.batch_count, avg_loss])\n",
    "                    self.accum_loss = 0.0\n",
    "\n",
    "                    print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx+1}/{len(train_dataloader)} | \"\n",
    "                        f\"Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "                # Every checkpoint_interval\n",
    "                if self.batch_count % self.checkpoint_interval == 0:\n",
    "                    acc = self.evaluate(test_dataloader)\n",
    "                    self.acc_history.append([self.batch_count, acc])\n",
    "                    self._save_checkpoint(save_path, epoch, batch_idx)\n",
    "                \n",
    "                if self.batch_count == 64:\n",
    "                    probs = torch.sigmoid(outputs)\n",
    "                    print(\"➡️ Example probs:\", probs[0].detach().cpu().numpy())\n",
    "                    print(\"➡️ Predictions:\", (probs > 0.5).float()[0])\n",
    "                    print(\"➡️ Ground truth :\", labels[0].cpu().numpy())\n",
    "                \n",
    "                del inputs, labels, outputs, loss\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_samples = 0\n",
    "        num_classes = self.model.num_classes\n",
    "        mismatches_per_class = torch.zeros(num_classes, device=self.device)\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                \n",
    "                # Get binary predictions (0 or 1) using threshold\n",
    "                predicted = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "\n",
    "                all_preds.append(predicted.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "                \n",
    "                # Track mismatches per class\n",
    "                mismatches_per_class += (predicted != labels).sum(dim=0).float()\n",
    "                total_samples += inputs.size(0)  # Batch size\n",
    "\n",
    "        # Concatenate all predictions and labels\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        y_true = all_labels.numpy().astype(int)\n",
    "        y_pred = all_preds.numpy().astype(int)  \n",
    "\n",
    "        # Accuracy metrics\n",
    "        f1_score_per_class = self.f1(all_preds, all_labels).cpu().numpy()\n",
    "        print(f\"F1 Score per class: {f1_score_per_class}\")\n",
    "        hamming_loss_per_class = mismatches_per_class.cpu().numpy() / total_samples\n",
    "\n",
    "        # Per-class metrics\n",
    "        f1_score_per_class = self.f1(all_preds, all_labels).cpu().numpy()\n",
    "        hamming_loss_per_class = mismatches_per_class.cpu().numpy() / total_samples\n",
    "        overall_hamming_loss = mismatches_per_class.sum().item() / (total_samples * num_classes)\n",
    "\n",
    "        precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "        recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "\n",
    "        # Extract TP, FP, FN, TN from multilabel confusion matrices\n",
    "        conf_matrices = multilabel_confusion_matrix(y_true, y_pred)\n",
    "        tp = conf_matrices[:, 1, 1].tolist()\n",
    "        fp = conf_matrices[:, 0, 1].tolist()\n",
    "        fn = conf_matrices[:, 1, 0].tolist()\n",
    "        tn = conf_matrices[:, 0, 0].tolist()\n",
    "\n",
    "        print(f\"F1 Score per class: {f1_score_per_class}\")\n",
    "\n",
    "        return {\n",
    "            \"f1_per_class\": f1_score_per_class,\n",
    "            \"overall_hamming_loss\": overall_hamming_loss,\n",
    "            \"hamming_loss_per_class\": hamming_loss_per_class,\n",
    "            \"precision\": precision_per_class,\n",
    "            \"recall\": recall_per_class,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn\n",
    "        }\n",
    "    \n",
    "    def _update_parameters(self):\n",
    "        \"\"\"Update model parameters with gradient clipping\"\"\"\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def _save_checkpoint(self, path, epoch, batch_idx):\n",
    "        \"\"\"Save model and training state\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'batch': batch_idx,\n",
    "            'batch_count': self.batch_count,\n",
    "            'checkpoint_interval': self.checkpoint_interval,\n",
    "            'model_state': self.model.state_dict(),\n",
    "            'optimizer_state': self.optimizer.state_dict(),\n",
    "            'loss_history': self.loss_history,\n",
    "            'acc_history': self.acc_history\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, f\"{path}/checkpoint_ep{epoch}_b{batch_idx}.pt\")\n",
    "        print(f\"\\nCheckpoint saved at epoch {epoch+1} batch {batch_idx+1}\")\n",
    "\n",
    "        np.save(f\"{path}/loss_history.npy\", np.array(self.loss_history))\n",
    "        np.save(f\"{path}/acc_history.npy\", np.array(self.acc_history))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Go Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta\n",
    "diagnoses = \"data/diagnoses_balanced.csv\"\n",
    "data_path = \"data/ecg_clipped\"\n",
    "save_path = \"training_progress/new_balanced\"\n",
    "checkpoint_interval = 64\n",
    "\n",
    "# Hyperparameters\n",
    "add_pos_weights = True\n",
    "normalize = True\n",
    "batch_size = 4\n",
    "accum_steps = 4         # Updates every accum_steps batches\n",
    "starting_lr = 1e-5      # For resuming, set lr (could be lower) at the resume cell below\n",
    "\n",
    "# Embeddings parameters\n",
    "d_input = 12\n",
    "d_model = 128\n",
    "\n",
    "# Transformer parameters\n",
    "nhead = 4\n",
    "num_encoder_layers = 2\n",
    "dim_feedforward = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 5\n"
     ]
    }
   ],
   "source": [
    "ecg_dataset = ECGDataset(diagnoses=diagnoses, transform=NormalizeECG() if normalize else None)\n",
    "pos_weights = ecg_dataset.get_pos_weights() if add_pos_weights else None\n",
    "num_classes = ecg_dataset.get_num_classes()\n",
    "\n",
    "train_dataset, test_dataset, val_dataset = random_split(\n",
    "                                            ecg_dataset, [len(ecg_dataset) - 1000, 500, 500])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start from 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiacheng/dl-proj/.venv/lib/python3.12/site-packages/torch/nn/functional.py:5614: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:310.)\n",
      "  proj = linear(q, w, b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Batch 4/6998 | Avg Loss: 1.0758\n",
      "Epoch 1/1 | Batch 8/6998 | Avg Loss: 1.0608\n",
      "Epoch 1/1 | Batch 12/6998 | Avg Loss: 1.1923\n",
      "Epoch 1/1 | Batch 16/6998 | Avg Loss: 1.0454\n",
      "Epoch 1/1 | Batch 20/6998 | Avg Loss: 1.0194\n",
      "Epoch 1/1 | Batch 24/6998 | Avg Loss: 1.2011\n",
      "Epoch 1/1 | Batch 28/6998 | Avg Loss: 1.0529\n",
      "Epoch 1/1 | Batch 32/6998 | Avg Loss: 1.0375\n",
      "Epoch 1/1 | Batch 36/6998 | Avg Loss: 1.0557\n",
      "Epoch 1/1 | Batch 40/6998 | Avg Loss: 1.0674\n",
      "Epoch 1/1 | Batch 44/6998 | Avg Loss: 1.0651\n",
      "Epoch 1/1 | Batch 48/6998 | Avg Loss: 1.0788\n",
      "Epoch 1/1 | Batch 52/6998 | Avg Loss: 1.1982\n",
      "Epoch 1/1 | Batch 56/6998 | Avg Loss: 1.1743\n",
      "Epoch 1/1 | Batch 60/6998 | Avg Loss: 1.1770\n",
      "Epoch 1/1 | Batch 64/6998 | Avg Loss: 1.0971\n",
      "➡️ Example probs: [0.57436013 0.4888995  0.4334205  0.7291495  0.5747777 ]\n",
      "➡️ Predictions: tensor([1., 0., 0., 1., 1.], device='cuda:0')\n",
      "➡️ Ground truth : [0. 0. 0. 1. 0.]\n",
      "Epoch 1/1 | Batch 68/6998 | Avg Loss: 1.1165\n",
      "Epoch 1/1 | Batch 72/6998 | Avg Loss: 1.1874\n",
      "Epoch 1/1 | Batch 76/6998 | Avg Loss: 1.1568\n",
      "Epoch 1/1 | Batch 80/6998 | Avg Loss: 1.1368\n",
      "Epoch 1/1 | Batch 84/6998 | Avg Loss: 1.1474\n",
      "Epoch 1/1 | Batch 88/6998 | Avg Loss: 1.1399\n",
      "Epoch 1/1 | Batch 92/6998 | Avg Loss: 1.1190\n",
      "Epoch 1/1 | Batch 96/6998 | Avg Loss: 1.1217\n",
      "Epoch 1/1 | Batch 100/6998 | Avg Loss: 1.2080\n",
      "Epoch 1/1 | Batch 104/6998 | Avg Loss: 1.1073\n",
      "Epoch 1/1 | Batch 108/6998 | Avg Loss: 1.1535\n",
      "Epoch 1/1 | Batch 112/6998 | Avg Loss: 1.0466\n",
      "Epoch 1/1 | Batch 116/6998 | Avg Loss: 0.9572\n",
      "Epoch 1/1 | Batch 120/6998 | Avg Loss: 1.0752\n",
      "Epoch 1/1 | Batch 124/6998 | Avg Loss: 1.0621\n",
      "Epoch 1/1 | Batch 128/6998 | Avg Loss: 1.1016\n",
      "Epoch 1/1 | Batch 132/6998 | Avg Loss: 1.0795\n",
      "Epoch 1/1 | Batch 136/6998 | Avg Loss: 0.9924\n",
      "Epoch 1/1 | Batch 140/6998 | Avg Loss: 1.0889\n",
      "Epoch 1/1 | Batch 144/6998 | Avg Loss: 1.1118\n",
      "Epoch 1/1 | Batch 148/6998 | Avg Loss: 1.0349\n",
      "Epoch 1/1 | Batch 152/6998 | Avg Loss: 1.0833\n",
      "Epoch 1/1 | Batch 156/6998 | Avg Loss: 1.1046\n",
      "Epoch 1/1 | Batch 160/6998 | Avg Loss: 1.0397\n",
      "Epoch 1/1 | Batch 164/6998 | Avg Loss: 1.0825\n",
      "Epoch 1/1 | Batch 168/6998 | Avg Loss: 1.1259\n",
      "Epoch 1/1 | Batch 172/6998 | Avg Loss: 1.1241\n",
      "Epoch 1/1 | Batch 176/6998 | Avg Loss: 1.0833\n",
      "Epoch 1/1 | Batch 180/6998 | Avg Loss: 1.0241\n",
      "Epoch 1/1 | Batch 184/6998 | Avg Loss: 1.0479\n",
      "Epoch 1/1 | Batch 188/6998 | Avg Loss: 1.1468\n",
      "Epoch 1/1 | Batch 192/6998 | Avg Loss: 0.9664\n",
      "Epoch 1/1 | Batch 196/6998 | Avg Loss: 1.0866\n",
      "Epoch 1/1 | Batch 200/6998 | Avg Loss: 1.1180\n",
      "Epoch 1/1 | Batch 204/6998 | Avg Loss: 1.0222\n",
      "Epoch 1/1 | Batch 208/6998 | Avg Loss: 1.0825\n",
      "Epoch 1/1 | Batch 212/6998 | Avg Loss: 1.1055\n",
      "Epoch 1/1 | Batch 216/6998 | Avg Loss: 1.0042\n",
      "Epoch 1/1 | Batch 220/6998 | Avg Loss: 1.0442\n",
      "Epoch 1/1 | Batch 224/6998 | Avg Loss: 1.0824\n",
      "Epoch 1/1 | Batch 228/6998 | Avg Loss: 1.0372\n",
      "Epoch 1/1 | Batch 232/6998 | Avg Loss: 1.0730\n",
      "Epoch 1/1 | Batch 236/6998 | Avg Loss: 1.0437\n",
      "Epoch 1/1 | Batch 240/6998 | Avg Loss: 1.0788\n",
      "Epoch 1/1 | Batch 244/6998 | Avg Loss: 1.0402\n",
      "Epoch 1/1 | Batch 248/6998 | Avg Loss: 1.0395\n",
      "Epoch 1/1 | Batch 252/6998 | Avg Loss: 1.0645\n",
      "Epoch 1/1 | Batch 256/6998 | Avg Loss: 1.0922\n",
      "Epoch 1/1 | Batch 260/6998 | Avg Loss: 1.0421\n",
      "Epoch 1/1 | Batch 264/6998 | Avg Loss: 1.0346\n",
      "Epoch 1/1 | Batch 268/6998 | Avg Loss: 1.0547\n",
      "Epoch 1/1 | Batch 272/6998 | Avg Loss: 1.0458\n",
      "Epoch 1/1 | Batch 276/6998 | Avg Loss: 1.1321\n",
      "Epoch 1/1 | Batch 280/6998 | Avg Loss: 1.0853\n",
      "Epoch 1/1 | Batch 284/6998 | Avg Loss: 1.0891\n",
      "Epoch 1/1 | Batch 288/6998 | Avg Loss: 1.0826\n",
      "Epoch 1/1 | Batch 292/6998 | Avg Loss: 1.0862\n",
      "Epoch 1/1 | Batch 296/6998 | Avg Loss: 0.9737\n",
      "Epoch 1/1 | Batch 300/6998 | Avg Loss: 1.0382\n",
      "Epoch 1/1 | Batch 304/6998 | Avg Loss: 1.0549\n",
      "Epoch 1/1 | Batch 308/6998 | Avg Loss: 1.0179\n",
      "Epoch 1/1 | Batch 312/6998 | Avg Loss: 1.0281\n",
      "Epoch 1/1 | Batch 316/6998 | Avg Loss: 1.0363\n",
      "Epoch 1/1 | Batch 320/6998 | Avg Loss: 1.1239\n",
      "Epoch 1/1 | Batch 324/6998 | Avg Loss: 0.9870\n",
      "Epoch 1/1 | Batch 328/6998 | Avg Loss: 1.1034\n",
      "Epoch 1/1 | Batch 332/6998 | Avg Loss: 1.0446\n",
      "Epoch 1/1 | Batch 336/6998 | Avg Loss: 1.0423\n",
      "Epoch 1/1 | Batch 340/6998 | Avg Loss: 1.0285\n",
      "Epoch 1/1 | Batch 344/6998 | Avg Loss: 1.0332\n",
      "Epoch 1/1 | Batch 348/6998 | Avg Loss: 1.0930\n",
      "Epoch 1/1 | Batch 352/6998 | Avg Loss: 1.0711\n",
      "Epoch 1/1 | Batch 356/6998 | Avg Loss: 1.0422\n",
      "Epoch 1/1 | Batch 360/6998 | Avg Loss: 1.0212\n",
      "Epoch 1/1 | Batch 364/6998 | Avg Loss: 1.0988\n",
      "Epoch 1/1 | Batch 368/6998 | Avg Loss: 1.0526\n",
      "Epoch 1/1 | Batch 372/6998 | Avg Loss: 1.0369\n",
      "Epoch 1/1 | Batch 376/6998 | Avg Loss: 1.0721\n",
      "Epoch 1/1 | Batch 380/6998 | Avg Loss: 1.0425\n",
      "Epoch 1/1 | Batch 384/6998 | Avg Loss: 1.0220\n",
      "Epoch 1/1 | Batch 388/6998 | Avg Loss: 1.0653\n",
      "Epoch 1/1 | Batch 392/6998 | Avg Loss: 1.0780\n",
      "Epoch 1/1 | Batch 396/6998 | Avg Loss: 1.0562\n",
      "Epoch 1/1 | Batch 400/6998 | Avg Loss: 1.0102\n",
      "Epoch 1/1 | Batch 404/6998 | Avg Loss: 1.1237\n",
      "Epoch 1/1 | Batch 408/6998 | Avg Loss: 1.0803\n",
      "Epoch 1/1 | Batch 412/6998 | Avg Loss: 1.0744\n",
      "Epoch 1/1 | Batch 416/6998 | Avg Loss: 1.0242\n",
      "Epoch 1/1 | Batch 420/6998 | Avg Loss: 1.0549\n",
      "Epoch 1/1 | Batch 424/6998 | Avg Loss: 1.0725\n",
      "Epoch 1/1 | Batch 428/6998 | Avg Loss: 1.0566\n",
      "Epoch 1/1 | Batch 432/6998 | Avg Loss: 1.0514\n",
      "Epoch 1/1 | Batch 436/6998 | Avg Loss: 1.0924\n",
      "Epoch 1/1 | Batch 440/6998 | Avg Loss: 1.0310\n",
      "Epoch 1/1 | Batch 444/6998 | Avg Loss: 1.0853\n",
      "Epoch 1/1 | Batch 448/6998 | Avg Loss: 1.0407\n",
      "Epoch 1/1 | Batch 452/6998 | Avg Loss: 1.0247\n",
      "Epoch 1/1 | Batch 456/6998 | Avg Loss: 1.0868\n",
      "Epoch 1/1 | Batch 460/6998 | Avg Loss: 1.0409\n",
      "Epoch 1/1 | Batch 464/6998 | Avg Loss: 1.0033\n",
      "Epoch 1/1 | Batch 468/6998 | Avg Loss: 1.0271\n",
      "Epoch 1/1 | Batch 472/6998 | Avg Loss: 1.0790\n",
      "Epoch 1/1 | Batch 476/6998 | Avg Loss: 0.9960\n",
      "Epoch 1/1 | Batch 480/6998 | Avg Loss: 1.0531\n",
      "Epoch 1/1 | Batch 484/6998 | Avg Loss: 1.0553\n",
      "Epoch 1/1 | Batch 488/6998 | Avg Loss: 1.0585\n",
      "Epoch 1/1 | Batch 492/6998 | Avg Loss: 1.0045\n",
      "Epoch 1/1 | Batch 496/6998 | Avg Loss: 0.9801\n",
      "Epoch 1/1 | Batch 500/6998 | Avg Loss: 1.0805\n",
      "Epoch 1/1 | Batch 504/6998 | Avg Loss: 1.0519\n",
      "Epoch 1/1 | Batch 508/6998 | Avg Loss: 1.0771\n",
      "Epoch 1/1 | Batch 512/6998 | Avg Loss: 1.1074\n",
      "Epoch 1/1 | Batch 516/6998 | Avg Loss: 1.0864\n",
      "Epoch 1/1 | Batch 520/6998 | Avg Loss: 1.0374\n",
      "Epoch 1/1 | Batch 524/6998 | Avg Loss: 1.0637\n",
      "Epoch 1/1 | Batch 528/6998 | Avg Loss: 1.0480\n",
      "Epoch 1/1 | Batch 532/6998 | Avg Loss: 1.0748\n",
      "Epoch 1/1 | Batch 536/6998 | Avg Loss: 1.0853\n",
      "Epoch 1/1 | Batch 540/6998 | Avg Loss: 1.0235\n",
      "Epoch 1/1 | Batch 544/6998 | Avg Loss: 1.0721\n",
      "Epoch 1/1 | Batch 548/6998 | Avg Loss: 1.0697\n",
      "Epoch 1/1 | Batch 552/6998 | Avg Loss: 1.0286\n",
      "Epoch 1/1 | Batch 556/6998 | Avg Loss: 1.1084\n",
      "Epoch 1/1 | Batch 560/6998 | Avg Loss: 1.1113\n",
      "Epoch 1/1 | Batch 564/6998 | Avg Loss: 1.0392\n",
      "Epoch 1/1 | Batch 568/6998 | Avg Loss: 1.0721\n",
      "Epoch 1/1 | Batch 572/6998 | Avg Loss: 1.0367\n",
      "Epoch 1/1 | Batch 576/6998 | Avg Loss: 1.0645\n",
      "Epoch 1/1 | Batch 580/6998 | Avg Loss: 1.0511\n",
      "Epoch 1/1 | Batch 584/6998 | Avg Loss: 1.0497\n",
      "Epoch 1/1 | Batch 588/6998 | Avg Loss: 1.0546\n",
      "Epoch 1/1 | Batch 592/6998 | Avg Loss: 1.0839\n",
      "Epoch 1/1 | Batch 596/6998 | Avg Loss: 1.0182\n",
      "Epoch 1/1 | Batch 600/6998 | Avg Loss: 1.0719\n",
      "Epoch 1/1 | Batch 604/6998 | Avg Loss: 1.0295\n",
      "Epoch 1/1 | Batch 608/6998 | Avg Loss: 1.0654\n",
      "Epoch 1/1 | Batch 612/6998 | Avg Loss: 1.1061\n",
      "Epoch 1/1 | Batch 616/6998 | Avg Loss: 1.0828\n",
      "Epoch 1/1 | Batch 620/6998 | Avg Loss: 1.0591\n",
      "Epoch 1/1 | Batch 624/6998 | Avg Loss: 1.0552\n",
      "Epoch 1/1 | Batch 628/6998 | Avg Loss: 1.0561\n",
      "Epoch 1/1 | Batch 632/6998 | Avg Loss: 1.1034\n",
      "Epoch 1/1 | Batch 636/6998 | Avg Loss: 1.0907\n",
      "Epoch 1/1 | Batch 640/6998 | Avg Loss: 1.0671\n",
      "Epoch 1/1 | Batch 644/6998 | Avg Loss: 1.0076\n",
      "Epoch 1/1 | Batch 648/6998 | Avg Loss: 1.0538\n",
      "Epoch 1/1 | Batch 652/6998 | Avg Loss: 1.0584\n",
      "Epoch 1/1 | Batch 656/6998 | Avg Loss: 1.0445\n",
      "Epoch 1/1 | Batch 660/6998 | Avg Loss: 1.0481\n",
      "Epoch 1/1 | Batch 664/6998 | Avg Loss: 1.0155\n",
      "Epoch 1/1 | Batch 668/6998 | Avg Loss: 1.0055\n",
      "Epoch 1/1 | Batch 672/6998 | Avg Loss: 1.0564\n",
      "Epoch 1/1 | Batch 676/6998 | Avg Loss: 1.0609\n",
      "Epoch 1/1 | Batch 680/6998 | Avg Loss: 1.0173\n",
      "Epoch 1/1 | Batch 684/6998 | Avg Loss: 1.0579\n",
      "Epoch 1/1 | Batch 688/6998 | Avg Loss: 1.0209\n",
      "Epoch 1/1 | Batch 692/6998 | Avg Loss: 1.0294\n",
      "Epoch 1/1 | Batch 696/6998 | Avg Loss: 1.1099\n",
      "Epoch 1/1 | Batch 700/6998 | Avg Loss: 1.1010\n",
      "Epoch 1/1 | Batch 704/6998 | Avg Loss: 1.0981\n",
      "Epoch 1/1 | Batch 708/6998 | Avg Loss: 1.0704\n",
      "Epoch 1/1 | Batch 712/6998 | Avg Loss: 1.1342\n",
      "Epoch 1/1 | Batch 716/6998 | Avg Loss: 1.0658\n",
      "Epoch 1/1 | Batch 720/6998 | Avg Loss: 1.1308\n",
      "Epoch 1/1 | Batch 724/6998 | Avg Loss: 1.0456\n",
      "Epoch 1/1 | Batch 728/6998 | Avg Loss: 0.9917\n",
      "Epoch 1/1 | Batch 732/6998 | Avg Loss: 1.0604\n",
      "Epoch 1/1 | Batch 736/6998 | Avg Loss: 1.0782\n",
      "Epoch 1/1 | Batch 740/6998 | Avg Loss: 1.0036\n",
      "Epoch 1/1 | Batch 744/6998 | Avg Loss: 1.0286\n",
      "Epoch 1/1 | Batch 748/6998 | Avg Loss: 1.0086\n",
      "Epoch 1/1 | Batch 752/6998 | Avg Loss: 1.0369\n",
      "Epoch 1/1 | Batch 756/6998 | Avg Loss: 1.1079\n",
      "Epoch 1/1 | Batch 760/6998 | Avg Loss: 1.0656\n",
      "Epoch 1/1 | Batch 764/6998 | Avg Loss: 1.0084\n",
      "Epoch 1/1 | Batch 768/6998 | Avg Loss: 1.0375\n",
      "Epoch 1/1 | Batch 772/6998 | Avg Loss: 1.0208\n",
      "Epoch 1/1 | Batch 776/6998 | Avg Loss: 0.9958\n",
      "Epoch 1/1 | Batch 780/6998 | Avg Loss: 1.0737\n",
      "Epoch 1/1 | Batch 784/6998 | Avg Loss: 1.0573\n",
      "Epoch 1/1 | Batch 788/6998 | Avg Loss: 0.9992\n",
      "Epoch 1/1 | Batch 792/6998 | Avg Loss: 1.0421\n",
      "Epoch 1/1 | Batch 796/6998 | Avg Loss: 1.0161\n",
      "Epoch 1/1 | Batch 800/6998 | Avg Loss: 1.0391\n",
      "Epoch 1/1 | Batch 804/6998 | Avg Loss: 1.0019\n",
      "Epoch 1/1 | Batch 808/6998 | Avg Loss: 1.0774\n",
      "Epoch 1/1 | Batch 812/6998 | Avg Loss: 1.0198\n",
      "Epoch 1/1 | Batch 816/6998 | Avg Loss: 1.0260\n",
      "Epoch 1/1 | Batch 820/6998 | Avg Loss: 1.0108\n",
      "Epoch 1/1 | Batch 824/6998 | Avg Loss: 1.0417\n",
      "Epoch 1/1 | Batch 828/6998 | Avg Loss: 1.1150\n",
      "Epoch 1/1 | Batch 832/6998 | Avg Loss: 1.0032\n",
      "Epoch 1/1 | Batch 836/6998 | Avg Loss: 1.0689\n",
      "Epoch 1/1 | Batch 840/6998 | Avg Loss: 1.1083\n",
      "Epoch 1/1 | Batch 844/6998 | Avg Loss: 1.0174\n",
      "Epoch 1/1 | Batch 848/6998 | Avg Loss: 0.9734\n",
      "Epoch 1/1 | Batch 852/6998 | Avg Loss: 1.0596\n",
      "Epoch 1/1 | Batch 856/6998 | Avg Loss: 0.9985\n",
      "Epoch 1/1 | Batch 860/6998 | Avg Loss: 1.0377\n",
      "Epoch 1/1 | Batch 864/6998 | Avg Loss: 1.1126\n",
      "Epoch 1/1 | Batch 868/6998 | Avg Loss: 1.0090\n",
      "Epoch 1/1 | Batch 872/6998 | Avg Loss: 1.0388\n",
      "Epoch 1/1 | Batch 876/6998 | Avg Loss: 1.0975\n",
      "Epoch 1/1 | Batch 880/6998 | Avg Loss: 1.0946\n",
      "Epoch 1/1 | Batch 884/6998 | Avg Loss: 1.1579\n",
      "Epoch 1/1 | Batch 888/6998 | Avg Loss: 1.0917\n",
      "Epoch 1/1 | Batch 892/6998 | Avg Loss: 1.0507\n",
      "Epoch 1/1 | Batch 896/6998 | Avg Loss: 1.0305\n",
      "Epoch 1/1 | Batch 900/6998 | Avg Loss: 1.0743\n",
      "Epoch 1/1 | Batch 904/6998 | Avg Loss: 1.0761\n",
      "Epoch 1/1 | Batch 908/6998 | Avg Loss: 0.9809\n",
      "Epoch 1/1 | Batch 912/6998 | Avg Loss: 1.0581\n",
      "Epoch 1/1 | Batch 916/6998 | Avg Loss: 0.9941\n",
      "Epoch 1/1 | Batch 920/6998 | Avg Loss: 1.0488\n",
      "Epoch 1/1 | Batch 924/6998 | Avg Loss: 1.0816\n",
      "Epoch 1/1 | Batch 928/6998 | Avg Loss: 1.1032\n",
      "Epoch 1/1 | Batch 932/6998 | Avg Loss: 1.0660\n",
      "Epoch 1/1 | Batch 936/6998 | Avg Loss: 1.0405\n",
      "Epoch 1/1 | Batch 940/6998 | Avg Loss: 1.1122\n",
      "Epoch 1/1 | Batch 944/6998 | Avg Loss: 1.0682\n",
      "Epoch 1/1 | Batch 948/6998 | Avg Loss: 1.0731\n",
      "Epoch 1/1 | Batch 952/6998 | Avg Loss: 1.0534\n",
      "Epoch 1/1 | Batch 956/6998 | Avg Loss: 1.0459\n",
      "Epoch 1/1 | Batch 960/6998 | Avg Loss: 1.0453\n",
      "Epoch 1/1 | Batch 964/6998 | Avg Loss: 0.9804\n",
      "Epoch 1/1 | Batch 968/6998 | Avg Loss: 1.0666\n",
      "Epoch 1/1 | Batch 972/6998 | Avg Loss: 1.0658\n",
      "Epoch 1/1 | Batch 976/6998 | Avg Loss: 1.0431\n",
      "Epoch 1/1 | Batch 980/6998 | Avg Loss: 1.0270\n",
      "Epoch 1/1 | Batch 984/6998 | Avg Loss: 1.0090\n",
      "Epoch 1/1 | Batch 988/6998 | Avg Loss: 1.0559\n",
      "Epoch 1/1 | Batch 992/6998 | Avg Loss: 1.0545\n",
      "Epoch 1/1 | Batch 996/6998 | Avg Loss: 1.0093\n",
      "Epoch 1/1 | Batch 1000/6998 | Avg Loss: 1.0581\n",
      "Epoch 1/1 | Batch 1004/6998 | Avg Loss: 1.0319\n",
      "Epoch 1/1 | Batch 1008/6998 | Avg Loss: 1.0077\n",
      "Epoch 1/1 | Batch 1012/6998 | Avg Loss: 1.0417\n",
      "Epoch 1/1 | Batch 1016/6998 | Avg Loss: 1.0030\n",
      "Epoch 1/1 | Batch 1020/6998 | Avg Loss: 1.0526\n",
      "Epoch 1/1 | Batch 1024/6998 | Avg Loss: 1.0381\n",
      "F1 Score per class: [0.3250478  0.26229507 0.33157894 0.39506173 0.33974358]\n",
      "\n",
      "Checkpoint saved at epoch 1 batch 1024\n",
      "Epoch 1/1 | Batch 1028/6998 | Avg Loss: 1.0636\n",
      "Epoch 1/1 | Batch 1032/6998 | Avg Loss: 1.0037\n",
      "Epoch 1/1 | Batch 1036/6998 | Avg Loss: 1.0514\n",
      "Epoch 1/1 | Batch 1040/6998 | Avg Loss: 1.0188\n",
      "Epoch 1/1 | Batch 1044/6998 | Avg Loss: 1.0943\n",
      "Epoch 1/1 | Batch 1048/6998 | Avg Loss: 1.0637\n",
      "Epoch 1/1 | Batch 1052/6998 | Avg Loss: 1.0254\n",
      "Epoch 1/1 | Batch 1056/6998 | Avg Loss: 1.0031\n",
      "Epoch 1/1 | Batch 1060/6998 | Avg Loss: 0.9777\n",
      "Epoch 1/1 | Batch 1064/6998 | Avg Loss: 0.9963\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m model = ECGCombined(d_input=d_input, d_model=d_model, num_classes=num_classes, nhead=nhead, num_encoder_layers=num_encoder_layers, dim_feedforward=dim_feedforward).to(device)\n\u001b[32m      2\u001b[39m trainer = Trainer(model, device, accum_steps=accum_steps, lr=starting_lr, pos_weights=pos_weights, checkpoint_interval=checkpoint_interval)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, train_dataloader, test_dataloader, num_epochs, save_path)\u001b[39m\n\u001b[32m     59\u001b[39m loss.backward()\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Every batch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[38;5;28mself\u001b[39m.accum_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_count += \u001b[32m1\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Every accum_steps\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = ECGCombined(d_input=d_input, d_model=d_model, num_classes=num_classes, nhead=nhead, num_encoder_layers=num_encoder_layers, dim_feedforward=dim_feedforward).to(device)\n",
    "trainer = Trainer(model, device, accum_steps=accum_steps, lr=starting_lr, pos_weights=pos_weights, checkpoint_interval=checkpoint_interval)\n",
    "trainer.train(train_dataloader, test_dataloader, num_epochs=1, save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume from a checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_from = f\"{save_path}/checkpoint_ep0_b2047.pt\"\n",
    "resume_lr = 1e-8\n",
    "\n",
    "model = ECGCombined(d_input=d_input, d_model=d_model, num_classes=num_classes, nhead=nhead, num_encoder_layers=num_encoder_layers, dim_feedforward=dim_feedforward).to(device)\n",
    "trainer = Trainer(model, device, accum_steps=accum_steps, lr=starting_lr, pos_weights=pos_weights, checkpoint_interval=checkpoint_interval, resume_checkpoint=resume_from)\n",
    "trainer.train(train_dataloader, test_dataloader, num_epochs=1, save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = np.load(f'{save_path}/loss_history.npy', allow_pickle=True)  # Load loss history\n",
    "acc_history = np.load(f'{save_path}/acc_history.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss history plot\n",
    "x = [epoch[0] for epoch in loss_history]\n",
    "y = [epoch[1] for epoch in loss_history]\n",
    "plt.plot(x, y, label='Loss')\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score plot\n",
    "x = [epoch[0] for epoch in acc_history]\n",
    "y = [epoch[1]['f1_per_class'] for epoch in acc_history]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Batches')\n",
    "plt.legend([f'Class {i}' for i in range(len(y[0]))])\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hamming accuracy plot\n",
    "x = [epoch[0] for epoch in acc_history]\n",
    "y = [epoch[1]['hamming_loss_per_class'] for epoch in acc_history]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.legend([f'Class {i}' for i in range(len(y[0]))])\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('Hamming Loss')\n",
    "#plt.yscale('log')\n",
    "plt.title('Hamming Loss per class')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
