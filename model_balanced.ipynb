{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import platform\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import F1Score\n",
    "from sklearn.metrics import multilabel_confusion_matrix, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Define device for torch\n",
    "device = torch.device(\"cpu\")\n",
    "# MPS for Apple Silicon GPUs\n",
    "if torch.mps.is_available():\n",
    "   print(\"MPS is available\")\n",
    "   device = torch.device(\"mps\")\n",
    "\n",
    "# CUDA for Nvidia GPUs\n",
    "if torch.cuda.is_available():\n",
    "   print(\"CUDA is available\")\n",
    "   device = torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    for name, module in model.named_modules():\n",
    "        params = sum(p.numel() for p in module.parameters())\n",
    "        print(f\"{name}: {params} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader\n",
    "\n",
    "To investigate: Normalization or other transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeECG:\n",
    "    def __call__(self, tensor):\n",
    "        # Z-score normalization per lead\n",
    "        means = tensor.mean(dim=1, keepdim=True)\n",
    "        stds = tensor.std(dim=1, keepdim=True)\n",
    "        return (tensor - means) / (stds + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, path=\"data/ecg_clipped\", diagnoses='data/diagnoses_balanced.csv', transform=None):\n",
    "        # Load and prepare labels\n",
    "        self.labels_df = pd.read_csv(diagnoses)\n",
    "        self.path = path\n",
    "\n",
    "        self.labels_df['ID'] = self.labels_df['ID'].astype(str).str.replace(r'\\D', '', regex=True) # Remove the JS\n",
    "        self.labels_df.set_index('ID', inplace=True)\n",
    "        self.num_classes = self.labels_df.shape[1]\n",
    "        print(f'Number of classes: {self.num_classes}')\n",
    "\n",
    "        self.transform = transform\n",
    "        self.cache = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.cache:\n",
    "            return self.cache[idx]\n",
    "        \n",
    "        try:\n",
    "            # Access the row through iloc of the index,\n",
    "            # Use the ID to make filepath\n",
    "            ID = self.labels_df.iloc[idx].name\n",
    "\n",
    "            file_path = self.path + f'/{ID}.csv'\n",
    "            \n",
    "            # Load ECG data\n",
    "            df = pd.read_csv(file_path)\n",
    "            ecg_data = df.drop(columns=['time']).values\n",
    "            tensor = torch.tensor(ecg_data, dtype=torch.float32).T  # (leads, timesteps)\n",
    "            \n",
    "            if self.transform:\n",
    "                tensor = self.transform(tensor)\n",
    "                \n",
    "            # Get corresponding label\n",
    "\n",
    "            label_values = self.labels_df.loc[ID].values  # Get all label columns\n",
    "            label = torch.tensor(label_values, dtype=torch.float32)  # Use float for multi-label\n",
    "\n",
    "            return tensor, label\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {str(e)}\")\n",
    "            # Return zero tensor and -1 label placeholder\n",
    "            return torch.zeros((12, 5000), dtype=torch.float32), torch.full((self.num_classes,), -1, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 5\n",
      "torch.Size([12, 5000])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "dataset = ECGDataset()\n",
    "data, label = dataset.__getitem__(23423)\n",
    "print(data.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGTransformer(nn.Module):\n",
    "    def __init__(self, d_model, num_classes=63, nhead=8, num_encoder_layers=2, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "\n",
    "        # Encoder stack\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.transformer(x)\n",
    "        # encoded shape: (batch_size, seq_len, d_model)\n",
    "        # Pick out only the last in the sequence for classification\n",
    "        encoded = encoded[:, -1, :]\n",
    "        result = self.classifier(encoded)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGTransformer(\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=12, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=12, bias=True)\n",
      "        (norm1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=12, out_features=63, bias=True)\n",
      ")\n",
      ": 81723 parameters\n",
      "transformer: 80904 parameters\n",
      "transformer.layers: 80904 parameters\n",
      "transformer.layers.0: 13484 parameters\n",
      "transformer.layers.0.self_attn: 624 parameters\n",
      "transformer.layers.0.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.0.linear1: 6656 parameters\n",
      "transformer.layers.0.dropout: 0 parameters\n",
      "transformer.layers.0.linear2: 6156 parameters\n",
      "transformer.layers.0.norm1: 24 parameters\n",
      "transformer.layers.0.norm2: 24 parameters\n",
      "transformer.layers.0.dropout1: 0 parameters\n",
      "transformer.layers.0.dropout2: 0 parameters\n",
      "transformer.layers.1: 13484 parameters\n",
      "transformer.layers.1.self_attn: 624 parameters\n",
      "transformer.layers.1.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.1.linear1: 6656 parameters\n",
      "transformer.layers.1.dropout: 0 parameters\n",
      "transformer.layers.1.linear2: 6156 parameters\n",
      "transformer.layers.1.norm1: 24 parameters\n",
      "transformer.layers.1.norm2: 24 parameters\n",
      "transformer.layers.1.dropout1: 0 parameters\n",
      "transformer.layers.1.dropout2: 0 parameters\n",
      "transformer.layers.2: 13484 parameters\n",
      "transformer.layers.2.self_attn: 624 parameters\n",
      "transformer.layers.2.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.2.linear1: 6656 parameters\n",
      "transformer.layers.2.dropout: 0 parameters\n",
      "transformer.layers.2.linear2: 6156 parameters\n",
      "transformer.layers.2.norm1: 24 parameters\n",
      "transformer.layers.2.norm2: 24 parameters\n",
      "transformer.layers.2.dropout1: 0 parameters\n",
      "transformer.layers.2.dropout2: 0 parameters\n",
      "transformer.layers.3: 13484 parameters\n",
      "transformer.layers.3.self_attn: 624 parameters\n",
      "transformer.layers.3.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.3.linear1: 6656 parameters\n",
      "transformer.layers.3.dropout: 0 parameters\n",
      "transformer.layers.3.linear2: 6156 parameters\n",
      "transformer.layers.3.norm1: 24 parameters\n",
      "transformer.layers.3.norm2: 24 parameters\n",
      "transformer.layers.3.dropout1: 0 parameters\n",
      "transformer.layers.3.dropout2: 0 parameters\n",
      "transformer.layers.4: 13484 parameters\n",
      "transformer.layers.4.self_attn: 624 parameters\n",
      "transformer.layers.4.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.4.linear1: 6656 parameters\n",
      "transformer.layers.4.dropout: 0 parameters\n",
      "transformer.layers.4.linear2: 6156 parameters\n",
      "transformer.layers.4.norm1: 24 parameters\n",
      "transformer.layers.4.norm2: 24 parameters\n",
      "transformer.layers.4.dropout1: 0 parameters\n",
      "transformer.layers.4.dropout2: 0 parameters\n",
      "transformer.layers.5: 13484 parameters\n",
      "transformer.layers.5.self_attn: 624 parameters\n",
      "transformer.layers.5.self_attn.out_proj: 156 parameters\n",
      "transformer.layers.5.linear1: 6656 parameters\n",
      "transformer.layers.5.dropout: 0 parameters\n",
      "transformer.layers.5.linear2: 6156 parameters\n",
      "transformer.layers.5.norm1: 24 parameters\n",
      "transformer.layers.5.norm2: 24 parameters\n",
      "transformer.layers.5.dropout1: 0 parameters\n",
      "transformer.layers.5.dropout2: 0 parameters\n",
      "classifier: 819 parameters\n"
     ]
    }
   ],
   "source": [
    "model = ECGTransformer(d_model=12, nhead=4, num_classes=63, num_encoder_layers=6, dim_feedforward=512)\n",
    "print(model)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 63])\n",
      "tensor([-0.5976,  0.0157, -0.1560,  1.2270,  0.7546, -0.7115, -0.2304,  0.0610,\n",
      "         0.2929,  0.2745, -1.1032,  0.2608,  0.7337,  0.3054,  0.5114, -0.7468,\n",
      "        -0.7540, -0.6236, -0.1163,  0.0468, -0.6683, -0.3627,  1.0099, -0.1941,\n",
      "        -0.4496,  1.0456, -0.2819,  0.8145, -0.2478, -0.7113,  0.0505,  1.0545,\n",
      "         1.2402,  0.5754, -0.2997, -0.4403,  0.1334,  0.2674, -0.3430, -0.8559,\n",
      "        -0.0499,  0.5307, -0.0683,  0.2259,  0.1282,  0.2585,  1.5873,  0.4660,\n",
      "        -0.3691, -0.3999,  0.4655, -0.1192,  0.1596, -0.5121, -0.2434,  0.2127,\n",
      "         0.5876,  0.1357, -0.4895, -0.1691,  0.1823, -0.5510, -0.3298],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.rand((2, 5000, 12))\n",
    "out = model(inputs)\n",
    "print(out.shape)\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An embedding model \n",
    "that uses convolution\n",
    "\n",
    "Convolution turning 12 channels to 128, repeated to transfer forward 200ms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGEmbeddings(nn.Module):\n",
    "    def __init__(self, d_input, d_model, n_conv_layers=3):\n",
    "        super().__init__()\n",
    "        # Keep original layers\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(d_model if i>0 else d_input, d_model, 51, stride=1, padding='same')\n",
    "            for i in range(n_conv_layers)\n",
    "        ])\n",
    "        self.activation = nn.ReLU(inplace=False)  # Important for checkpointing\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.conv_layers)):\n",
    "            x = self.conv_layers[i](x)\n",
    "            if i < len(self.conv_layers) - 1:\n",
    "                x = self.activation(x)\n",
    "\n",
    "        if not x.requires_grad:\n",
    "            x = x.detach().requires_grad_(True)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGEmbeddings(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv1d(12, 512, kernel_size=(51,), stride=(1,), padding=same)\n",
      "    (1-2): 2 x Conv1d(512, 512, kernel_size=(51,), stride=(1,), padding=same)\n",
      "  )\n",
      "  (activation): ReLU()\n",
      ")\n",
      ": 27053568 parameters\n",
      "conv_layers: 27053568 parameters\n",
      "conv_layers.0: 313856 parameters\n",
      "conv_layers.1: 13369856 parameters\n",
      "conv_layers.2: 13369856 parameters\n",
      "activation: 0 parameters\n"
     ]
    }
   ],
   "source": [
    "embedding_model = ECGEmbeddings(d_input = 12, d_model=512)\n",
    "print(embedding_model)\n",
    "count_parameters(embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining together embedding with transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGCombined(nn.Module):\n",
    "    def __init__(self, d_input, d_model, num_classes=63, nhead=8, num_encoder_layers=2, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.embedding_model = ECGEmbeddings(d_input, d_model)\n",
    "        self.transformer = ECGTransformer(d_model, num_classes, nhead, num_encoder_layers, dim_feedforward)\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_model(x)\n",
    "\n",
    "        # Shape before transformer: (batch, channels, seq_len)\n",
    "        x = x.permute(0, 2, 1)  # → (batch, seq_len, d_model)\n",
    "\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"❗ NaNs BEFORE transformer\")\n",
    "            print(\"Input stats → min:\", x.min(), \"max:\", x.max(), \"mean:\", x.mean())\n",
    "            raise ValueError(\"NaNs before transformer\")\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"❗ NaNs AFTER transformer\")\n",
    "            print(\"Output stats → min:\", x.min(), \"max:\", x.max(), \"mean:\", x.mean())\n",
    "            raise ValueError(\"NaNs after transformer\")\n",
    "\n",
    "        return x\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.11993747 3.12052302 3.11993747 3.12052302 3.12052302]\n"
     ]
    }
   ],
   "source": [
    "# Drop 'ID' column if still present\n",
    "balanced_df = pd.read_csv(\"data/diagnoses_balanced.csv\")\n",
    "label_df = balanced_df.drop(columns='ID')\n",
    "\n",
    "# Compute counts\n",
    "pos_counts = label_df.sum()\n",
    "neg_counts = len(label_df) - pos_counts\n",
    "\n",
    "# Calculate pos_weight = #neg / #pos for each class\n",
    "pos_weight = (neg_counts / pos_counts).values\n",
    "\n",
    "# Move to device\n",
    "pos_weight_tensor = torch.tensor(pos_weight, dtype=torch.float32, device=device)\n",
    "print(pos_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, device, accum_steps=4, checkpoint_interval=256, lr=1e-5,\n",
    "                 resume_checkpoint=None):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.accum_steps = accum_steps\n",
    "        self.checkpoint_interval = checkpoint_interval\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "\n",
    "        if self.device.type == 'cuda':\n",
    "            self.autocast = torch.cuda.amp.autocast\n",
    "            self.scaler = torch.cuda.amp.GradScaler()\n",
    "        else:\n",
    "            from contextlib import nullcontext\n",
    "            self.autocast = lambda: nullcontext()  # no-op context\n",
    "            self.scaler = None\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_history = []\n",
    "        self.acc_history = []\n",
    "        self.batch_count = 0\n",
    "        self.start_epoch = 0\n",
    "        self.start_batch = 0\n",
    "\n",
    "        if resume_checkpoint:\n",
    "            self._load_checkpoint(resume_checkpoint)\n",
    "\n",
    "    def _load_checkpoint(self, checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)\n",
    "        self.model.load_state_dict(checkpoint['model_state'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "\n",
    "        if self.scaler and checkpoint.get('scaler_state'):\n",
    "            self.scaler.load_state_dict(checkpoint['scaler_state'])\n",
    "\n",
    "        self.loss_history = checkpoint['loss_history']\n",
    "        self.acc_history = checkpoint['acc_history']\n",
    "        self.batch_count = checkpoint.get('batch_count', 0)\n",
    "        self.start_epoch = checkpoint['epoch']\n",
    "        self.start_batch = checkpoint.get('batch', 0) + 1\n",
    "        self.checkpoint_interval = checkpoint.get('checkpoint_interval', self.checkpoint_interval)\n",
    "        print(f\"Resuming from epoch {self.start_epoch} batch {self.start_batch}\")\n",
    "\n",
    "    def loss(self, output, target):\n",
    "        return self.loss_fn(output, target.float())\n",
    "\n",
    "    def train(self, train_dataloader, test_dataloader, num_epochs, save_path=\"training_progress\"):\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in range(self.start_epoch, num_epochs):\n",
    "            for batch_idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "                if batch_idx < self.start_batch:\n",
    "                    continue\n",
    "\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                if torch.isnan(inputs).any() or torch.isinf(inputs).any():\n",
    "                    print(\"⚠️ Bad input detected\")\n",
    "                    raise ValueError(\"Inputs contain NaNs or Infs\")\n",
    "\n",
    "                with self.autocast():\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = self.loss(outputs, labels) / self.accum_steps\n",
    "                    if torch.isnan(loss):\n",
    "                        print(\"⚠️ Loss is NaN!\")\n",
    "                        raise ValueError(\"Loss turned NaN\")\n",
    "\n",
    "                if self.scaler:\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "\n",
    "                if (batch_idx + 1) % self.accum_steps == 0:\n",
    "                    self._update_parameters()\n",
    "\n",
    "                current_loss = loss.item() * self.accum_steps\n",
    "                self.loss_history.append(current_loss)\n",
    "                self.batch_count += 1\n",
    "\n",
    "                if self.batch_count % self.accum_steps == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx+1}/{len(train_dataloader)} | \"\n",
    "                          f\"Loss: {current_loss:.4f}\")\n",
    "\n",
    "                if self.batch_count % self.checkpoint_interval == 0:\n",
    "                    acc = self.evaluate(test_dataloader)\n",
    "                    self.acc_history.append([self.batch_count, acc])\n",
    "                    self._save_checkpoint(save_path, epoch, batch_idx, test_dataloader)\n",
    "\n",
    "                del inputs, labels, outputs, loss\n",
    "                if torch.backends.mps.is_available():\n",
    "                    torch.mps.empty_cache()\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_samples = 0\n",
    "        num_classes = self.model.num_classes\n",
    "        mismatches_per_class = torch.zeros(num_classes, device=self.device)\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                predicted = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "\n",
    "                all_preds.append(predicted.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "\n",
    "                mismatches_per_class += (predicted != labels).sum(dim=0).float()\n",
    "                total_samples += inputs.size(0)\n",
    "\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "\n",
    "        y_true = all_labels.numpy().astype(int)\n",
    "        y_pred = all_preds.numpy().astype(int)\n",
    "\n",
    "        f1 = F1Score(task='multilabel', num_labels=num_classes, average='macro')\n",
    "        f1_score = f1(all_preds, all_labels)\n",
    "\n",
    "        hamming_loss_per_class = mismatches_per_class.cpu().numpy() / total_samples\n",
    "        overall_hamming_loss = mismatches_per_class.sum().item() / (total_samples * num_classes)\n",
    "\n",
    "        precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "        recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "        conf_matrices = multilabel_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        return {\n",
    "            \"f1\": f1_score.item(),\n",
    "            \"overall\": overall_hamming_loss,\n",
    "            \"per_class\": hamming_loss_per_class,\n",
    "            \"precision\": precision_per_class,\n",
    "            \"recall\": recall_per_class,\n",
    "            \"conf_matrices\": conf_matrices\n",
    "        }\n",
    "\n",
    "    def _update_parameters(self):\n",
    "        if self.scaler:\n",
    "            self.scaler.unscale_(self.optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def _save_checkpoint(self, path, epoch, batch_idx, dataloader):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'batch': batch_idx,\n",
    "            'batch_count': self.batch_count,\n",
    "            'checkpoint_interval': self.checkpoint_interval,\n",
    "            'model_state': self.model.state_dict(),\n",
    "            'optimizer_state': self.optimizer.state_dict(),\n",
    "            'loss_history': self.loss_history,\n",
    "            'acc_history': self.acc_history,\n",
    "            'scaler_state': self.scaler.state_dict() if self.scaler else None\n",
    "        }\n",
    "\n",
    "        checkpoint_path = f\"{path}/checkpoint_ep{epoch}_b{batch_idx}.pt\"\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"\\nCheckpoint saved at epoch {epoch+1} batch {batch_idx+1}\")\n",
    "\n",
    "        np.save(f\"{path}/loss_history.npy\", np.array(self.loss_history))\n",
    "        np.save(f\"{path}/acc_history.npy\", np.array(self.acc_history))\n",
    "\n",
    "        eval_metrics = self.evaluate(dataloader=dataloader)\n",
    "\n",
    "        np.save(f\"{path}/precision_ep{epoch}_b{batch_idx}.npy\", eval_metrics[\"precision\"])\n",
    "        np.save(f\"{path}/recall_ep{epoch}_b{batch_idx}.npy\", eval_metrics[\"recall\"])\n",
    "\n",
    "        conf_matrices = eval_metrics[\"conf_matrices\"]\n",
    "        label_names = [\n",
    "            \"Dx_426177001\",\n",
    "            \"Dx_426783006\",\n",
    "            \"Dx_164890007\",\n",
    "            \"Dx_427084000\",\n",
    "            \"Dx_164934002\"\n",
    "        ]\n",
    "\n",
    "        for i, cm in enumerate(conf_matrices):\n",
    "            plt.figure(figsize=(4, 3))\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                        xticklabels=[\"Pred 0\", \"Pred 1\"], yticklabels=[\"True 0\", \"True 1\"])\n",
    "            plt.title(f'Confusion Matrix for {label_names[i]}')\n",
    "            plt.xlabel('Prediction')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{path}/conf_matrix_class{i}_ep{epoch}_b{batch_idx}.png\")\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Go Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 5\n"
     ]
    }
   ],
   "source": [
    "ecg_dataset = ECGDataset(diagnoses='data/diagnoses_balanced.csv', transform=NormalizeECG())\n",
    "train_dataset, test_dataset, val_dataset = random_split(\n",
    "                                            ecg_dataset, [len(ecg_dataset) - 1000, 500, 500], \n",
    "                                            torch.Generator().manual_seed(42))\n",
    "\n",
    "batch_size = 4\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start from 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Batch 16/6998 | Loss: 1.3350\n",
      "Epoch 1/1 | Batch 32/6998 | Loss: 1.3596\n",
      "Epoch 1/1 | Batch 48/6998 | Loss: 1.2424\n",
      "Epoch 1/1 | Batch 64/6998 | Loss: 1.1674\n",
      "Epoch 1/1 | Batch 80/6998 | Loss: 1.4725\n",
      "Epoch 1/1 | Batch 96/6998 | Loss: 1.6019\n",
      "Epoch 1/1 | Batch 112/6998 | Loss: 1.2695\n",
      "Epoch 1/1 | Batch 128/6998 | Loss: 1.3575\n",
      "Epoch 1/1 | Batch 144/6998 | Loss: 1.2457\n",
      "Epoch 1/1 | Batch 160/6998 | Loss: 1.2416\n",
      "Epoch 1/1 | Batch 176/6998 | Loss: 1.0487\n",
      "Epoch 1/1 | Batch 192/6998 | Loss: 0.9922\n",
      "Epoch 1/1 | Batch 208/6998 | Loss: 1.3511\n",
      "Epoch 1/1 | Batch 224/6998 | Loss: 1.0293\n",
      "Epoch 1/1 | Batch 240/6998 | Loss: 1.1364\n",
      "Epoch 1/1 | Batch 256/6998 | Loss: 1.3376\n",
      "\n",
      "Checkpoint saved at epoch 1 batch 256\n",
      "Epoch 1/1 | Batch 272/6998 | Loss: 1.0293\n",
      "Epoch 1/1 | Batch 288/6998 | Loss: 1.3979\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m ECGCombined(d_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, nhead\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_encoder_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim_feedforward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, device, accum_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,checkpoint_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-6\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_progress/balanced\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 63\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_dataloader, test_dataloader, num_epochs, save_path)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs contain NaNs or Infs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 63\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(outputs, labels) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccum_steps\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ECGCombined(d_input=12, d_model=64, num_classes=5, nhead=4, num_encoder_layers=2, dim_feedforward=128).to(device)\n",
    "trainer = Trainer(model, device, accum_steps=16,checkpoint_interval=256, lr=5e-6)\n",
    "trainer.train(train_dataloader, test_dataloader, num_epochs=1, save_path=\"training_progress/balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: nan nan\n",
      "Predicted class counts per label: [0. 0. 0. 0. 0.]\n",
      "True class counts per label: [3. 1. 0. 0. 3.]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)  # raw logits\n",
    "        print(\"Logits:\", outputs.min().item(), outputs.max().item())\n",
    "\n",
    "        # Optional: look at predicted classes\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs >= 0.5).float()\n",
    "\n",
    "        print(\"Predicted class counts per label:\", preds.sum(dim=0).cpu().numpy())\n",
    "        print(\"True class counts per label:\", labels.sum(dim=0).cpu().numpy())\n",
    "\n",
    "        break  # just do one batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume from a checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_from = \"training_progress/cut/checkpoint_ep0_b2559.pt\"\n",
    "\n",
    "model = ECGCombined(d_input=12, d_model=64, num_classes=2, nhead=4, num_encoder_layers=2, dim_feedforward=128).to(device)\n",
    "trainer = Trainer(model, device, accum_steps=16, lr=1e-8, resume_checkpoint=resume_from)\n",
    "trainer.train(train_dataloader, test_dataloader, num_epochs=1, save_path=\"training_progress/cut\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_history = np.load('training_progress/cut/acc_history.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss history plot\n",
    "loss_history = np.load('training_progress/cut/loss_history.npy', allow_pickle=True)  # Load loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score plot\n",
    "x = np.array([i[0] for i in acc_history])\n",
    "\n",
    "f1 = np.array([i[1]['f1'] for i in acc_history])\n",
    "plt.plot(x, f1)\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hamming accuracy plot\n",
    "classes_acc = np.array([epoch[1]['per_class'] for epoch in acc_history])\n",
    "plt.plot(x, classes_acc)\n",
    "plt.legend([f'Class {i}' for i in range(2)], loc='upper right')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Hamming Loss')\n",
    "plt.yscale('log')\n",
    "plt.title('Hamming Loss per class')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
